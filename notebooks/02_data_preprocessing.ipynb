{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9a55b5",
   "metadata": {},
   "source": [
    "# Machine Learning Course: Data Preprocessing\n",
    "\n",
    "## Notebook 2: Data Preprocessing, Normalization, and Cleaning\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Load and validate preprocessed data from exploration phase\n",
    "2. Implement strategies for handling missing values in genomics data\n",
    "3. Filter genes based on variance and quality metrics\n",
    "4. Normalize gene expression data using different methods\n",
    "5. Scale and encode clinical variables appropriately\n",
    "6. Identify and handle outlier samples\n",
    "7. Create balanced train/validation/test splits\n",
    "8. Export clean, analysis-ready datasets\n",
    "\n",
    "### Prerequisites\n",
    "- Completed `01_data_exploration.ipynb`\n",
    "- Understanding of basic statistics and data quality concepts\n",
    "- Familiarity with pandas and numpy operations\n",
    "\n",
    "### Preprocessing Pipeline Overview\n",
    "This notebook follows a systematic preprocessing pipeline:\n",
    "1. **Data Loading & Validation** - Load exploration results and verify data integrity\n",
    "2. **Missing Values Handling** - Implement appropriate strategies for different data types\n",
    "3. **Gene Filtering** - Remove low-quality and uninformative genes\n",
    "4. **Outlier Detection** - Identify and handle problematic samples\n",
    "5. **Normalization** - Apply appropriate scaling to expression and clinical data\n",
    "6. **Data Splitting** - Create stratified train/validation/test sets\n",
    "7. **Quality Validation** - Verify preprocessing results\n",
    "8. **Data Export** - Save clean datasets for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c52d23",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries for data preprocessing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38021d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 1: Import Required Libraries for Data Preprocessing\n",
    "#\n",
    "# Your task: Import comprehensive libraries for data preprocessing, normalization, and quality control\n",
    "#\n",
    "# TODO: Import core data manipulation libraries:\n",
    "# 1. pandas (as pd) - for data manipulation and analysis\n",
    "# 2. numpy (as np) - for numerical operations and array handling\n",
    "# 3. matplotlib.pyplot (as plt) - for plotting and visualization\n",
    "# 4. seaborn (as sns) - for statistical visualization\n",
    "# 5. os - for file system operations\n",
    "# 6. warnings - to suppress warning messages\n",
    "#\n",
    "# TODO: Import preprocessing and machine learning libraries:\n",
    "# 7. From sklearn.preprocessing import: StandardScaler, MinMaxScaler, RobustScaler\n",
    "# 8. From sklearn.impute import: SimpleImputer, KNNImputer\n",
    "# 9. From sklearn.model_selection import: train_test_split, StratifiedShuffleSplit\n",
    "# 10. From sklearn.feature_selection import: VarianceThreshold\n",
    "#\n",
    "# TODO: Import statistical and scientific libraries:\n",
    "# 11. From scipy.stats import: zscore, pearsonr, spearmanr\n",
    "# 12. From scipy import stats\n",
    "# 13. import json - for saving/loading configuration files\n",
    "#\n",
    "# TODO: Configure the environment:\n",
    "# 14. Suppress warnings using warnings.filterwarnings('ignore')\n",
    "# 15. Set matplotlib plotting style to 'seaborn-v0_8' (or 'default' if seaborn style unavailable)\n",
    "# 16. Set seaborn color palette to \"Set2\"\n",
    "# 17. Set numpy random seed to 42 for reproducibility\n",
    "# 18. Set pandas display options: pd.set_option('display.max_columns', 20)\n",
    "#\n",
    "# TODO: Print library versions and setup confirmation:\n",
    "# 19. Print versions of key libraries (pandas, numpy, sklearn)\n",
    "# 20. Print confirmation that preprocessing environment is ready\n",
    "#\n",
    "# Expected output: Successfully imported libraries with version information\n",
    "\n",
    "# Write your code below:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d761703",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Validation\n",
    "\n",
    "Before we can preprocess the data, we need to load it and validate its structure based on our exploration findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3368690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 2: Load Raw Data and Exploration Results\n",
    "#\n",
    "# Your task: Load the datasets and exploration results from the previous notebook\n",
    "#\n",
    "# TODO: Set up data paths and configuration:\n",
    "# 1. Define DATA_PATH = '../data/' for raw data location\n",
    "# 2. Define RESULTS_PATH = '../results/exploration/' for exploration results\n",
    "# 3. Define PROCESSED_PATH = '../results/preprocessed/' for output location\n",
    "# 4. Create the processed results directory using os.makedirs() with exist_ok=True\n",
    "#\n",
    "# TODO: Load raw datasets (same as in exploration notebook):\n",
    "# 5. Load expression data from 'data_mrna_illumina_microarray.txt'\n",
    "# 6. Load clinical patient data from 'data_clinical_patient.txt'\n",
    "# 7. Load clinical sample data from 'data_clinical_sample.txt' (if available)\n",
    "# 8. Use appropriate pd.read_csv() parameters (sep='\\t', comment='#', index_col for expression)\n",
    "# 9. Include proper error handling with try-except blocks\n",
    "#\n",
    "# TODO: Load exploration results:\n",
    "# 10. Load gene statistics from '../results/exploration/gene_statistics.csv'\n",
    "# 11. Load sample statistics from '../results/exploration/sample_statistics.csv'\n",
    "# 12. Load clinical summary from '../results/exploration/clinical_summary.json'\n",
    "# 13. Handle cases where exploration results might not exist\n",
    "#\n",
    "# TODO: Validate data consistency:\n",
    "# 14. Check that data shapes match expectations from exploration\n",
    "# 15. Verify that gene names and sample IDs are consistent\n",
    "# 16. Compare current missing value counts with exploration results\n",
    "# 17. Print validation summary with ‚úì for passed checks and ‚ö†Ô∏è for issues\n",
    "#\n",
    "# TODO: Display loading summary:\n",
    "# 18. Print header \"DATA LOADING AND VALIDATION SUMMARY\"\n",
    "# 19. Show dimensions of loaded datasets\n",
    "# 20. Report which exploration results were successfully loaded\n",
    "# 21. Confirm data is ready for preprocessing\n",
    "#\n",
    "# Expected output: Successfully loaded datasets with validation confirmation\n",
    "\n",
    "# Write your code below:\n",
    "# DATA_PATH = '../data/'\n",
    "# RESULTS_PATH = '../results/exploration/'\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 3: Create Data Quality Baseline\n",
    "#\n",
    "# Your task: Establish baseline data quality metrics before preprocessing\n",
    "#\n",
    "# TODO: Calculate baseline statistics for expression data:\n",
    "# 1. Print \"BASELINE DATA QUALITY METRICS\" header with separators\n",
    "# 2. Calculate and store original data dimensions\n",
    "# 3. Count missing values: expression_data.isnull().sum().sum()\n",
    "# 4. Calculate percentage of missing values relative to total data size\n",
    "# 5. Count genes with zero variance: (expression_data.var(axis=1) == 0).sum()\n",
    "# 6. Count low-variance genes (variance < 0.01): (expression_data.var(axis=1) < 0.01).sum()\n",
    "#\n",
    "# TODO: Analyze sample quality:\n",
    "# 7. Calculate sample means and standard deviations\n",
    "# 8. Identify outlier samples using 3-sigma rule:\n",
    "#    - Find samples where |sample_mean - overall_mean| > 3 * overall_std\n",
    "#    - Do the same for sample standard deviations\n",
    "# 9. Count outlier samples and store their IDs\n",
    "#\n",
    "# TODO: Assess clinical data quality:\n",
    "# 10. Count missing values per clinical variable\n",
    "# 11. Identify variables with >50% missing data\n",
    "# 12. Count categorical variables vs numerical variables\n",
    "# 13. Check for duplicate patient records\n",
    "#\n",
    "# TODO: Create baseline quality summary:\n",
    "# 14. Store all baseline metrics in a dictionary: baseline_metrics\n",
    "# 15. Include: original_genes, original_samples, expression_missing_pct,\n",
    "#     zero_var_genes, low_var_genes, outlier_samples, clinical_high_missing\n",
    "# 16. Print formatted summary of all baseline metrics\n",
    "# 17. Save baseline metrics as JSON for later comparison\n",
    "#\n",
    "# TODO: Flag potential issues:\n",
    "# 18. Create issues_found list to collect quality concerns\n",
    "# 19. Add warnings for high missing rates, many low-variance genes, etc.\n",
    "# 20. Print issues found or \"No major quality issues detected\"\n",
    "#\n",
    "# Expected output: Comprehensive baseline quality report with identified issues\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"BASELINE DATA QUALITY METRICS\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98dc19f",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis and Handling\n",
    "\n",
    "Missing values are common in genomics data. Let's implement appropriate strategies for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b04c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 4: Analyze Missing Value Patterns in Detail\n",
    "#\n",
    "# Your task: Perform comprehensive analysis of missing value patterns to guide imputation strategy\n",
    "#\n",
    "# TODO: Set up missing values analysis:\n",
    "# 1. Print \"DETAILED MISSING VALUES ANALYSIS\" header with separators\n",
    "# 2. Create copies of original data for analysis: expression_missing = expression_data.copy()\n",
    "#\n",
    "# TODO: Analyze expression data missing patterns:\n",
    "# 3. Calculate missing values per gene: missing_per_gene = expression_data.isnull().sum(axis=1)\n",
    "# 4. Calculate missing values per sample: missing_per_sample = expression_data.isnull().sum(axis=0)\n",
    "# 5. Find genes with any missing values: genes_with_missing = missing_per_gene[missing_per_gene > 0]\n",
    "# 6. Find samples with any missing values: samples_with_missing = missing_per_sample[missing_per_sample > 0]\n",
    "#\n",
    "# TODO: Display missing value statistics:\n",
    "# 7. Print total missing values and percentage of entire dataset\n",
    "# 8. Print number of genes affected and percentage of total genes\n",
    "# 9. Print number of samples affected and percentage of total samples\n",
    "# 10. If genes have missing values, show top 10 genes with most missing\n",
    "# 11. If samples have missing values, show top 10 samples with most missing\n",
    "#\n",
    "# TODO: Analyze clinical data missing patterns:\n",
    "# 12. Calculate missing per variable: clinical_missing = clinical_data.isnull().sum()\n",
    "# 13. Find variables with missing values: vars_with_missing = clinical_missing[clinical_missing > 0]\n",
    "# 14. Calculate missing percentages: missing_pct = (vars_with_missing / len(clinical_data)) * 100\n",
    "# 15. Create summary DataFrame with variable name, missing count, and percentage\n",
    "# 16. Sort by missing percentage (descending) and display\n",
    "#\n",
    "# TODO: Categorize missing value severity:\n",
    "# 17. Categorize clinical variables by missing percentage:\n",
    "#     - Low missing: <5%, Moderate: 5-20%, High: 20-50%, Very high: >50%\n",
    "# 18. Count variables in each category\n",
    "# 19. Print categorization summary\n",
    "#\n",
    "# TODO: Check for missing value patterns:\n",
    "# 20. Look for systematic missing patterns (e.g., all related variables missing together)\n",
    "# 21. Check correlation between missing indicators using pd.DataFrame(clinical_data.isnull())\n",
    "# 22. Identify any non-random missing patterns\n",
    "#\n",
    "# Expected output: Comprehensive missing value analysis with categorization and pattern detection\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"DETAILED MISSING VALUES ANALYSIS\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 5: Implement Missing Values Imputation Strategy\n",
    "#\n",
    "# Your task: Apply appropriate imputation methods for different types of missing data\n",
    "#\n",
    "# TODO: Set up imputation strategy:\n",
    "# 1. Print \"MISSING VALUES IMPUTATION\" header with separators\n",
    "# 2. Create working copies: expression_imputed = expression_data.copy()\n",
    "# 3. Create clinical_imputed = clinical_data.copy()\n",
    "#\n",
    "# TODO: Handle expression data missing values:\n",
    "# 4. Check if expression data has missing values: if expression_data.isnull().sum().sum() > 0:\n",
    "# 5. For expression data, use KNN imputation (genes have biological relationships):\n",
    "#    - Import KNNImputer from sklearn.impute\n",
    "#    - Create imputer with n_neighbors=5\n",
    "#    - Fit and transform the data: imputer.fit_transform(expression_data.T).T\n",
    "#    - The .T transpose is because KNN works on samples, but we want to impute across samples for each gene\n",
    "# 6. If no missing values, print \"No missing values in expression data\"\n",
    "#\n",
    "# TODO: Handle clinical data missing values by variable type:\n",
    "# 7. Separate numerical and categorical clinical variables\n",
    "# 8. For numerical variables with missing values:\n",
    "#    - Use median imputation for variables with <20% missing\n",
    "#    - Use KNN imputation for variables with 20-50% missing\n",
    "#    - Consider dropping variables with >50% missing (flag for review)\n",
    "# 9. For categorical variables with missing values:\n",
    "#    - Use mode (most frequent) imputation for variables with <30% missing\n",
    "#    - Create \"Unknown\" category for variables with >30% missing\n",
    "#    - Use pd.get_dummies() later for encoding\n",
    "#\n",
    "# TODO: Implement clinical imputation:\n",
    "# 10. Create SimpleImputer for numerical: SimpleImputer(strategy='median')\n",
    "# 11. Create SimpleImputer for categorical: SimpleImputer(strategy='most_frequent')\n",
    "# 12. Apply imputers separately to numerical and categorical variables\n",
    "# 13. Combine results back into single DataFrame\n",
    "#\n",
    "# TODO: Validate imputation results:\n",
    "# 14. Check that no missing values remain: assert imputed_data.isnull().sum().sum() == 0\n",
    "# 15. Compare distributions before/after imputation for key variables\n",
    "# 16. Print summary of imputation methods used\n",
    "#\n",
    "# TODO: Create imputation summary:\n",
    "# 17. Document what imputation methods were applied to which variables\n",
    "# 18. Save imputation log for reproducibility\n",
    "#\n",
    "# Expected output: Successfully imputed datasets with validation and method documentation\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"MISSING VALUES IMPUTATION\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b8299",
   "metadata": {},
   "source": [
    "## 4. Gene Filtering and Quality Control\n",
    "\n",
    "Not all genes are informative for machine learning. Let's filter out low-quality and uninformative genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da39371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 6: Filter Low-Variance and Uninformative Genes\n",
    "#\n",
    "# Your task: Remove genes that are not informative for machine learning modeling\n",
    "#\n",
    "# TODO: Set up gene filtering analysis:\n",
    "# 1. Print \"GENE FILTERING AND QUALITY CONTROL\" header with separators\n",
    "# 2. Create working copy: expression_filtered = expression_imputed.copy()\n",
    "# 3. Store original gene count: original_gene_count = len(expression_filtered)\n",
    "#\n",
    "# TODO: Calculate gene-level statistics:\n",
    "# 4. Calculate variance for each gene: gene_variances = expression_filtered.var(axis=1)\n",
    "# 5. Calculate mean expression for each gene: gene_means = expression_filtered.mean(axis=1)\n",
    "# 6. Calculate coefficient of variation: gene_cv = expression_filtered.std(axis=1) / gene_means.abs()\n",
    "# 7. Handle division by zero in CV calculation using np.where() or .fillna(0)\n",
    "#\n",
    "# TODO: Apply variance filtering:\n",
    "# 8. Use VarianceThreshold from sklearn.feature_selection\n",
    "# 9. Set threshold=0.01 (genes with variance < 0.01 will be removed)\n",
    "# 10. Create variance_selector = VarianceThreshold(threshold=0.01)\n",
    "# 11. Fit selector on transposed data: variance_selector.fit(expression_filtered.T)\n",
    "# 12. Get selected gene indices: selected_genes_idx = variance_selector.get_support()\n",
    "# 13. Filter expression data: expression_filtered = expression_filtered.loc[selected_genes_idx]\n",
    "#\n",
    "# TODO: Apply additional quality filters:\n",
    "# 14. Remove genes with very low mean expression (mean < -3 for log2 data)\n",
    "# 15. Remove genes with very high mean expression (potential housekeeping genes, mean > 3)\n",
    "# 16. Filter using boolean indexing on gene_means\n",
    "# 17. Update expression_filtered with these additional filters\n",
    "#\n",
    "# TODO: Apply coefficient of variation filter (optional):\n",
    "# 18. Keep genes with moderate to high CV (CV > 0.1) - these show meaningful variation\n",
    "# 19. This removes genes that are too stable or too noisy\n",
    "# 20. Apply CV filter: expression_filtered = expression_filtered[gene_cv > 0.1]\n",
    "#\n",
    "# TODO: Calculate and display filtering results:\n",
    "# 21. Calculate filtered_gene_count = len(expression_filtered)\n",
    "# 22. Calculate genes_removed = original_gene_count - filtered_gene_count\n",
    "# 23. Calculate percentage_removed = (genes_removed / original_gene_count) * 100\n",
    "# 24. Print summary: original count, final count, removed count and percentage\n",
    "#\n",
    "# TODO: Analyze filtering impact:\n",
    "# 25. Compare variance distribution before/after filtering\n",
    "# 26. Show range of mean expression values before/after\n",
    "# 27. Create before/after comparison visualization if needed\n",
    "#\n",
    "# Expected output: Filtered expression dataset with quality summary\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"GENE FILTERING AND QUALITY CONTROL\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e615cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 7: Outlier Detection and Handling\n",
    "#\n",
    "# Your task: Identify and handle outlier samples that might affect model performance\n",
    "#\n",
    "# TODO: Set up outlier detection:\n",
    "# 1. Print \"OUTLIER DETECTION AND HANDLING\" header with separators\n",
    "# 2. Create working copy: expression_outlier_checked = expression_filtered.copy()\n",
    "#\n",
    "# TODO: Sample-level outlier detection:\n",
    "# 3. Calculate sample statistics:\n",
    "#    - sample_means = expression_outlier_checked.mean(axis=0)\n",
    "#    - sample_stds = expression_outlier_checked.std(axis=0)\n",
    "#    - sample_medians = expression_outlier_checked.median(axis=0)\n",
    "# 4. Calculate z-scores for sample means: mean_zscores = np.abs(zscore(sample_means))\n",
    "# 5. Calculate z-scores for sample stds: std_zscores = np.abs(zscore(sample_stds))\n",
    "#\n",
    "# TODO: Identify outlier samples:\n",
    "# 6. Set outlier threshold: z_threshold = 3.0 (3-sigma rule)\n",
    "# 7. Find samples with outlier means: outlier_means = sample_means[mean_zscores > z_threshold]\n",
    "# 8. Find samples with outlier stds: outlier_stds = sample_stds[std_zscores > z_threshold]\n",
    "# 9. Combine all outlier samples: outlier_samples = set(outlier_means.index) | set(outlier_stds.index)\n",
    "#\n",
    "# TODO: Analyze outlier characteristics:\n",
    "# 10. Print number of outlier samples found\n",
    "# 11. If outliers exist, print their sample IDs\n",
    "# 12. For each outlier, print its mean and std z-scores\n",
    "# 13. Calculate outlier percentage: (len(outlier_samples) / len(sample_means)) * 100\n",
    "#\n",
    "# TODO: Gene-level outlier detection:\n",
    "# 14. Calculate gene statistics for outlier detection:\n",
    "#     - gene_means_filtered = expression_outlier_checked.mean(axis=1)\n",
    "#     - gene_stds_filtered = expression_outlier_checked.std(axis=1)\n",
    "# 15. Find genes with extreme values: genes with mean > 4 or mean < -4 (for log2 data)\n",
    "# 16. Find genes with extreme variability: genes with std > 3\n",
    "#\n",
    "# TODO: Decide on outlier handling strategy:\n",
    "# 17. For samples: decide whether to remove or cap outlier values\n",
    "# 18. Conservative approach: flag for investigation but don't remove automatically\n",
    "# 19. Alternative: remove samples that are outliers in multiple metrics\n",
    "# 20. Document outlier handling decisions\n",
    "#\n",
    "# TODO: Apply outlier handling (if decided):\n",
    "# 21. If removing outlier samples: expression_outlier_checked = expression_outlier_checked.drop(columns=outlier_samples)\n",
    "# 22. If capping values: use np.clip() to limit extreme values\n",
    "# 23. Update clinical data accordingly to match remaining samples\n",
    "#\n",
    "# TODO: Validate outlier handling:\n",
    "# 24. Recalculate sample statistics after handling\n",
    "# 25. Verify that extreme outliers are reduced\n",
    "# 26. Print final sample count and outlier handling summary\n",
    "#\n",
    "# Expected output: Outlier analysis with handling strategy and validation\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"OUTLIER DETECTION AND HANDLING\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddde24",
   "metadata": {},
   "source": [
    "## 5. Data Normalization and Scaling\n",
    "\n",
    "Different normalization methods are appropriate for different types of data. Let's apply the right methods for expression and clinical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2beadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 8: Normalize Expression Data\n",
    "#\n",
    "# Your task: Apply appropriate normalization to gene expression data for machine learning\n",
    "#\n",
    "# TODO: Set up normalization analysis:\n",
    "# 1. Print \"EXPRESSION DATA NORMALIZATION\" header with separators\n",
    "# 2. Create working copy: expression_to_normalize = expression_outlier_checked.copy()\n",
    "#\n",
    "# TODO: Analyze current data distribution:\n",
    "# 3. Calculate current statistics: overall_mean, overall_std, data_min, data_max\n",
    "# 4. Create histogram of all expression values to understand current distribution\n",
    "# 5. Check if data appears to be already log2-transformed (values typically -10 to +10)\n",
    "# 6. Print current data characteristics\n",
    "#\n",
    "# TODO: Gene-wise (row-wise) Z-score normalization:\n",
    "# 7. Apply Z-score normalization per gene (across samples):\n",
    "#    - expression_gene_zscore = expression_to_normalize.apply(zscore, axis=1)\n",
    "#    - This makes each gene have mean=0, std=1 across samples\n",
    "# 8. Handle any NaN values that result from zero-variance genes\n",
    "# 9. Calculate statistics after gene-wise normalization\n",
    "#\n",
    "# TODO: Sample-wise (column-wise) normalization:\n",
    "# 10. Apply quantile normalization or Z-score per sample:\n",
    "#     - For sample Z-score: expression_sample_zscore = expression_to_normalize.apply(zscore, axis=0)\n",
    "#     - This makes each sample have mean=0, std=1 across genes\n",
    "# 11. Alternative: Use RobustScaler for sample-wise scaling (less sensitive to outliers)\n",
    "# 12. Compare different normalization approaches\n",
    "#\n",
    "# TODO: Choose and apply final normalization method:\n",
    "# 13. For machine learning, gene-wise Z-score is often preferred:\n",
    "#     - It makes genes comparable in their variation patterns\n",
    "#     - Removes gene-specific baseline differences\n",
    "# 14. Apply chosen method: expression_normalized = expression_gene_zscore.copy()\n",
    "# 15. Ensure no NaN values remain: expression_normalized.fillna(0, inplace=True)\n",
    "#\n",
    "# TODO: Validate normalization results:\n",
    "# 16. Check that genes now have mean ‚âà 0 and std ‚âà 1 (within tolerance)\n",
    "# 17. Verify that sample relationships are preserved\n",
    "# 18. Create before/after distribution comparison plots\n",
    "# 19. Calculate normalization quality metrics\n",
    "#\n",
    "# TODO: Create normalization summary:\n",
    "# 20. Print normalization method used\n",
    "# 21. Show before/after statistics comparison\n",
    "# 22. Document any issues encountered and how they were handled\n",
    "#\n",
    "# Expected output: Normalized expression data with validation metrics and quality assessment\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"EXPRESSION DATA NORMALIZATION\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de75ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 9: Scale and Encode Clinical Variables\n",
    "#\n",
    "# Your task: Prepare clinical variables for machine learning by appropriate scaling and encoding\n",
    "#\n",
    "# TODO: Set up clinical data preprocessing:\n",
    "# 1. Print \"CLINICAL DATA SCALING AND ENCODING\" header with separators\n",
    "# 2. Create working copy: clinical_to_process = clinical_imputed.copy()\n",
    "# 3. Ensure clinical data matches remaining samples after any outlier removal\n",
    "#\n",
    "# TODO: Separate variable types:\n",
    "# 4. Get numerical variables: numerical_vars = clinical_to_process.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# 5. Get categorical variables: categorical_vars = clinical_to_process.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# 6. Print counts of each variable type\n",
    "#\n",
    "# TODO: Scale numerical variables:\n",
    "# 7. For most ML algorithms, use StandardScaler (Z-score normalization):\n",
    "#    - scaler = StandardScaler()\n",
    "#    - clinical_numerical_scaled = scaler.fit_transform(clinical_to_process[numerical_vars])\n",
    "#    - Convert back to DataFrame: pd.DataFrame(clinical_numerical_scaled, columns=numerical_vars, index=clinical_to_process.index)\n",
    "# 8. Alternative: Use RobustScaler if outliers are a concern\n",
    "# 9. Store scaler object for later use on test data\n",
    "#\n",
    "# TODO: Encode categorical variables:\n",
    "# 10. For binary variables (like ER_IHC: Positive/Negative):\n",
    "#     - Use LabelEncoder or simple mapping: {'Positive': 1, 'Negative': 0}\n",
    "# 11. For multi-class variables (like CLAUDIN_SUBTYPE):\n",
    "#     - Use pd.get_dummies() for one-hot encoding\n",
    "#     - Set drop_first=True to avoid multicollinearity\n",
    "# 12. Handle any 'Unknown' or missing categories appropriately\n",
    "#\n",
    "# TODO: Create encoded categorical dataset:\n",
    "# 13. Apply encoding to each categorical variable\n",
    "# 14. For one-hot encoded variables, use prefix to identify original variable\n",
    "# 15. Combine all encoded variables: clinical_categorical_encoded\n",
    "#\n",
    "# TODO: Combine scaled numerical and encoded categorical data:\n",
    "# 16. Concatenate along columns: clinical_processed = pd.concat([clinical_numerical_scaled, clinical_categorical_encoded], axis=1)\n",
    "# 17. Ensure index alignment with expression data\n",
    "# 18. Handle any column name conflicts\n",
    "#\n",
    "# TODO: Validate clinical preprocessing:\n",
    "# 19. Check for any remaining missing values\n",
    "# 20. Verify that all variables are now numerical\n",
    "# 21. Check data types and value ranges\n",
    "# 22. Print summary of transformations applied\n",
    "#\n",
    "# TODO: Create preprocessing summary:\n",
    "# 23. Document which variables were scaled vs encoded\n",
    "# 24. Save preprocessing objects (scalers, encoders) for later use\n",
    "# 25. Print final clinical data shape and characteristics\n",
    "#\n",
    "# Expected output: Processed clinical data ready for machine learning with transformation summary\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"CLINICAL DATA SCALING AND ENCODING\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c278f3d",
   "metadata": {},
   "source": [
    "## 6. Data Splitting Strategy\n",
    "\n",
    "Creating proper train/validation/test splits is crucial for unbiased model evaluation. Let's implement stratified splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7659bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 10: Prepare Target Variable and Stratification Strategy\n",
    "#\n",
    "# Your task: Define the target variable for risk classification and prepare stratification strategy\n",
    "#\n",
    "# TODO: Set up target variable definition:\n",
    "# 1. Print \"TARGET VARIABLE AND STRATIFICATION SETUP\" header with separators\n",
    "# 2. Identify potential target variables from clinical data (e.g., survival, ER status, molecular subtype)\n",
    "#\n",
    "# TODO: Create risk classification target:\n",
    "# 3. For survival-based risk classification, create binary target:\n",
    "#    - If OS_MONTHS exists: high_risk = (OS_MONTHS < median_survival) & (VITAL_STATUS == 'DECEASED')\n",
    "#    - Alternative: use established risk groups based on molecular subtypes\n",
    "# 4. If survival data unavailable, use ER_IHC status as proxy: target = (ER_IHC == 'Negative')\n",
    "# 5. Ensure target variable has reasonable class balance (30-70% split is good)\n",
    "#\n",
    "# TODO: Analyze target variable distribution:\n",
    "# 6. Calculate class counts: target.value_counts()\n",
    "# 7. Calculate class percentages: target.value_counts(normalize=True) * 100\n",
    "# 8. Print target variable distribution\n",
    "# 9. Check if classes are severely imbalanced (>90% in one class)\n",
    "#\n",
    "# TODO: Identify stratification variables:\n",
    "# 10. Besides target variable, consider stratifying by:\n",
    "#     - Molecular subtype (CLAUDIN_SUBTYPE) if available\n",
    "#     - Age groups (create age bins: young <50, middle 50-65, old >65)\n",
    "#     - Tumor stage/grade if available\n",
    "# 11. Create composite stratification variable combining target + important clinical factor\n",
    "#\n",
    "# TODO: Handle class imbalance if present:\n",
    "# 12. If severe imbalance detected (>80% in one class):\n",
    "#     - Consider redefining target variable\n",
    "#     - Plan for stratified sampling\n",
    "#     - Note need for balanced evaluation metrics\n",
    "# 13. Document class distribution issues\n",
    "#\n",
    "# TODO: Prepare data for splitting:\n",
    "# 14. Ensure expression_normalized and clinical_processed have matching sample indices\n",
    "# 15. Create combined feature matrix if needed: X = pd.concat([expression_normalized.T, clinical_processed], axis=1)\n",
    "# 16. Define target vector: y = target variable aligned with X\n",
    "# 17. Remove any samples where target is undefined/missing\n",
    "#\n",
    "# TODO: Validate data alignment:\n",
    "# 18. Check X.index == y.index (all samples match)\n",
    "# 19. Print final dataset dimensions\n",
    "# 20. Confirm no missing values in features or target\n",
    "#\n",
    "# Expected output: Defined target variable with class distribution and aligned feature matrix\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"TARGET VARIABLE AND STRATIFICATION SETUP\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd16cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 11: Create Stratified Train/Validation/Test Splits\n",
    "#\n",
    "# Your task: Create balanced and representative data splits for model development and evaluation\n",
    "#\n",
    "# TODO: Set up splitting configuration:\n",
    "# 1. Print \"STRATIFIED DATA SPLITTING\" header with separators\n",
    "# 2. Define split ratios: train=60%, validation=20%, test=20%\n",
    "# 3. Set random_state=42 for reproducibility\n",
    "#\n",
    "# TODO: Create initial train/temp split:\n",
    "# 4. Use train_test_split from sklearn.model_selection\n",
    "# 5. Split into train (60%) and temp (40%): X_train, X_temp, y_train, y_temp\n",
    "# 6. Use stratify=y to maintain class balance\n",
    "# 7. Set test_size=0.4, random_state=42\n",
    "#\n",
    "# TODO: Split temp into validation and test:\n",
    "# 8. Split X_temp, y_temp into validation (50% of temp = 20% overall) and test (50% of temp = 20% overall)\n",
    "# 9. Use train_test_split again: X_val, X_test, y_val, y_test\n",
    "# 10. Use stratify=y_temp, test_size=0.5, random_state=42\n",
    "#\n",
    "# TODO: Validate split quality:\n",
    "# 11. Check final split sizes:\n",
    "#     - print(f\"Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "#     - print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "#     - print(f\"Test: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "#\n",
    "# TODO: Verify stratification worked:\n",
    "# 12. Check class distribution in each split:\n",
    "#     - train_dist = y_train.value_counts(normalize=True)\n",
    "#     - val_dist = y_val.value_counts(normalize=True)\n",
    "#     - test_dist = y_test.value_counts(normalize=True)\n",
    "# 13. Create comparison DataFrame showing class percentages across splits\n",
    "# 14. Ensure distributions are similar (within 5% of each other)\n",
    "#\n",
    "# TODO: Split expression and clinical data separately (if combined):\n",
    "# 15. If X contains both expression and clinical data:\n",
    "#     - Determine column indices for expression vs clinical features\n",
    "#     - Create separate matrices: X_train_expr, X_train_clin, etc.\n",
    "# 16. Maintain index alignment across all data splits\n",
    "#\n",
    "# TODO: Create split summary:\n",
    "# 17. Create split_info dictionary with:\n",
    "#     - split_sizes, class_distributions, feature_counts\n",
    "# 18. Print comprehensive splitting summary\n",
    "# 19. Document any stratification issues encountered\n",
    "#\n",
    "# TODO: Validate no data leakage:\n",
    "# 20. Confirm no overlap between train/val/test sample indices\n",
    "# 21. Check: len(set(X_train.index) & set(X_val.index)) == 0\n",
    "# 22. Check: len(set(X_train.index) & set(X_test.index)) == 0\n",
    "# 23. Check: len(set(X_val.index) & set(X_test.index)) == 0\n",
    "#\n",
    "# Expected output: Balanced train/validation/test splits with distribution verification\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"STRATIFIED DATA SPLITTING\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfde296",
   "metadata": {},
   "source": [
    "## üí° Coding Hints and Templates\n",
    "\n",
    "Need help getting started? Here are some code templates and hints for preprocessing operations:\n",
    "\n",
    "### üìã **Template: Missing Values Imputation**\n",
    "```python\n",
    "# KNN Imputation for expression data\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "# Transpose for gene-wise imputation\n",
    "expression_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(expression_data.T).T,\n",
    "    index=expression_data.index,\n",
    "    columns=expression_data.columns\n",
    ")\n",
    "```\n",
    "\n",
    "### üìã **Template: Variance Filtering**\n",
    "```python\n",
    "# Remove low-variance genes\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selected_genes = selector.fit_transform(expression_data.T)\n",
    "selected_indices = selector.get_support()\n",
    "expression_filtered = expression_data.loc[selected_indices]\n",
    "```\n",
    "\n",
    "### üìã **Template: Z-score Normalization**\n",
    "```python\n",
    "# Gene-wise Z-score normalization\n",
    "from scipy.stats import zscore\n",
    "expression_normalized = expression_data.apply(zscore, axis=1)\n",
    "# Handle any NaN values from zero-variance genes\n",
    "expression_normalized = expression_normalized.fillna(0)\n",
    "```\n",
    "\n",
    "### üìã **Template: Clinical Data Scaling**\n",
    "```python\n",
    "# Scale numerical variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(clinical_data[numerical_vars]),\n",
    "    columns=numerical_vars,\n",
    "    index=clinical_data.index\n",
    ")\n",
    "```\n",
    "\n",
    "### üìã **Template: One-Hot Encoding**\n",
    "```python\n",
    "# Encode categorical variables\n",
    "categorical_encoded = pd.get_dummies(\n",
    "    clinical_data[categorical_vars], \n",
    "    drop_first=True, \n",
    "    prefix=categorical_vars\n",
    ")\n",
    "```\n",
    "\n",
    "### üìã **Template: Stratified Splitting**\n",
    "```python\n",
    "# Create stratified train/validation/test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train vs temp (60% vs 40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: validation vs test (20% vs 20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### üìã **Template: Outlier Detection**\n",
    "```python\n",
    "# Sample-wise outlier detection using Z-scores\n",
    "from scipy.stats import zscore\n",
    "sample_means = expression_data.mean(axis=0)\n",
    "mean_zscores = np.abs(zscore(sample_means))\n",
    "outlier_samples = sample_means[mean_zscores > 3].index\n",
    "print(f\"Found {len(outlier_samples)} outlier samples\")\n",
    "```\n",
    "\n",
    "### üîç **Key Preprocessing Decisions**\n",
    "- **Missing Values**: KNN for expression (biological relationships), median/mode for clinical\n",
    "- **Gene Filtering**: Remove variance < 0.01, extreme mean values\n",
    "- **Normalization**: Gene-wise Z-score for expression, StandardScaler for clinical\n",
    "- **Encoding**: One-hot for multi-class, binary encoding for binary variables\n",
    "- **Splitting**: Stratified 60/20/20 with class balance preservation\n",
    "\n",
    "### üîç **Quality Checks to Remember**\n",
    "- Verify no missing values remain after imputation\n",
    "- Check that normalization resulted in mean‚âà0, std‚âà1\n",
    "- Ensure class balance is maintained across splits\n",
    "- Confirm no data leakage between train/val/test\n",
    "- Validate that feature distributions look reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e504f",
   "metadata": {},
   "source": [
    "## üéØ Learning Assessment\n",
    "\n",
    "### ‚úÖ **Self-Check Questions**\n",
    "\n",
    "After completing the preprocessing activities, you should be able to answer:\n",
    "\n",
    "1. **Missing Values Handling**\n",
    "   - What imputation method is most appropriate for gene expression data and why?\n",
    "   - How do you handle categorical variables with >50% missing values?\n",
    "   - What are the risks of using mean imputation vs KNN imputation?\n",
    "   - How do you verify that imputation preserved data distributions?\n",
    "\n",
    "2. **Gene Filtering and Quality Control**\n",
    "   - Why do we remove low-variance genes before machine learning?\n",
    "   - What expression value ranges indicate potential data quality issues?\n",
    "   - How do you identify outlier samples without removing important biological variation?\n",
    "   - What percentage of genes is typically filtered in transcriptomics studies?\n",
    "\n",
    "3. **Normalization and Scaling**\n",
    "   - What's the difference between gene-wise and sample-wise normalization?\n",
    "   - When should you use StandardScaler vs RobustScaler vs MinMaxScaler?\n",
    "   - Why is Z-score normalization often preferred for gene expression data?\n",
    "   - How do you handle zero-variance genes during normalization?\n",
    "\n",
    "4. **Data Splitting Strategy**\n",
    "   - Why is stratification important when creating train/validation/test splits?\n",
    "   - How do you ensure no data leakage between splits?\n",
    "   - What's the impact of class imbalance on model evaluation?\n",
    "   - Why use 60/20/20 splits instead of 80/20 or other ratios?\n",
    "\n",
    "5. **Clinical Data Preprocessing**\n",
    "   - How do you decide between one-hot encoding vs label encoding?\n",
    "   - What's the multicollinearity issue with one-hot encoding and how to address it?\n",
    "   - How do you handle ordinal vs nominal categorical variables differently?\n",
    "   - When should you create interaction features between clinical variables?\n",
    "\n",
    "### üèÜ **Success Criteria**\n",
    "\n",
    "You have successfully completed this preprocessing notebook if you can:\n",
    "- ‚úÖ Load and validate data from the exploration phase\n",
    "- ‚úÖ Apply appropriate imputation strategies for different data types\n",
    "- ‚úÖ Filter genes based on variance and quality metrics\n",
    "- ‚úÖ Normalize expression data using gene-wise Z-score method\n",
    "- ‚úÖ Scale and encode clinical variables appropriately\n",
    "- ‚úÖ Create stratified train/validation/test splits with balanced classes\n",
    "- ‚úÖ Validate preprocessing results and identify potential issues\n",
    "- ‚úÖ Export clean, analysis-ready datasets\n",
    "\n",
    "### üöÄ **Extension Challenges** (Optional)\n",
    "\n",
    "For advanced students:\n",
    "1. **Advanced Imputation**:\n",
    "   - Implement matrix factorization-based imputation (e.g., iterative imputer)\n",
    "   - Compare multiple imputation methods and evaluate their impact\n",
    "   - Use biological pathway information to guide gene imputation\n",
    "\n",
    "2. **Sophisticated Normalization**:\n",
    "   - Implement quantile normalization for cross-sample comparability\n",
    "   - Apply Combat for batch effect correction\n",
    "   - Compare different normalization methods on downstream model performance\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Create gene ratio features (e.g., ER/PR ratios)\n",
    "   - Generate pathway-level summary features\n",
    "   - Create interaction terms between clinical and expression features\n",
    "\n",
    "4. **Advanced Quality Control**:\n",
    "   - Implement statistical tests for outlier detection\n",
    "   - Use PCA for quality assessment and batch effect detection\n",
    "   - Apply clustering to identify sample subgroups\n",
    "\n",
    "5. **Preprocessing Pipeline**:\n",
    "   - Create automated preprocessing pipeline using sklearn Pipeline\n",
    "   - Implement cross-validation-aware preprocessing\n",
    "   - Build preprocessing parameter tuning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80d51f",
   "metadata": {},
   "source": [
    "## 7. Export Preprocessed Data\n",
    "\n",
    "Let's save our preprocessed datasets and create comprehensive documentation for the next modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 12: Export Preprocessed Datasets and Create Documentation\n",
    "#\n",
    "# Your task: Save all preprocessed data and create comprehensive documentation for modeling phase\n",
    "#\n",
    "# TODO: Set up export directory structure:\n",
    "# 1. Create directories for organized data export:\n",
    "#    - os.makedirs(PROCESSED_PATH + 'data/', exist_ok=True)\n",
    "#    - os.makedirs(PROCESSED_PATH + 'splits/', exist_ok=True)\n",
    "#    - os.makedirs(PROCESSED_PATH + 'preprocessing_objects/', exist_ok=True)\n",
    "# 2. Print \"EXPORTING PREPROCESSED DATA\" header with separators\n",
    "#\n",
    "# TODO: Export preprocessed datasets:\n",
    "# 3. Save normalized expression data:\n",
    "#    - expression_normalized.to_csv(PROCESSED_PATH + 'data/expression_normalized.csv')\n",
    "# 4. Save processed clinical data:\n",
    "#    - clinical_processed.to_csv(PROCESSED_PATH + 'data/clinical_processed.csv')\n",
    "# 5. Save target variable:\n",
    "#    - pd.DataFrame({'target': y}).to_csv(PROCESSED_PATH + 'data/target_variable.csv')\n",
    "#\n",
    "# TODO: Export train/validation/test splits:\n",
    "# 6. Save all split datasets:\n",
    "#    - X_train.to_csv(PROCESSED_PATH + 'splits/X_train.csv')\n",
    "#    - X_val.to_csv(PROCESSED_PATH + 'splits/X_val.csv')\n",
    "#    - X_test.to_csv(PROCESSED_PATH + 'splits/X_test.csv')\n",
    "#    - pd.DataFrame({'target': y_train}).to_csv(PROCESSED_PATH + 'splits/y_train.csv')\n",
    "#    - pd.DataFrame({'target': y_val}).to_csv(PROCESSED_PATH + 'splits/y_val.csv')\n",
    "#    - pd.DataFrame({'target': y_test}).to_csv(PROCESSED_PATH + 'splits/y_test.csv')\n",
    "#\n",
    "# TODO: Save preprocessing objects (for applying to new data):\n",
    "# 7. Use joblib to save sklearn objects:\n",
    "#    - import joblib\n",
    "#    - joblib.dump(scaler, PROCESSED_PATH + 'preprocessing_objects/clinical_scaler.pkl')\n",
    "#    - Save any other preprocessing objects (imputers, encoders, etc.)\n",
    "#\n",
    "# TODO: Create comprehensive preprocessing summary:\n",
    "# 8. Create preprocessing_summary dictionary with:\n",
    "#    - original_dimensions: original data shapes\n",
    "#    - final_dimensions: final processed data shapes\n",
    "#    - preprocessing_steps: list of all steps applied\n",
    "#    - gene_filtering_summary: genes removed and criteria\n",
    "#    - missing_values_handling: methods used for each data type\n",
    "#    - normalization_methods: scaling applied to each dataset\n",
    "#    - split_information: sample counts and class distributions\n",
    "#    - quality_metrics: before/after quality comparisons\n",
    "#\n",
    "# TODO: Export preprocessing documentation:\n",
    "# 9. Save preprocessing summary as JSON:\n",
    "#    - with open(PROCESSED_PATH + 'preprocessing_summary.json', 'w') as f:\n",
    "#    -     json.dump(preprocessing_summary, f, indent=2, default=str)\n",
    "#\n",
    "# TODO: Create data dictionary:\n",
    "# 10. Create feature_descriptions DataFrame with:\n",
    "#     - feature_name, feature_type, description, preprocessing_applied\n",
    "# 11. Include information about expression genes and clinical variables\n",
    "# 12. Save as CSV: PROCESSED_PATH + 'feature_dictionary.csv'\n",
    "#\n",
    "# TODO: Generate processing report:\n",
    "# 13. Create human-readable processing report:\n",
    "#     - Data dimensions before/after each step\n",
    "#     - Number of samples and features in final datasets\n",
    "#     - Class balance in train/validation/test splits\n",
    "#     - Quality metrics and any issues encountered\n",
    "# 14. Save as text file: PROCESSED_PATH + 'preprocessing_report.txt'\n",
    "#\n",
    "# TODO: Print export confirmation:\n",
    "# 15. List all files created with their purposes\n",
    "# 16. Print data ready for modeling confirmation\n",
    "# 17. Provide guidance on next steps (modeling notebook)\n",
    "#\n",
    "# Expected output: Complete set of preprocessed data files with comprehensive documentation\n",
    "\n",
    "# Write your code below:\n",
    "# import joblib\n",
    "# print(\"EXPORTING PREPROCESSED DATA\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb6956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Preprocessing Summary\n",
    "\n",
    "In this notebook, you have successfully completed:\n",
    "\n",
    "### ‚úÖ **Completed Tasks:**\n",
    "1. **Data Loading & Validation**: Loaded raw data and exploration results with integrity checks\n",
    "2. **Missing Values Analysis**: Identified patterns and applied appropriate imputation strategies\n",
    "3. **Gene Filtering**: Removed low-variance and uninformative genes based on quality metrics\n",
    "4. **Outlier Detection**: Identified and handled extreme samples using statistical methods\n",
    "5. **Expression Normalization**: Applied gene-wise Z-score normalization for ML compatibility\n",
    "6. **Clinical Data Processing**: Scaled numerical and encoded categorical variables appropriately\n",
    "7. **Target Variable Definition**: Created binary risk classification target with proper stratification\n",
    "8. **Data Splitting**: Generated balanced 60/20/20 train/validation/test splits\n",
    "9. **Quality Validation**: Verified preprocessing results and data integrity\n",
    "10. **Data Export**: Saved clean datasets with comprehensive documentation\n",
    "\n",
    "### üéØ **Key Preprocessing Achievements:**\n",
    "- **Data Quality**: Eliminated missing values and filtered uninformative features\n",
    "- **Standardization**: Normalized all features for consistent ML algorithm performance\n",
    "- **Balance**: Maintained class balance across all data splits\n",
    "- **Documentation**: Created comprehensive preprocessing logs for reproducibility\n",
    "- **Validation**: Verified no data leakage and proper preprocessing application\n",
    "\n",
    "### üîÑ **Next Steps:**\n",
    "In the next notebook (`03_model_development.ipynb`), we will:\n",
    "1. **Baseline Models**: Train simple classifiers for performance benchmarking\n",
    "2. **Feature Selection**: Apply statistical and ML-based feature selection methods\n",
    "3. **Model Comparison**: Evaluate multiple algorithms (SVM, Random Forest, Neural Networks)\n",
    "4. **Hyperparameter Tuning**: Optimize model parameters using grid/random search\n",
    "5. **Cross-Validation**: Implement robust model evaluation strategies\n",
    "6. **Model Interpretation**: Understand which genes drive risk predictions\n",
    "\n",
    "### üìÅ **Exported Files:**\n",
    "- `../results/preprocessed/data/`: Cleaned datasets ready for modeling\n",
    "- `../results/preprocessed/splits/`: Train/validation/test splits with balanced classes\n",
    "- `../results/preprocessed/preprocessing_objects/`: Saved scalers and transformers\n",
    "- `../results/preprocessed/preprocessing_summary.json`: Complete processing documentation\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work completing the data preprocessing phase! üéâ**\n",
    "\n",
    "Your data is now clean, normalized, and ready for machine learning model development. The careful preprocessing work you've completed will be crucial for building robust and reliable risk classification models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
