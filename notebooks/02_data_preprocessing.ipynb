{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9a55b5",
   "metadata": {},
   "source": [
    "# Machine Learning Course: Data Preprocessing\n",
    "\n",
    "## Notebook 2: Data Preprocessing and Quality Control\n",
    "\n",
    "> **üìã Prerequisites:** Complete `01_data_exploration.ipynb` first. This notebook focuses on preprocessing tasks not covered in the exploration phase.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. **Handle missing values** in clinical data using appropriate imputation\n",
    "2. **Normalize gene expression data** for machine learning compatibility  \n",
    "3. **Filter low-variance genes** that are uninformative for modeling\n",
    "4. **Create train/validation/test splits** with proper stratification\n",
    "5. **Set up survival analysis targets** using RFS_MONTHS and RFS_STATUS\n",
    "6. **Export clean datasets** ready for feature selection and modeling\n",
    "\n",
    "### Target Variable Focus\n",
    "This notebook prepares data for **recurrence-free survival (RFS) prediction**:\n",
    "- **RFS_MONTHS**: Time to recurrence or last follow-up (continuous target)\n",
    "- **RFS_STATUS**: Recurrence event indicator (0=no recurrence, 1=recurrence)\n",
    "\n",
    "### Key Preprocessing Steps\n",
    "1. **Load Data**: Import datasets from notebook 1\n",
    "2. **Missing Value Imputation**: Handle clinical data gaps\n",
    "3. **Gene Filtering**: Remove low-variance/uninformative genes\n",
    "4. **Normalization**: Scale data for ML algorithms\n",
    "5. **Target Setup**: Prepare RFS variables for survival analysis\n",
    "6. **Data Splitting**: Create stratified train/val/test sets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c52d23",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries for data preprocessing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38021d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 1: Import Preprocessing Libraries\n",
    "#\n",
    "# Note: Basic libraries (pandas, numpy, matplotlib, seaborn) are already imported from notebook 1\n",
    "#\n",
    "# Your task: Import specialized preprocessing libraries\n",
    "#\n",
    "# TODO: Import preprocessing tools:\n",
    "# 1. From sklearn.preprocessing import: StandardScaler, RobustScaler\n",
    "# 2. From sklearn.impute import: SimpleImputer\n",
    "# 3. From sklearn.model_selection import: train_test_split, StratifiedShuffleSplit\n",
    "# 4. From sklearn.feature_selection import: VarianceThreshold\n",
    "#\n",
    "# TODO: Set environment:\n",
    "# 5. Set random seed: np.random.seed(42)\n",
    "# 6. Configure sklearn random state\n",
    "#\n",
    "# Expected output: Preprocessing libraries imported successfully\n",
    "\n",
    "# Write your code below:\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# import numpy as np\n",
    "# np.random.seed(42)\n",
    "# print(\"‚úì Preprocessing libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d761703",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Validation\n",
    "\n",
    "Before we can preprocess the data, we need to load it and validate its structure based on our exploration findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3368690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 2: Load Data from Notebook 1\n",
    "#\n",
    "# Your task: Load the datasets explored in the previous notebook\n",
    "#\n",
    "# TODO: Load datasets:\n",
    "# 1. Reload expression_data and clinical_data (same as notebook 1)\n",
    "# 2. Use the same file paths and loading code\n",
    "# 3. Verify data shapes match exploration results\n",
    "#\n",
    "# TODO: Focus on RFS target variables:\n",
    "# 4. Check if RFS_MONTHS and RFS_STATUS columns exist in clinical data\n",
    "# 5. Print basic info about RFS variables (range, missing values, distribution)\n",
    "# 6. These will be our primary targets for survival analysis\n",
    "#\n",
    "# Expected output: Datasets loaded with focus on RFS target variables\n",
    "\n",
    "# Write your code below:\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Load data (same as notebook 1)\n",
    "# DATA_PATH = '../data/'\n",
    "# clinical_file = os.path.join(DATA_PATH, 'data_clinical_patient.txt')\n",
    "# expression_file = os.path.join(DATA_PATH, 'data_mrna_illumina_microarray.txt')\n",
    "# \n",
    "# clinical_data = pd.read_csv(clinical_file, sep='\\t', comment='#')\n",
    "# expression_data = pd.read_csv(expression_file, sep='\\t', index_col=0)\n",
    "# if 'Entrez_Gene_Id' in expression_data.columns:\n",
    "#     expression_data = expression_data.drop('Entrez_Gene_Id', axis=1)\n",
    "# \n",
    "# print(f\"‚úì Expression data: {expression_data.shape}\")\n",
    "# print(f\"‚úì Clinical data: {clinical_data.shape}\")\n",
    "# \n",
    "# # Check RFS target variables\n",
    "# if 'RFS_MONTHS' in clinical_data.columns and 'RFS_STATUS' in clinical_data.columns:\n",
    "#     print(f\"‚úì RFS_MONTHS range: {clinical_data['RFS_MONTHS'].min():.1f} - {clinical_data['RFS_MONTHS'].max():.1f}\")\n",
    "#     print(f\"‚úì RFS_STATUS distribution: {clinical_data['RFS_STATUS'].value_counts().to_dict()}\")\n",
    "#     print(f\"‚úì RFS missing values: {clinical_data[['RFS_MONTHS', 'RFS_STATUS']].isnull().sum().to_dict()}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è RFS variables not found - check column names\")\n",
    "#     print(f\"Available columns with 'RFS': {[col for col in clinical_data.columns if 'RFS' in col.upper()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 3: Handle Missing Values in Clinical Data (or exclude)\n",
    "#\n",
    "# Your task: Impute missing values in clinical variables (expression data usually has no missing values)\n",
    "#\n",
    "# TODO: Analyze missing values in clinical data:\n",
    "# 1. Check which clinical variables have missing values\n",
    "# 2. Focus on important variables for survival analysis\n",
    "# 3. Decide imputation strategy for each variable type\n",
    "#\n",
    "# TODO: Implement imputation:\n",
    "# 4. For numerical variables: use median imputation (SimpleImputer with strategy='median')\n",
    "# 5. For categorical variables: use most frequent imputation (strategy='most_frequent')\n",
    "# 6. Apply imputation and create clinical_data_imputed\n",
    "#\n",
    "# TODO: Validate imputation:\n",
    "# 7. Verify no missing values remain\n",
    "# 8. Compare distributions before/after imputation\n",
    "#\n",
    "# Expected output: Clinical data with missing values properly imputed\n",
    "\n",
    "# Write your code below:\n",
    "# # Check missing values\n",
    "# missing_clinical = clinical_data.isnull().sum()\n",
    "# missing_vars = missing_clinical[missing_clinical > 0]\n",
    "# print(f\"Variables with missing values: {len(missing_vars)}\")\n",
    "# if len(missing_vars) > 0:\n",
    "#     print(missing_vars.head(10))\n",
    "# \n",
    "# # Separate numerical and categorical columns\n",
    "# numerical_cols = clinical_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# categorical_cols = clinical_data.select_dtypes(include=['object']).columns.tolist()\n",
    "# \n",
    "# # Create imputed copy\n",
    "# clinical_data_imputed = clinical_data.copy()\n",
    "# \n",
    "# # Impute numerical variables\n",
    "# if len(numerical_cols) > 0:\n",
    "#     num_imputer = SimpleImputer(strategy='median')\n",
    "#     clinical_data_imputed[numerical_cols] = num_imputer.fit_transform(clinical_data_imputed[numerical_cols])\n",
    "# \n",
    "# # Impute categorical variables  \n",
    "# if len(categorical_cols) > 0:\n",
    "#     cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#     clinical_data_imputed[categorical_cols] = cat_imputer.fit_transform(clinical_data_imputed[categorical_cols])\n",
    "# \n",
    "# print(f\"‚úì Missing values after imputation: {clinical_data_imputed.isnull().sum().sum()}\")\n",
    "# print(f\"‚úì Clinical data ready: {clinical_data_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 4: Filter Low-Variance Genes\n",
    "#\n",
    "# Your task: Remove genes with low variance that won't be informative for machine learning\n",
    "#\n",
    "# TODO: Calculate gene variance:\n",
    "# 1. Calculate variance for each gene across all samples\n",
    "# 2. Examine the distribution of gene variances\n",
    "# 3. Set a reasonable threshold (e.g., remove bottom 10% or variance < 0.1)\n",
    "#\n",
    "# TODO: Apply variance filtering:\n",
    "# 4. Use VarianceThreshold from sklearn.feature_selection\n",
    "# 5. Set threshold (e.g., 0.1 or percentile-based)\n",
    "# 6. Fit on transposed expression data (genes as features)\n",
    "# 7. Transform data to remove low-variance genes\n",
    "#\n",
    "# TODO: Validate filtering:\n",
    "# 8. Print number of genes before and after filtering\n",
    "# 9. Show which genes were removed\n",
    "# 10. Verify remaining genes have sufficient variance\n",
    "#\n",
    "# Expected output: Expression data with uninformative genes removed\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"GENE VARIANCE FILTERING\")\n",
    "# print(\"=\"*40)\n",
    "# \n",
    "# # Calculate gene variances\n",
    "# gene_variances = expression_data.var(axis=1)\n",
    "# print(f\"Original genes: {len(gene_variances)}\")\n",
    "# print(f\"Variance range: {gene_variances.min():.4f} - {gene_variances.max():.4f}\")\n",
    "# print(f\"Variance median: {gene_variances.median():.4f}\")\n",
    "# \n",
    "# # Set threshold (remove genes with variance < 0.1)\n",
    "# variance_threshold = 0.1\n",
    "# \n",
    "# # Apply VarianceThreshold\n",
    "# selector = VarianceThreshold(threshold=variance_threshold)\n",
    "# expression_filtered = expression_data.T  # Transpose: samples as rows, genes as columns\n",
    "# expression_filtered = selector.fit_transform(expression_filtered)\n",
    "# expression_filtered = pd.DataFrame(expression_filtered.T,  # Transpose back\n",
    "#                                   index=expression_data.index[selector.get_support()],\n",
    "#                                   columns=expression_data.columns)\n",
    "# \n",
    "# genes_removed = len(expression_data) - len(expression_filtered)\n",
    "# print(f\"‚úì Genes after filtering: {len(expression_filtered)}\")\n",
    "# print(f\"‚úì Genes removed: {genes_removed} ({genes_removed/len(expression_data)*100:.1f}%)\")\n",
    "# print(f\"‚úì Remaining variance range: {expression_filtered.var(axis=1).min():.4f} - {expression_filtered.var(axis=1).max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da39371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 5: Normalize Expression Data (yes or no?)\n",
    "#\n",
    "# Your task: Scale expression data for machine learning compatibility\n",
    "#\n",
    "# TODO: Choose and apply scaling:\n",
    "# 1. Use StandardScaler (mean=0, std=1) or RobustScaler (median-based, outlier-resistant)\n",
    "# 2. Fit scaler on expression data (transpose first: samples as rows)\n",
    "# 3. Transform data and convert back to original format\n",
    "# 4. Keep gene names and sample names\n",
    "#\n",
    "# TODO: Validate normalization:\n",
    "# 5. Check that mean ‚âà 0 and std ‚âà 1 for StandardScaler\n",
    "# 6. Print before/after statistics\n",
    "# 7. Ensure no NaN or infinite values introduced\n",
    "#\n",
    "# Expected output: Normalized expression data ready for machine learning\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"EXPRESSION DATA NORMALIZATION\")\n",
    "# print(\"=\"*40)\n",
    "# \n",
    "# # Print before statistics\n",
    "# print(f\"Before - Mean: {expression_filtered.values.mean():.3f}, Std: {expression_filtered.values.std():.3f}\")\n",
    "# print(f\"Before - Range: {expression_filtered.values.min():.3f} to {expression_filtered.values.max():.3f}\")\n",
    "# \n",
    "# # Apply StandardScaler (samples as rows for sklearn)\n",
    "# scaler = StandardScaler()\n",
    "# expression_scaled = scaler.fit_transform(expression_filtered.T)  # Transpose for sklearn\n",
    "# \n",
    "# # Convert back to DataFrame with original structure\n",
    "# expression_scaled = pd.DataFrame(expression_scaled.T,  # Transpose back\n",
    "#                                 index=expression_filtered.index,\n",
    "#                                 columns=expression_filtered.columns)\n",
    "# \n",
    "# # Validate normalization\n",
    "# print(f\"After - Mean: {expression_scaled.values.mean():.3f}, Std: {expression_scaled.values.std():.3f}\")\n",
    "# print(f\"After - Range: {expression_scaled.values.min():.3f} to {expression_scaled.values.max():.3f}\")\n",
    "# print(f\"‚úì Shape maintained: {expression_scaled.shape}\")\n",
    "# print(f\"‚úì No missing values: {expression_scaled.isnull().sum().sum() == 0}\")\n",
    "# print(f\"‚úì Expression data normalized and ready for ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e615cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 6: Prepare RFS Target Variables\n",
    "#\n",
    "# Your task: Set up RFS_MONTHS and RFS_STATUS for survival analysis and classification\n",
    "#\n",
    "# TODO: Extract and validate RFS variables:\n",
    "# 1. Extract RFS_MONTHS and RFS_STATUS from clinical_data_imputed\n",
    "# 2. Check for any remaining missing values in these key variables\n",
    "# 3. Validate data types and value ranges\n",
    "#\n",
    "# TODO: Create target variables for different analyses:\n",
    "# 4. For survival analysis: keep RFS_MONTHS (time) and RFS_STATUS (event) as is\n",
    "# 5. For classification: create binary outcome (e.g., recurrence within 5 years)\n",
    "# 6. Handle any edge cases (negative times, invalid status codes)\n",
    "#\n",
    "# TODO: Align samples between expression and clinical data:\n",
    "# 7. Find common samples between expression_scaled and clinical_data_imputed\n",
    "# 8. Filter both datasets to include only matching samples\n",
    "# 9. Ensure sample order is consistent\n",
    "#\n",
    "# Expected output: Aligned datasets with properly formatted RFS target variables\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"RFS TARGET VARIABLE PREPARATION\")\n",
    "# print(\"=\"*40)\n",
    "# \n",
    "# # Extract RFS variables\n",
    "# if 'RFS_MONTHS' in clinical_data_imputed.columns and 'RFS_STATUS' in clinical_data_imputed.columns:\n",
    "#     rfs_months = clinical_data_imputed['RFS_MONTHS'].copy()\n",
    "#     rfs_status = clinical_data_imputed['RFS_STATUS'].copy()\n",
    "#     \n",
    "#     print(f\"RFS_MONTHS - Missing: {rfs_months.isnull().sum()}, Range: {rfs_months.min():.1f}-{rfs_months.max():.1f}\")\n",
    "#     print(f\"RFS_STATUS - Missing: {rfs_status.isnull().sum()}, Values: {rfs_status.value_counts().to_dict()}\")\n",
    "#     \n",
    "#     # Create binary classification target (recurrence within 60 months/5 years)\n",
    "#     recurrence_5yr = ((rfs_months <= 60) & (rfs_status == 1)).astype(int)\n",
    "#     print(f\"5-year recurrence rate: {recurrence_5yr.mean():.3f} ({recurrence_5yr.sum()}/{len(recurrence_5yr)})\")\n",
    "#     \n",
    "#     # Align samples (assuming PATIENT_ID or similar identifier exists)\n",
    "#     # For now, use index alignment - adjust based on actual sample ID column\n",
    "#     common_samples = list(set(expression_scaled.columns) & set(clinical_data_imputed.index))\n",
    "#     if len(common_samples) == 0:\n",
    "#         # Try with PATIENT_ID if available\n",
    "#         print(\"‚ö†Ô∏è No direct index match - check sample ID alignment\")\n",
    "#         print(f\"Expression samples (first 5): {list(expression_scaled.columns[:5])}\")\n",
    "#         print(f\"Clinical samples (first 5): {list(clinical_data_imputed.index[:5])}\")\n",
    "#     else:\n",
    "#         print(f\"‚úì Common samples found: {len(common_samples)}\")\n",
    "# \n",
    "# else:\n",
    "#     print(\"‚ùå RFS variables not found in clinical data\")\n",
    "#     print(\"Available columns:\", [col for col in clinical_data_imputed.columns if any(x in col.upper() for x in ['RFS', 'RECUR', 'SURV', 'STATUS', 'MONTH'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2beadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 7: Create Train/Validation/Test Splits\n",
    "#\n",
    "# Your task: Split data into training, validation, and test sets with proper stratification\n",
    "#\n",
    "# TODO: Prepare data for splitting:\n",
    "# 1. Ensure expression_scaled and target variables are aligned\n",
    "# 2. Create stratification variable (use RFS_STATUS or 5-year recurrence)\n",
    "# 3. Handle any remaining data alignment issues\n",
    "#\n",
    "# TODO: Create splits:\n",
    "# 4. Use train_test_split to create 70% train, 30% temp\n",
    "# 5. Split temp into 15% validation, 15% test\n",
    "# 6. Use stratify parameter to maintain class balance\n",
    "# 7. Set random_state=42 for reproducibility\n",
    "#\n",
    "# TODO: Validate splits:\n",
    "# 8. Check sizes of train/val/test sets\n",
    "# 9. Verify class distributions are maintained\n",
    "# 10. Ensure no data leakage between sets\n",
    "#\n",
    "# Expected output: Balanced train/val/test splits ready for feature selection and modeling\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"DATA SPLITTING\")\n",
    "# print(\"=\"*30)\n",
    "# \n",
    "# # Prepare aligned data (adjust sample alignment as needed)\n",
    "# # This assumes we have aligned samples - modify based on actual data structure\n",
    "# if 'common_samples' in locals() and len(common_samples) > 0:\n",
    "#     # Use aligned samples\n",
    "#     X = expression_scaled[common_samples].T  # Samples as rows\n",
    "#     y_survival = clinical_data_imputed.loc[common_samples, ['RFS_MONTHS', 'RFS_STATUS']]\n",
    "#     y_binary = recurrence_5yr[clinical_data_imputed.index.isin(common_samples)]\n",
    "#     \n",
    "#     print(f\"Total samples for splitting: {len(X)}\")\n",
    "#     print(f\"Features (genes): {X.shape[1]}\")\n",
    "#     print(f\"Class distribution: {y_binary.value_counts().to_dict()}\")\n",
    "#     \n",
    "#     # Create stratified splits\n",
    "#     X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "#         X, y_binary, test_size=0.15, random_state=42, stratify=y_binary\n",
    "#     )\n",
    "#     \n",
    "#     X_train, X_val, y_train, y_val = train_test_split(\n",
    "#         X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ‚âà 0.15\n",
    "#     )\n",
    "#     \n",
    "#     print(f\"‚úì Train set: {X_train.shape} (class dist: {y_train.value_counts().to_dict()})\")\n",
    "#     print(f\"‚úì Val set: {X_val.shape} (class dist: {y_val.value_counts().to_dict()})\")\n",
    "#     print(f\"‚úì Test set: {X_test.shape} (class dist: {y_test.value_counts().to_dict()})\")\n",
    "#     \n",
    "# else:\n",
    "#     print(\"‚ùå Cannot create splits - sample alignment needed\")\n",
    "#     print(\"Fix sample alignment in previous activity first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de75ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 8: Export Preprocessed Data\n",
    "#\n",
    "# Your task: Save cleaned and split datasets for feature selection and modeling\n",
    "#\n",
    "# TODO: Create results directory and save datasets:\n",
    "# 1. Create '../results/preprocessing/' directory\n",
    "# 2. Save train/val/test splits as CSV files\n",
    "# 3. Save both expression data (X) and target variables (y)\n",
    "# 4. Include both survival targets (RFS_MONTHS, RFS_STATUS) and binary target\n",
    "#\n",
    "# TODO: Save preprocessing objects:\n",
    "# 5. Save scaler and other preprocessing objects using joblib\n",
    "# 6. Create preprocessing summary with key statistics\n",
    "#\n",
    "# Expected output: All preprocessed data saved and ready for next notebook\n",
    "\n",
    "# Write your code below:\n",
    "# import os\n",
    "# import joblib\n",
    "# \n",
    "# # Create results directory\n",
    "# os.makedirs('../results/preprocessing', exist_ok=True)\n",
    "# \n",
    "# if 'X_train' in locals():\n",
    "#     # Save expression data splits\n",
    "#     pd.DataFrame(X_train).to_csv('../results/preprocessing/X_train_scaled.csv')\n",
    "#     pd.DataFrame(X_val).to_csv('../results/preprocessing/X_val_scaled.csv') \n",
    "#     pd.DataFrame(X_test).to_csv('../results/preprocessing/X_test_scaled.csv')\n",
    "#     \n",
    "#     # Save target variables\n",
    "#     pd.DataFrame({'rfs_status': y_train}).to_csv('../results/preprocessing/y_train.csv')\n",
    "#     pd.DataFrame({'rfs_status': y_val}).to_csv('../results/preprocessing/y_val.csv')\n",
    "#     pd.DataFrame({'rfs_status': y_test}).to_csv('../results/preprocessing/y_test.csv')\n",
    "#     \n",
    "#     # Save preprocessing objects\n",
    "#     joblib.dump(scaler, '../results/preprocessing/expression_scaler.pkl')\n",
    "#     \n",
    "#     # Create summary\n",
    "#     preprocessing_summary = {\n",
    "#         'original_genes': len(expression_data),\n",
    "#         'filtered_genes': len(expression_filtered),\n",
    "#         'final_genes': X_train.shape[1],\n",
    "#         'train_samples': len(X_train),\n",
    "#         'val_samples': len(X_val),\n",
    "#         'test_samples': len(X_test),\n",
    "#         'target_variable': 'RFS_STATUS (5-year recurrence)',\n",
    "#         'missing_values_imputed': True,\n",
    "#         'normalization': 'StandardScaler'\n",
    "#     }\n",
    "#     \n",
    "#     import json\n",
    "#     with open('../results/preprocessing/preprocessing_summary.json', 'w') as f:\n",
    "#         json.dump(preprocessing_summary, f, indent=2)\n",
    "#     \n",
    "#     print(\"‚úì All datasets exported to ../results/preprocessing/\")\n",
    "#     print(\"‚úì Ready for feature selection (notebook 3)\")\n",
    "#     \n",
    "# else:\n",
    "#     print(\"‚ùå No splits to export - complete previous activities first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80d51f",
   "metadata": {},
   "source": [
    "## 7. Export Preprocessed Data\n",
    "\n",
    "Let's save our preprocessed datasets and create comprehensive documentation for the next modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb6956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Preprocessing Summary\n",
    "\n",
    "### ‚úÖ **What You Accomplished:**\n",
    "1. **Library Import**: Added preprocessing-specific libraries (sklearn tools)\n",
    "2. **Data Loading**: Loaded datasets with focus on RFS target variables\n",
    "3. **Missing Value Handling**: Imputed missing clinical data appropriately  \n",
    "4. **Gene Filtering**: Removed low-variance genes uninformative for ML\n",
    "5. **Normalization**: Standardized expression data for ML compatibility\n",
    "6. **Target Preparation**: Set up RFS_MONTHS and RFS_STATUS for survival analysis\n",
    "7. **Data Splitting**: Created stratified train/val/test splits (70/15/15)\n",
    "8. **Export**: Saved preprocessed datasets for feature selection\n",
    "\n",
    "### üéØ **Key Focus - RFS Target Variables:**\n",
    "- **RFS_MONTHS**: Time to recurrence or last follow-up (continuous)\n",
    "- **RFS_STATUS**: Recurrence event indicator (0=no event, 1=recurrence)\n",
    "- **5-Year Classification**: Binary target for recurrence within 5 years\n",
    "\n",
    "### üìä **Data Ready for Feature Selection:**\n",
    "- **Clean expression data**: Normalized, low-variance genes removed\n",
    "- **Imputed clinical data**: No missing values\n",
    "- **Survival targets**: RFS variables ready for time-to-event modeling\n",
    "- **Balanced splits**: Stratified train/val/test maintaining class distribution\n",
    "\n",
    "### üîÑ **Next Steps:**\n",
    "Run `03_feature_selection.ipynb` to select optimal features for RFS prediction modeling.\n",
    "\n",
    "---\n",
    "\n",
    "**Great job preprocessing the data for survival analysis! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
