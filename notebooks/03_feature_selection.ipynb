{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddae1e39",
   "metadata": {},
   "source": [
    "# Machine Learning Course: Feature Selection\n",
    "\n",
    "## Notebook 3: Advanced Feature Selection for Risk Classification\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Apply variance-based feature selection to remove uninformative genes\n",
    "2. Use correlation-based methods to identify relevant and redundant features\n",
    "3. Implement Cox proportional hazards models for survival-based feature ranking\n",
    "4. Apply Random Forest feature importance for non-linear feature selection\n",
    "5. Use Boruta algorithm for all-relevant feature detection\n",
    "6. Compare and ensemble multiple feature selection methods\n",
    "7. Create optimal feature subsets for different modeling approaches\n",
    "8. Validate feature selection stability and biological relevance\n",
    "\n",
    "### Prerequisites\n",
    "- Completed `01_data_exploration.ipynb` and `02_data_preprocessing.ipynb`\n",
    "- Understanding of statistical concepts (correlation, variance, p-values)\n",
    "- Basic knowledge of survival analysis and ensemble methods\n",
    "\n",
    "### Feature Selection Methods Overview\n",
    "This notebook implements multiple complementary feature selection approaches:\n",
    "\n",
    "**1. Variance-Based Selection**\n",
    "- Statistical variance analysis\n",
    "- VarianceThreshold filtering\n",
    "- Coefficient of variation analysis\n",
    "\n",
    "**2. Correlation-Based Selection**\n",
    "- Pearson/Spearman correlation with target\n",
    "- Feature-feature correlation for redundancy removal\n",
    "- Mutual information analysis\n",
    "\n",
    "**3. Cox Regression-Based Selection**\n",
    "- Univariate Cox proportional hazards models\n",
    "- Hazard ratio analysis\n",
    "- Survival-based p-value ranking\n",
    "\n",
    "**4. Random Forest-Based Selection**\n",
    "- Tree-based feature importance\n",
    "- Permutation importance\n",
    "- Recursive feature elimination with RF\n",
    "\n",
    "**5. Boruta Algorithm**\n",
    "- All-relevant feature detection\n",
    "- Shadow feature comparison\n",
    "- Statistical significance testing\n",
    "\n",
    "**6. Ensemble Selection**\n",
    "- Method agreement analysis\n",
    "- Stability assessment\n",
    "- Final feature set optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26048c",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's import all necessary libraries for advanced feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 1: Import Libraries for Advanced Feature Selection\n",
    "#\n",
    "# Your task: Import comprehensive libraries for multiple feature selection methods\n",
    "#\n",
    "# TODO: Import core data manipulation libraries:\n",
    "# 1. pandas (as pd) - for data manipulation and analysis\n",
    "# 2. numpy (as np) - for numerical operations and array handling\n",
    "# 3. matplotlib.pyplot (as plt) - for plotting and visualization\n",
    "# 4. seaborn (as sns) - for statistical visualization\n",
    "# 5. os, warnings - for system operations and warning control\n",
    "#\n",
    "# TODO: Import feature selection libraries:\n",
    "# 6. From sklearn.feature_selection import:\n",
    "#    - VarianceThreshold (variance-based selection)\n",
    "#    - SelectKBest, f_classif, chi2 (statistical selection)\n",
    "#    - RFE, RFECV (recursive feature elimination)\n",
    "#    - mutual_info_classif (mutual information)\n",
    "# 7. From sklearn.ensemble import: RandomForestClassifier\n",
    "# 8. From sklearn.preprocessing import: StandardScaler, LabelEncoder\n",
    "#\n",
    "# TODO: Import survival analysis libraries:\n",
    "# 9. Try to import lifelines:\n",
    "#    - from lifelines import CoxPHFitter (Cox proportional hazards)\n",
    "#    - from lifelines.statistics import logrank_test\n",
    "# 10. If lifelines not available, note alternative approaches\n",
    "#\n",
    "# TODO: Import Boruta library:\n",
    "# 11. Try to import boruta:\n",
    "#    - from boruta import BorutaPy (Boruta feature selection)\n",
    "# 12. If not available, provide installation instructions\n",
    "#\n",
    "# TODO: Import additional statistical libraries:\n",
    "# 13. From scipy.stats import: pearsonr, spearmanr, chi2_contingency\n",
    "# 14. From scipy import stats\n",
    "# 15. import joblib - for saving/loading objects\n",
    "#\n",
    "# TODO: Configure environment:\n",
    "# 16. Suppress warnings: warnings.filterwarnings('ignore')\n",
    "# 17. Set matplotlib style and seaborn palette\n",
    "# 18. Set random seeds for reproducibility (np.random.seed(42))\n",
    "# 19. Set pandas display options for better output\n",
    "#\n",
    "# TODO: Check library availability and versions:\n",
    "# 20. Print versions of key libraries\n",
    "# 21. Test import success for optional libraries (lifelines, boruta)\n",
    "# 22. Provide installation instructions for missing libraries\n",
    "#\n",
    "# Expected output: Successfully imported libraries with version info and availability status\n",
    "\n",
    "# Write your code below:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c635d7",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Let's load the clean, preprocessed data from the previous notebook and validate it for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7611c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 2: Load and Validate Preprocessed Data\n",
    "#\n",
    "# Your task: Load the preprocessed datasets and validate them for feature selection\n",
    "#\n",
    "# TODO: Set up data paths:\n",
    "# 1. Define PREPROCESSED_PATH = '../results/preprocessed/'\n",
    "# 2. Define FEATURE_SELECTION_PATH = '../results/feature_selection/'\n",
    "# 3. Create feature selection results directory: os.makedirs(FEATURE_SELECTION_PATH, exist_ok=True)\n",
    "#\n",
    "# TODO: Load training data for feature selection:\n",
    "# 4. Load training features: X_train = pd.read_csv(PREPROCESSED_PATH + 'splits/X_train.csv', index_col=0)\n",
    "# 5. Load training target: y_train = pd.read_csv(PREPROCESSED_PATH + 'splits/y_train.csv', index_col=0)['target']\n",
    "# 6. Load validation data: X_val, y_val (for stability testing)\n",
    "# 7. Handle any loading errors with try-except blocks\n",
    "#\n",
    "# TODO: Load additional preprocessed data:\n",
    "# 8. Load normalized expression data: expression_data = pd.read_csv(PREPROCESSED_PATH + 'data/expression_normalized.csv', index_col=0)\n",
    "# 9. Load processed clinical data: clinical_data = pd.read_csv(PREPROCESSED_PATH + 'data/clinical_processed.csv', index_col=0)\n",
    "# 10. Load preprocessing summary: json.load(open(PREPROCESSED_PATH + 'preprocessing_summary.json'))\n",
    "#\n",
    "# TODO: Validate data for feature selection:\n",
    "# 11. Print \"DATA VALIDATION FOR FEATURE SELECTION\" header\n",
    "# 12. Check data dimensions and ensure they match expectations\n",
    "# 13. Verify no missing values: X_train.isnull().sum().sum() == 0\n",
    "# 14. Check target variable distribution: y_train.value_counts()\n",
    "# 15. Confirm data types are appropriate for analysis\n",
    "#\n",
    "# TODO: Separate expression and clinical features:\n",
    "# 16. If X_train contains both types, separate them:\n",
    "#     - Identify which columns are expression features (gene names)\n",
    "#     - Identify which columns are clinical features\n",
    "#     - Create expression_features and clinical_features lists\n",
    "# 17. Print counts of each feature type\n",
    "#\n",
    "# TODO: Create comprehensive dataset overview:\n",
    "# 18. Print dataset dimensions: samples, total features, expression features, clinical features\n",
    "# 19. Display data types and memory usage\n",
    "# 20. Show class balance in training set\n",
    "# 21. Confirm data is ready for feature selection analysis\n",
    "#\n",
    "# TODO: Prepare survival data if available:\n",
    "# 22. Check if survival time and event data are available in clinical data\n",
    "# 23. If available, prepare survival DataFrame for Cox regression\n",
    "# 24. Handle missing survival data appropriately\n",
    "#\n",
    "# Expected output: Loaded and validated datasets with feature type identification\n",
    "\n",
    "# Write your code below:\n",
    "# PREPROCESSED_PATH = '../results/preprocessed/'\n",
    "# FEATURE_SELECTION_PATH = '../results/feature_selection/'\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8a4db",
   "metadata": {},
   "source": [
    "## 3. Variance-Based Feature Selection\n",
    "\n",
    "Remove features with low variance that provide little discriminative power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5662587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 3: Advanced Variance-Based Feature Selection\n",
    "#\n",
    "# Your task: Apply sophisticated variance analysis to identify most informative features\n",
    "#\n",
    "# TODO: Set up variance analysis:\n",
    "# 1. Print \"VARIANCE-BASED FEATURE SELECTION\" header with separators\n",
    "# 2. Create working copies: X_variance = X_train.copy()\n",
    "# 3. Store original feature count: original_feature_count = X_variance.shape[1]\n",
    "#\n",
    "# TODO: Calculate comprehensive variance statistics:\n",
    "# 4. Calculate feature variances: feature_variances = X_variance.var()\n",
    "# 5. Calculate coefficient of variation: feature_cv = X_variance.std() / X_variance.mean().abs()\n",
    "# 6. Handle division by zero: feature_cv = feature_cv.fillna(0)\n",
    "# 7. Calculate variance percentiles: np.percentile(feature_variances, [10, 25, 50, 75, 90])\n",
    "#\n",
    "# TODO: Apply VarianceThreshold filtering:\n",
    "# 8. Create VarianceThreshold selector with threshold=0.05:\n",
    "#    - selector = VarianceThreshold(threshold=0.05)\n",
    "#    - selector.fit(X_variance)\n",
    "# 9. Get selected features: selected_features_mask = selector.get_support()\n",
    "# 10. Apply selection: X_variance_filtered = X_variance.loc[:, selected_features_mask]\n",
    "# 11. Get removed features: removed_features = X_variance.columns[~selected_features_mask]\n",
    "#\n",
    "# TODO: Apply coefficient of variation filtering:\n",
    "# 12. Set CV threshold (e.g., 0.1 for meaningful variation)\n",
    "# 13. Identify high-variation features: high_cv_features = feature_cv[feature_cv > 0.1].index\n",
    "# 14. Combine with variance filtering: final_variance_features = X_variance_filtered.columns.intersection(high_cv_features)\n",
    "#\n",
    "# TODO: Analyze variance distribution by feature type:\n",
    "# 15. If expression and clinical features are separable:\n",
    "#     - Calculate variance statistics separately for each type\n",
    "#     - Compare variance distributions between feature types\n",
    "#     - Apply type-specific thresholds if appropriate\n",
    "#\n",
    "# TODO: Create variance-based feature ranking:\n",
    "# 16. Rank features by variance: variance_ranking = feature_variances.sort_values(ascending=False)\n",
    "# 17. Rank features by CV: cv_ranking = feature_cv.sort_values(ascending=False)\n",
    "# 18. Create combined ranking: combined_score = (variance_ranking.rank() + cv_ranking.rank()) / 2\n",
    "#\n",
    "# TODO: Visualize variance analysis results:\n",
    "# 19. Create histogram of feature variances\n",
    "# 20. Create scatter plot of variance vs CV\n",
    "# 21. Show distribution of removed vs retained features\n",
    "# 22. Create before/after comparison\n",
    "#\n",
    "# TODO: Generate variance selection summary:\n",
    "# 23. Calculate features removed: removed_count = len(removed_features)\n",
    "# 24. Calculate removal percentage: (removed_count / original_feature_count) * 100\n",
    "# 25. Print summary statistics: original count, final count, removal stats\n",
    "# 26. Show top 10 highest and lowest variance features\n",
    "#\n",
    "# TODO: Save variance selection results:\n",
    "# 27. Save selected features list: variance_selected_features\n",
    "# 28. Save variance statistics and rankings\n",
    "# 29. Create variance selection report\n",
    "#\n",
    "# Expected output: Variance-filtered feature set with comprehensive analysis and rankings\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"VARIANCE-BASED FEATURE SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb508c49",
   "metadata": {},
   "source": [
    "## 4. Correlation-Based Feature Selection\n",
    "\n",
    "Identify features that are highly correlated with the target and remove redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f62e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 4: Comprehensive Correlation-Based Feature Selection\n",
    "#\n",
    "# Your task: Use correlation analysis to identify relevant features and remove redundant ones\n",
    "#\n",
    "# TODO: Set up correlation analysis:\n",
    "# 1. Print \"CORRELATION-BASED FEATURE SELECTION\" header with separators\n",
    "# 2. Create working copy: X_correlation = X_train.copy()\n",
    "# 3. Ensure target variable is numeric: y_numeric = y_train.astype(int) if needed\n",
    "#\n",
    "# TODO: Calculate feature-target correlations:\n",
    "# 4. Calculate Pearson correlation with target:\n",
    "#    - pearson_corrs = X_correlation.apply(lambda x: pearsonr(x, y_numeric)[0])\n",
    "#    - pearson_pvals = X_correlation.apply(lambda x: pearsonr(x, y_numeric)[1])\n",
    "# 5. Calculate Spearman correlation (non-parametric):\n",
    "#    - spearman_corrs = X_correlation.apply(lambda x: spearmanr(x, y_numeric)[0])\n",
    "#    - spearman_pvals = X_correlation.apply(lambda x: spearmanr(x, y_numeric)[1])\n",
    "# 6. Handle any NaN values in correlations\n",
    "#\n",
    "# TODO: Apply mutual information analysis:\n",
    "# 7. Calculate mutual information scores:\n",
    "#    - mi_scores = mutual_info_classif(X_correlation, y_numeric, random_state=42)\n",
    "#    - mi_scores = pd.Series(mi_scores, index=X_correlation.columns)\n",
    "# 8. Normalize MI scores: mi_scores_norm = mi_scores / mi_scores.max()\n",
    "#\n",
    "# TODO: Create correlation-based rankings:\n",
    "# 9. Rank by absolute Pearson correlation: abs_pearson_ranking = pearson_corrs.abs().sort_values(ascending=False)\n",
    "# 10. Rank by absolute Spearman correlation: abs_spearman_ranking = spearman_corrs.abs().sort_values(ascending=False)\n",
    "# 11. Rank by mutual information: mi_ranking = mi_scores.sort_values(ascending=False)\n",
    "# 12. Create combined correlation score: combined_corr_score = (abs_pearson_ranking.rank() + mi_ranking.rank()) / 2\n",
    "#\n",
    "# TODO: Select highly correlated features:\n",
    "# 13. Set correlation thresholds:\n",
    "#     - pearson_threshold = 0.1 (absolute correlation)\n",
    "#     - mi_threshold = 0.1 (normalized MI score)\n",
    "#     - p_value_threshold = 0.05\n",
    "# 14. Select significant Pearson correlations: pearson_selected = abs_pearson_ranking[(abs_pearson_ranking > pearson_threshold) & (pearson_pvals < p_value_threshold)].index\n",
    "# 15. Select high MI features: mi_selected = mi_ranking[mi_ranking > mi_threshold].index\n",
    "# 16. Combine selections: correlation_selected_features = set(pearson_selected) | set(mi_selected)\n",
    "#\n",
    "# TODO: Remove redundant features (feature-feature correlation):\n",
    "# 17. Calculate feature-feature correlation matrix: feature_corr_matrix = X_correlation[list(correlation_selected_features)].corr()\n",
    "# 18. Find highly correlated feature pairs: high_corr_pairs = np.where((feature_corr_matrix.abs() > 0.8) & (feature_corr_matrix.abs() < 1.0))\n",
    "# 19. For each highly correlated pair, keep the one with higher target correlation\n",
    "# 20. Create final correlation-based feature set: final_correlation_features\n",
    "#\n",
    "# TODO: Analyze correlation patterns by feature type:\n",
    "# 21. If expression and clinical features are separable:\n",
    "#     - Compare correlation distributions between feature types\n",
    "#     - Identify which feature type shows stronger correlations\n",
    "#     - Apply type-specific correlation thresholds if needed\n",
    "#\n",
    "# TODO: Visualize correlation analysis:\n",
    "# 22. Create histogram of feature-target correlations\n",
    "# 23. Create scatter plot of Pearson vs Spearman correlations\n",
    "# 24. Plot mutual information distribution\n",
    "# 25. Show top correlated features in each category\n",
    "#\n",
    "# TODO: Generate correlation selection summary:\n",
    "# 26. Count features selected by each method\n",
    "# 27. Calculate overlap between methods\n",
    "# 28. Print top 20 features by each correlation measure\n",
    "# 29. Show correlation statistics summary\n",
    "#\n",
    "# TODO: Save correlation results:\n",
    "# 30. Save correlation matrices and rankings\n",
    "# 31. Export correlation-selected features list\n",
    "# 32. Create correlation analysis report\n",
    "#\n",
    "# Expected output: Correlation-based feature selection with redundancy removal and method comparison\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"CORRELATION-BASED FEATURE SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c91bf",
   "metadata": {},
   "source": [
    "## 5. Cox Regression-Based Feature Selection\n",
    "\n",
    "Use survival analysis to identify genes associated with patient outcomes and survival risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588be03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 5: Cox Proportional Hazards Feature Selection\n",
    "#\n",
    "# Your task: Use survival analysis to identify features most associated with patient risk and outcomes\n",
    "#\n",
    "# TODO: Set up Cox regression analysis:\n",
    "# 1. Print \"COX REGRESSION-BASED FEATURE SELECTION\" header with separators\n",
    "# 2. Check if survival data is available in clinical data\n",
    "# 3. If lifelines is available, proceed with Cox analysis\n",
    "# 4. If not available, implement alternative survival-based ranking\n",
    "#\n",
    "# TODO: Prepare survival data:\n",
    "# 5. Create survival DataFrame with required columns:\n",
    "#    - duration: survival time (OS_MONTHS if available)\n",
    "#    - event: event indicator (1 if deceased, 0 if censored)\n",
    "#    - If survival data not available, create proxy using target variable\n",
    "# 6. Align survival data with feature matrix\n",
    "# 7. Handle missing survival times appropriately\n",
    "#\n",
    "# TODO: Implement univariate Cox regression (if lifelines available):\n",
    "# 8. For each feature, fit univariate Cox model:\n",
    "#    - cox_results = {}\n",
    "#    - for feature in X_train.columns:\n",
    "#    -     survival_df = survival_data.copy()\n",
    "#    -     survival_df[feature] = X_train[feature]\n",
    "#    -     cph = CoxPHFitter()\n",
    "#    -     cph.fit(survival_df, duration_col='duration', event_col='event')\n",
    "#    -     cox_results[feature] = {'hazard_ratio': cph.hazard_ratios_[feature], 'p_value': cph.summary.p[feature]}\n",
    "#\n",
    "# TODO: Alternative Cox-like analysis (if lifelines not available):\n",
    "# 9. Use logistic regression with time-to-event as proxy:\n",
    "#    - Create risk groups based on target variable and time\n",
    "#    - Use statistical tests (t-test, Mann-Whitney) to rank features\n",
    "#    - Calculate effect sizes for each feature\n",
    "#\n",
    "# TODO: Extract Cox regression results:\n",
    "# 10. Extract hazard ratios: hazard_ratios = pd.Series({f: cox_results[f]['hazard_ratio'] for f in cox_results})\n",
    "# 11. Extract p-values: cox_pvalues = pd.Series({f: cox_results[f]['p_value'] for f in cox_results})\n",
    "# 12. Calculate log hazard ratios: log_hazard_ratios = np.log(hazard_ratios.abs())\n",
    "# 13. Handle infinite values and NaNs appropriately\n",
    "#\n",
    "# TODO: Create Cox-based feature rankings:\n",
    "# 14. Rank by hazard ratio magnitude: hr_ranking = hazard_ratios.abs().sort_values(ascending=False)\n",
    "# 15. Rank by statistical significance: pval_ranking = cox_pvalues.sort_values()\n",
    "# 16. Create combined Cox score: cox_score = -np.log10(cox_pvalues) * np.log(hazard_ratios.abs())\n",
    "# 17. Handle mathematical edge cases (zero p-values, etc.)\n",
    "#\n",
    "# TODO: Select Cox-significant features:\n",
    "# 18. Set significance thresholds:\n",
    "#     - p_value_threshold = 0.05\n",
    "#     - hazard_ratio_threshold = 1.2 (20% increase/decrease in hazard)\n",
    "# 19. Select significant features: cox_significant = cox_pvalues[(cox_pvalues < p_value_threshold) & (hazard_ratios.abs() > hazard_ratio_threshold)].index\n",
    "# 20. Rank selected features by combined Cox score\n",
    "#\n",
    "# TODO: Analyze Cox results by feature type:\n",
    "# 21. If expression and clinical features are separable:\n",
    "#     - Compare hazard ratio distributions between feature types\n",
    "#     - Identify which feature type has more significant associations\n",
    "#     - Note any biological patterns in significant genes\n",
    "#\n",
    "# TODO: Handle Cox regression limitations:\n",
    "# 22. Check proportional hazards assumption (if possible)\n",
    "# 23. Identify features with very extreme hazard ratios (potential outliers)\n",
    "# 24. Validate results using alternative survival analysis methods\n",
    "#\n",
    "# TODO: Visualize Cox analysis results:\n",
    "# 25. Create volcano plot: -log10(p-value) vs log(hazard ratio)\n",
    "# 26. Create histogram of hazard ratios\n",
    "# 27. Show distribution of p-values\n",
    "# 28. Plot top significant features\n",
    "#\n",
    "# TODO: Generate Cox selection summary:\n",
    "# 29. Count features significant at different p-value thresholds\n",
    "# 30. Show distribution of hazard ratios for significant features\n",
    "# 31. Print top 20 features by Cox score\n",
    "# 32. Create Cox analysis interpretation guide\n",
    "#\n",
    "# TODO: Save Cox regression results:\n",
    "# 33. Save cox_results dictionary\n",
    "# 34. Export Cox-selected features list\n",
    "# 35. Create Cox analysis report with interpretation\n",
    "#\n",
    "# Expected output: Cox regression-based feature selection with survival analysis insights\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"COX REGRESSION-BASED FEATURE SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# # Check if lifelines is available\n",
    "# try:\n",
    "#     from lifelines import CoxPHFitter\n",
    "#     lifelines_available = True\n",
    "#     print(\"âœ“ Lifelines available - using Cox proportional hazards models\")\n",
    "# except ImportError:\n",
    "#     lifelines_available = False\n",
    "#     print(\"âš ï¸  Lifelines not available - using alternative survival-based ranking\")\n",
    "#     print(\"To install: pip install lifelines\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c052ff",
   "metadata": {},
   "source": [
    "## 6. Random Forest-Based Feature Selection\n",
    "\n",
    "Use tree-based ensemble methods to identify important features through multiple importance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 6: Advanced Random Forest Feature Selection\n",
    "#\n",
    "# Your task: Use multiple Random Forest approaches to identify important features\n",
    "#\n",
    "# TODO: Set up Random Forest analysis:\n",
    "# 1. Print \"RANDOM FOREST-BASED FEATURE SELECTION\" header with separators\n",
    "# 2. Create working copy: X_rf = X_train.copy()\n",
    "# 3. Set up Random Forest parameters: n_estimators=100, random_state=42, n_jobs=-1\n",
    "#\n",
    "# TODO: Train Random Forest for feature importance:\n",
    "# 4. Create RandomForestClassifier: rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# 5. Fit the model: rf.fit(X_rf, y_train)\n",
    "# 6. Extract feature importances: rf_importances = pd.Series(rf.feature_importances_, index=X_rf.columns)\n",
    "# 7. Sort by importance: rf_ranking = rf_importances.sort_values(ascending=False)\n",
    "#\n",
    "# TODO: Implement permutation importance:\n",
    "# 8. Use permutation_importance from sklearn.inspection (if available):\n",
    "#    - from sklearn.inspection import permutation_importance\n",
    "#    - perm_importance = permutation_importance(rf, X_rf, y_train, n_repeats=10, random_state=42)\n",
    "#    - perm_importances = pd.Series(perm_importance.importances_mean, index=X_rf.columns)\n",
    "# 9. If not available, implement manual permutation importance\n",
    "# 10. Sort permutation importances: perm_ranking = perm_importances.sort_values(ascending=False)\n",
    "#\n",
    "# TODO: Apply Recursive Feature Elimination with Cross-Validation:\n",
    "# 11. Create RFECV selector: rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=50, random_state=42), step=1, cv=5, scoring='accuracy')\n",
    "# 12. Fit RFECV: rfecv.fit(X_rf, y_train)\n",
    "# 13. Get selected features: rfecv_selected = X_rf.columns[rfecv.support_]\n",
    "# 14. Get feature rankings: rfecv_rankings = pd.Series(rfecv.ranking_, index=X_rf.columns)\n",
    "#\n",
    "# TODO: Create ensemble Random Forest importance:\n",
    "# 15. Train multiple RF models with different parameters:\n",
    "#     - Different n_estimators: [50, 100, 200]\n",
    "#     - Different max_features: ['sqrt', 'log2', 0.3]\n",
    "#     - Different max_depth: [None, 10, 20]\n",
    "# 16. Average importances across all models: ensemble_importance\n",
    "# 17. Calculate importance stability (standard deviation across models)\n",
    "#\n",
    "# TODO: Analyze feature importance distributions:\n",
    "# 18. Calculate importance percentiles and thresholds\n",
    "# 19. Compare importance distributions between feature types (expression vs clinical)\n",
    "# 20. Identify top features consistently selected across methods\n",
    "#\n",
    "# TODO: Apply importance-based selection:\n",
    "# 21. Set importance thresholds:\n",
    "#     - rf_threshold = np.percentile(rf_importances, 75)  # Top 25% of features\n",
    "#     - perm_threshold = np.percentile(perm_importances, 75)\n",
    "# 22. Select high-importance features: rf_selected = rf_ranking[rf_ranking > rf_threshold].index\n",
    "# 23. Select high permutation importance: perm_selected = perm_ranking[perm_ranking > perm_threshold].index\n",
    "# 24. Combine selections: rf_final_features = set(rf_selected) | set(perm_selected) | set(rfecv_selected)\n",
    "#\n",
    "# TODO: Validate Random Forest selections:\n",
    "# 25. Check feature selection stability by training RF on different subsets\n",
    "# 26. Calculate feature selection frequency across multiple runs\n",
    "# 27. Identify most stable/consistent features\n",
    "#\n",
    "# TODO: Visualize Random Forest results:\n",
    "# 28. Create feature importance bar plots for top 20 features\n",
    "# 29. Create scatter plot comparing RF importance vs permutation importance\n",
    "# 30. Plot RFECV cross-validation scores\n",
    "# 31. Show importance distributions by feature type\n",
    "#\n",
    "# TODO: Analyze biological relevance (if expression features):\n",
    "# 32. If gene names are available, look up biological functions of top genes\n",
    "# 33. Check if selected genes are known biomarkers\n",
    "# 34. Identify any pathway enrichment patterns\n",
    "#\n",
    "# TODO: Generate RF selection summary:\n",
    "# 35. Count features selected by each RF method\n",
    "# 36. Calculate method agreement/overlap\n",
    "# 37. Print top 20 features by each importance measure\n",
    "# 38. Show importance statistics and thresholds used\n",
    "#\n",
    "# TODO: Save Random Forest results:\n",
    "# 39. Save all importance scores and rankings\n",
    "# 40. Export RF-selected features list\n",
    "# 41. Save trained RF model for later use\n",
    "# 42. Create RF analysis report\n",
    "#\n",
    "# Expected output: Random Forest-based feature selection with multiple importance measures and validation\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"RANDOM FOREST-BASED FEATURE SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76a5f0",
   "metadata": {},
   "source": [
    "## 7. Boruta Feature Selection\n",
    "\n",
    "Apply the Boruta algorithm for all-relevant feature detection using shadow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 7: Boruta All-Relevant Feature Selection\n",
    "#\n",
    "# Your task: Apply Boruta algorithm to identify all features that are relevant for the prediction task\n",
    "#\n",
    "# TODO: Set up Boruta analysis:\n",
    "# 1. Print \"BORUTA-BASED FEATURE SELECTION\" header with separators\n",
    "# 2. Check if Boruta library is available\n",
    "# 3. If not available, provide installation instructions and alternative implementation\n",
    "#\n",
    "# TODO: Prepare data for Boruta:\n",
    "# 4. Create working copy: X_boruta = X_train.copy()\n",
    "# 5. Convert to numpy arrays if needed: X_array = X_boruta.values, y_array = y_train.values\n",
    "# 6. Ensure data types are appropriate for Boruta\n",
    "#\n",
    "# TODO: Configure and run Boruta (if available):\n",
    "# 7. Create Boruta selector:\n",
    "#    - boruta = BorutaPy(RandomForestClassifier(n_jobs=-1, random_state=42), n_estimators=100, verbose=2, random_state=42)\n",
    "# 8. Set Boruta parameters:\n",
    "#    - max_iter=100 (maximum iterations)\n",
    "#    - perc=100 (percentile for shadow feature importance)\n",
    "# 9. Fit Boruta: boruta.fit(X_array, y_array)\n",
    "#\n",
    "# TODO: Alternative Boruta implementation (if library not available):\n",
    "# 10. Implement simplified Boruta logic:\n",
    "#     - Create shadow features by permuting original features\n",
    "#     - Train Random Forest on original + shadow features\n",
    "#     - Compare feature importances with shadow feature importances\n",
    "#     - Select features that consistently outperform shadow features\n",
    "#\n",
    "# TODO: Extract Boruta results:\n",
    "# 11. Get confirmed features: boruta_confirmed = X_boruta.columns[boruta.support_].tolist()\n",
    "# 12. Get tentative features: boruta_tentative = X_boruta.columns[boruta.support_weak_].tolist()\n",
    "# 13. Get rejected features: boruta_rejected = X_boruta.columns[~(boruta.support_ | boruta.support_weak_)].tolist()\n",
    "# 14. Get feature rankings: boruta_rankings = boruta.ranking_\n",
    "#\n",
    "# TODO: Analyze Boruta results:\n",
    "# 15. Count features in each category: confirmed, tentative, rejected\n",
    "# 16. Calculate selection percentages\n",
    "# 17. Compare Boruta results with other selection methods\n",
    "# 18. Analyze convergence behavior (if available)\n",
    "#\n",
    "# TODO: Handle tentative features:\n",
    "# 19. Decide on tentative feature treatment:\n",
    "#     - Option 1: Include all tentative features\n",
    "#     - Option 2: Exclude all tentative features\n",
    "#     - Option 3: Apply additional criteria to tentative features\n",
    "# 20. Create final Boruta feature set: boruta_final_features\n",
    "#\n",
    "# TODO: Validate Boruta stability:\n",
    "# 21. Run Boruta multiple times with different random seeds\n",
    "# 22. Calculate feature selection stability across runs\n",
    "# 23. Identify most consistently selected features\n",
    "#\n",
    "# TODO: Compare with shadow feature importance:\n",
    "# 24. If shadow feature data is available, show importance comparisons\n",
    "# 25. Visualize feature importance vs shadow importance distributions\n",
    "# 26. Identify features with strongest signal vs noise ratios\n",
    "#\n",
    "# TODO: Analyze Boruta results by feature type:\n",
    "# 27. If expression and clinical features are separable:\n",
    "#     - Compare selection rates between feature types\n",
    "#     - Identify which type is more frequently selected\n",
    "#     - Look for biological patterns in selected genes\n",
    "#\n",
    "# TODO: Visualize Boruta analysis:\n",
    "# 28. Create feature importance plot with Boruta decisions\n",
    "# 29. Show histogram of confirmed vs rejected vs tentative features\n",
    "# 30. Plot Boruta convergence history (if available)\n",
    "# 31. Create comparison with Random Forest importance\n",
    "#\n",
    "# TODO: Generate Boruta summary:\n",
    "# 32. Print counts for each Boruta decision category\n",
    "# 33. Show top confirmed features with their rankings\n",
    "# 34. Compare Boruta selection with other methods\n",
    "# 35. Provide interpretation of Boruta results\n",
    "#\n",
    "# TODO: Save Boruta results:\n",
    "# 36. Save Boruta object and results\n",
    "# 37. Export confirmed and tentative features lists\n",
    "# 38. Create Boruta analysis report\n",
    "# 39. Document Boruta parameters and decisions made\n",
    "#\n",
    "# Expected output: Boruta-based all-relevant feature selection with comprehensive analysis\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"BORUTA-BASED FEATURE SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "#\n",
    "# # Check if Boruta is available\n",
    "# try:\n",
    "#     from boruta import BorutaPy\n",
    "#     boruta_available = True\n",
    "#     print(\"âœ“ Boruta library available\")\n",
    "# except ImportError:\n",
    "#     boruta_available = False\n",
    "#     print(\"âš ï¸  Boruta library not available\")\n",
    "#     print(\"To install: pip install Boruta\")\n",
    "#     print(\"Alternative: We'll implement simplified Boruta logic\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a84c2c",
   "metadata": {},
   "source": [
    "## 8. Feature Selection Comparison and Ensemble\n",
    "\n",
    "Compare all methods and create optimal feature sets through ensemble approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ ACTIVITY 8: Comprehensive Feature Selection Comparison and Ensemble\n",
    "#\n",
    "# Your task: Compare all feature selection methods and create optimal ensemble feature sets\n",
    "#\n",
    "# TODO: Set up method comparison:\n",
    "# 1. Print \"FEATURE SELECTION METHOD COMPARISON\" header with separators\n",
    "# 2. Collect all feature sets from previous methods:\n",
    "#    - variance_selected_features, correlation_selected_features\n",
    "#    - cox_selected_features, rf_selected_features, boruta_selected_features\n",
    "# 3. Handle cases where some methods might not have results\n",
    "#\n",
    "# TODO: Create method comparison matrix:\n",
    "# 4. Create binary matrix showing which features are selected by which methods\n",
    "# 5. Methods as columns, features as rows, 1 if selected, 0 if not\n",
    "# 6. Calculate method agreement scores: intersection/union for each pair\n",
    "# 7. Create method similarity matrix using Jaccard similarity\n",
    "#\n",
    "# TODO: Analyze feature selection overlap:\n",
    "# 8. Calculate intersection of all methods: core_features = intersection of all feature sets\n",
    "# 9. Find features selected by majority: majority_features = selected by >50% of methods\n",
    "# 10. Identify method-specific features: features selected by only one method\n",
    "# 11. Calculate total unique features across all methods\n",
    "#\n",
    "# TODO: Create voting-based ensemble selection:\n",
    "# 12. Count votes for each feature: vote_count = sum of selections across methods\n",
    "# 13. Create ensemble feature sets with different vote thresholds:\n",
    "#     - unanimous_features: vote_count == number_of_methods\n",
    "#     - strong_consensus: vote_count >= 80% of methods\n",
    "#     - majority_consensus: vote_count >= 50% of methods\n",
    "#     - weak_consensus: vote_count >= 30% of methods\n",
    "#\n",
    "# TODO: Weight methods by performance (optional):\n",
    "# 15. If validation performance is available, weight methods by accuracy\n",
    "# 16. Create weighted voting: weight high-performing methods more\n",
    "# 17. Calculate weighted ensemble scores for each feature\n",
    "#\n",
    "# TODO: Apply stability analysis:\n",
    "# 18. Run each method multiple times with bootstrap sampling\n",
    "# 19. Calculate selection stability for each feature: selection_frequency\n",
    "# 20. Combine stability with vote counts: stable_consensus_features\n",
    "#\n",
    "# TODO: Create different ensemble strategies:\n",
    "# 21. Conservative ensemble: high agreement threshold (>=4 methods)\n",
    "# 22. Moderate ensemble: medium agreement threshold (>=3 methods)\n",
    "# 23. Liberal ensemble: low agreement threshold (>=2 methods)\n",
    "# 24. Method-specific ensembles: combine similar methods (e.g., RF + Boruta)\n",
    "#\n",
    "# TODO: Validate ensemble feature sets:\n",
    "# 25. For each ensemble, train quick Random Forest model\n",
    "# 26. Evaluate on validation set: accuracy, precision, recall, F1-score\n",
    "# 27. Compare ensemble performance to individual methods\n",
    "# 28. Identify optimal ensemble size vs performance trade-off\n",
    "#\n",
    "# TODO: Analyze biological relevance (if gene features):\n",
    "# 29. For top ensemble features, look up biological functions\n",
    "# 30. Check literature for breast cancer biomarker validation\n",
    "# 31. Identify known pathways represented in selected features\n",
    "# 32. Note any surprising or novel feature selections\n",
    "#\n",
    "# TODO: Create comprehensive comparison visualization:\n",
    "# 33. Create UpSet plot or Venn diagram showing method overlaps\n",
    "# 34. Create heatmap of method agreement\n",
    "# 35. Plot ensemble performance vs feature set size\n",
    "# 36. Show feature ranking stability across methods\n",
    "#\n",
    "# TODO: Generate final feature recommendations:\n",
    "# 37. Recommend 3-5 different feature sets for different use cases:\n",
    "#     - Minimal set: core features selected by all methods\n",
    "#     - Balanced set: majority consensus features\n",
    "#     - Comprehensive set: liberal consensus features\n",
    "#     - Performance-optimized set: best validation performance\n",
    "# 38. Provide rationale for each recommendation\n",
    "#\n",
    "# TODO: Create feature selection summary report:\n",
    "# 39. Document all methods used and their parameters\n",
    "# 40. Show method comparison results and overlaps\n",
    "# 41. Present final feature set recommendations with justifications\n",
    "# 42. Include performance validation results\n",
    "#\n",
    "# TODO: Export final results:\n",
    "# 43. Save all ensemble feature sets\n",
    "# 44. Export method comparison matrices\n",
    "# 45. Save validation performance results\n",
    "# 46. Create comprehensive feature selection report\n",
    "#\n",
    "# Expected output: Complete feature selection comparison with optimized ensemble feature sets\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"FEATURE SELECTION METHOD COMPARISON\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b6e07",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Coding Hints and Templates\n",
    "\n",
    "Need help getting started? Here are some code templates and hints for feature selection methods:\n",
    "\n",
    "### ðŸ“‹ **Template: Variance-Based Selection**\n",
    "```python\n",
    "# Variance threshold filtering\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.05)\n",
    "selector.fit(X_train)\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Correlation Analysis**\n",
    "```python\n",
    "# Feature-target correlation\n",
    "from scipy.stats import pearsonr\n",
    "correlations = X_train.apply(lambda x: pearsonr(x, y_train)[0])\n",
    "high_corr_features = correlations[abs(correlations) > 0.1].index\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Cox Regression (with lifelines)**\n",
    "```python\n",
    "# Univariate Cox regression\n",
    "from lifelines import CoxPHFitter\n",
    "cox_results = {}\n",
    "for feature in X_train.columns:\n",
    "    df = pd.DataFrame({'duration': survival_time, 'event': event_indicator, 'feature': X_train[feature]})\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df, duration_col='duration', event_col='event')\n",
    "    cox_results[feature] = {'hr': cph.hazard_ratios_.values[0], 'p': cph.summary.p.values[0]}\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Random Forest Importance**\n",
    "```python\n",
    "# Random Forest feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "top_features = importances.nlargest(100).index\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Permutation Importance**\n",
    "```python\n",
    "# Permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=42)\n",
    "perm_scores = pd.Series(perm_importance.importances_mean, index=X_train.columns)\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Boruta Selection**\n",
    "```python\n",
    "# Boruta feature selection\n",
    "from boruta import BorutaPy\n",
    "boruta = BorutaPy(RandomForestClassifier(n_jobs=-1), n_estimators=100, verbose=2, random_state=42)\n",
    "boruta.fit(X_train.values, y_train.values)\n",
    "selected_features = X_train.columns[boruta.support_]\n",
    "```\n",
    "\n",
    "### ðŸ“‹ **Template: Method Comparison**\n",
    "```python\n",
    "# Compare feature selection methods\n",
    "method_results = {\n",
    "    'variance': variance_selected,\n",
    "    'correlation': correlation_selected,\n",
    "    'cox': cox_selected,\n",
    "    'rf': rf_selected,\n",
    "    'boruta': boruta_selected\n",
    "}\n",
    "\n",
    "# Create voting matrix\n",
    "all_features = set().union(*method_results.values())\n",
    "vote_matrix = pd.DataFrame(index=all_features, columns=method_results.keys())\n",
    "for method, features in method_results.items():\n",
    "    vote_matrix[method] = vote_matrix.index.isin(features).astype(int)\n",
    "\n",
    "# Majority voting\n",
    "vote_counts = vote_matrix.sum(axis=1)\n",
    "majority_features = vote_counts[vote_counts >= len(method_results) // 2 + 1].index\n",
    "```\n",
    "\n",
    "### ðŸ” **Key Feature Selection Principles**\n",
    "- **Variance**: Remove features with little variation (threshold typically 0.01-0.1)\n",
    "- **Correlation**: Balance relevance (high target correlation) with redundancy removal\n",
    "- **Cox Regression**: Focus on hazard ratios >1.2 or <0.8 with p<0.05\n",
    "- **Random Forest**: Use multiple importance measures and stability assessment\n",
    "- **Boruta**: All-relevant selection, good for comprehensive feature discovery\n",
    "- **Ensemble**: Combine methods to balance different selection criteria\n",
    "\n",
    "### ðŸ” **Selection Strategy Guidelines**\n",
    "- **Conservative**: Use intersection of multiple methods (high precision)\n",
    "- **Liberal**: Use union of methods (high recall)\n",
    "- **Balanced**: Use majority voting (balance precision/recall)\n",
    "- **Validate**: Always check performance on held-out validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2e2a1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Assessment\n",
    "\n",
    "### âœ… **Self-Check Questions**\n",
    "\n",
    "After completing the feature selection activities, you should be able to answer:\n",
    "\n",
    "1. **Variance-Based Selection**\n",
    "   - Why do we remove low-variance features before machine learning?\n",
    "   - What's the difference between variance and coefficient of variation for feature selection?\n",
    "   - How do you choose appropriate variance thresholds for different data types?\n",
    "   - What are the limitations of variance-based selection?\n",
    "\n",
    "2. **Correlation-Based Selection**\n",
    "   - What's the difference between Pearson and Spearman correlation for feature selection?\n",
    "   - How do you balance feature relevance (target correlation) with redundancy removal?\n",
    "   - When should you use mutual information vs linear correlation?\n",
    "   - How do you handle multicollinearity in selected features?\n",
    "\n",
    "3. **Cox Regression-Based Selection**\n",
    "   - What does a hazard ratio >1 vs <1 indicate in survival analysis?\n",
    "   - How do you interpret p-values in the context of multiple testing?\n",
    "   - What are the assumptions of Cox proportional hazards models?\n",
    "   - How do you handle censored survival data in feature selection?\n",
    "\n",
    "4. **Random Forest-Based Selection**\n",
    "   - What's the difference between impurity-based and permutation importance?\n",
    "   - Why might Random Forest importance be biased toward certain feature types?\n",
    "   - How do you assess feature importance stability across different RF models?\n",
    "   - When should you use RFECV vs simple importance thresholding?\n",
    "\n",
    "5. **Boruta Algorithm**\n",
    "   - What are shadow features and why are they important in Boruta?\n",
    "   - How does Boruta differ from other feature selection methods?\n",
    "   - What's the difference between confirmed, tentative, and rejected features?\n",
    "   - How do you handle tentative features in final feature selection?\n",
    "\n",
    "6. **Ensemble Methods**\n",
    "   - How do you combine results from different feature selection methods?\n",
    "   - What are the trade-offs between conservative vs liberal ensemble strategies?\n",
    "   - How do you validate the performance of ensemble feature sets?\n",
    "   - When should you weight different methods differently in ensembles?\n",
    "\n",
    "### ðŸ† **Success Criteria**\n",
    "\n",
    "You have successfully completed this feature selection notebook if you can:\n",
    "- âœ… Apply multiple feature selection methods to the same dataset\n",
    "- âœ… Understand the strengths and limitations of each method\n",
    "- âœ… Compare and contrast results from different approaches\n",
    "- âœ… Create ensemble feature sets using voting strategies\n",
    "- âœ… Validate feature selection results on held-out data\n",
    "- âœ… Interpret biological relevance of selected features (for genomics data)\n",
    "- âœ… Export optimized feature sets for downstream modeling\n",
    "- âœ… Document feature selection decisions and rationale\n",
    "\n",
    "### ðŸš€ **Extension Challenges** (Optional)\n",
    "\n",
    "For advanced students:\n",
    "1. **Advanced Selection Methods**:\n",
    "   - Implement LASSO regularization for feature selection\n",
    "   - Use Elastic Net for combined L1/L2 penalty selection\n",
    "   - Apply genetic algorithm-based feature selection\n",
    "\n",
    "2. **Stability Analysis**:\n",
    "   - Assess feature selection stability across bootstrap samples\n",
    "   - Implement Kuncheva stability index\n",
    "   - Create stability-weighted ensemble selection\n",
    "\n",
    "3. **Biological Integration**:\n",
    "   - Incorporate pathway information into feature selection\n",
    "   - Use protein-protein interaction networks for feature grouping\n",
    "   - Apply gene set enrichment analysis to selected features\n",
    "\n",
    "4. **Multi-objective Optimization**:\n",
    "   - Balance feature set size vs prediction performance\n",
    "   - Optimize for multiple objectives (accuracy, interpretability, cost)\n",
    "   - Use Pareto optimization for feature selection\n",
    "\n",
    "5. **Advanced Ensemble Methods**:\n",
    "   - Implement stacking-based feature selection ensemble\n",
    "   - Use meta-learning to weight different selection methods\n",
    "   - Apply Bayesian model averaging for feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa9ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Feature Selection Summary\n",
    "\n",
    "In this notebook, you have successfully completed:\n",
    "\n",
    "### âœ… **Completed Tasks:**\n",
    "1. **Data Loading & Validation**: Loaded preprocessed data and validated for feature selection\n",
    "2. **Variance-Based Selection**: Applied statistical variance analysis and VarianceThreshold filtering\n",
    "3. **Correlation-Based Selection**: Used Pearson, Spearman, and mutual information analysis\n",
    "4. **Cox Regression Selection**: Implemented survival-based feature ranking with hazard ratios\n",
    "5. **Random Forest Selection**: Applied multiple RF importance measures and RFECV\n",
    "6. **Boruta Selection**: Used all-relevant feature detection with shadow features\n",
    "7. **Method Comparison**: Analyzed agreement and overlap between selection methods\n",
    "8. **Ensemble Creation**: Developed voting-based ensemble feature sets\n",
    "9. **Performance Validation**: Evaluated feature sets on validation data\n",
    "10. **Results Export**: Saved optimal feature sets with comprehensive documentation\n",
    "\n",
    "### ðŸŽ¯ **Key Feature Selection Achievements:**\n",
    "- **Comprehensive Analysis**: Applied 5+ different feature selection approaches\n",
    "- **Method Comparison**: Quantified agreement and complementarity between methods\n",
    "- **Ensemble Optimization**: Created multiple feature sets for different use cases\n",
    "- **Biological Relevance**: Identified features with known biomarker significance\n",
    "- **Performance Validation**: Confirmed feature sets improve prediction accuracy\n",
    "- **Reproducibility**: Documented all parameters and decisions for reproducible analysis\n",
    "\n",
    "### ðŸ”„ **Next Steps:**\n",
    "In the next notebook (`04_model_development.ipynb`), we will:\n",
    "1. **Baseline Models**: Train simple classifiers using selected feature sets\n",
    "2. **Algorithm Comparison**: Evaluate multiple ML algorithms (SVM, RF, Neural Networks)\n",
    "3. **Hyperparameter Tuning**: Optimize model parameters using grid/random search\n",
    "4. **Cross-Validation**: Implement robust model evaluation with multiple CV strategies\n",
    "5. **Model Interpretation**: Understand how selected features contribute to predictions\n",
    "6. **Ensemble Modeling**: Combine multiple models for improved performance\n",
    "\n",
    "### ðŸ“ **Exported Files:**\n",
    "- `../results/feature_selection/`: All feature selection results and rankings\n",
    "- `../results/feature_selection/ensemble_features/`: Optimized feature sets for modeling\n",
    "- `../results/feature_selection/method_comparison/`: Method comparison matrices and analysis\n",
    "- `../results/feature_selection/feature_selection_report.json`: Complete selection documentation\n",
    "\n",
    "### ðŸ§¬ **Feature Selection Insights:**\n",
    "- **Most Important Methods**: Random Forest and Boruta showed highest agreement\n",
    "- **Core Features**: X genes/variables selected by all methods (biological significance)\n",
    "- **Method Complementarity**: Different methods captured different aspects of feature importance\n",
    "- **Optimal Feature Set Size**: 50-200 features provided best performance/interpretability trade-off\n",
    "\n",
    "---\n",
    "\n",
    "**Outstanding work completing the comprehensive feature selection analysis! ðŸŽ‰**\n",
    "\n",
    "You've successfully applied multiple state-of-the-art feature selection methods and created optimized feature sets that will serve as the foundation for robust machine learning models. The rigorous comparison and ensemble approach ensures you're working with the most informative and stable features for risk classification."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
