{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddae1e39",
   "metadata": {},
   "source": [
    "# Machine Learning Course: Feature Selection\n",
    "\n",
    "## Notebook 3: Advanced Feature Selection for High-Dimensional Genomics Data\n",
    "\n",
    "> **üìã Prerequisites:** Complete `01_data_exploration.ipynb` and `02_data_preprocessing.ipynb` first. This notebook uses the preprocessed data from notebook 2.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. **Apply advanced variance filtering** beyond basic preprocessing thresholds\n",
    "2. **Remove highly correlated features** to reduce redundancy\n",
    "3. **Use differential expression analysis** to identify category-specific genes\n",
    "4. **Implement Random Forest importance** with KneeLocator for optimal feature selection\n",
    "5. **Apply alternative ML-based importance** methods (LASSO, SVM, etc.)\n",
    "6. **Use Boruta algorithm** for all-relevant feature detection\n",
    "7. **Apply Cox regression** for survival-based feature association\n",
    "\n",
    "### Data Flow\n",
    "This notebook refines features from preprocessed data:\n",
    "- **Input**: Preprocessed, scaled datasets from `02_data_preprocessing.ipynb`\n",
    "- **Processing**: Seven advanced feature selection methods\n",
    "- **Output**: Optimized feature sets for final model development\n",
    "\n",
    "### Feature Selection Philosophy\n",
    "Unlike preprocessing (notebook 2) which focused on data quality and basic filtering, this notebook applies **advanced statistical and machine learning methods** to identify the most **predictive and biologically relevant** features for classification and survival prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26048c",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the cleaned and preprocessed data from notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 1: Load Preprocessed Data and Import Libraries\n",
    "#\n",
    "# Your task: Load the preprocessed results from notebook 2 and import feature selection libraries\n",
    "#\n",
    "# TODO: Import required libraries:\n",
    "# 1. Import pandas, numpy, matplotlib.pyplot, seaborn\n",
    "# 2. Import sklearn feature_selection: VarianceThreshold, SelectKBest, f_classif, mutual_info_classif\n",
    "# 3. Import sklearn models: RandomForestClassifier, LogisticRegression (with Lasso penalty)\n",
    "# 4. Import sklearn.svm: LinearSVC\n",
    "# 5. Import scipy.stats: ttest_ind, mannwhitneyu\n",
    "# 6. Try importing: from kneed import KneeLocator (for knee detection)\n",
    "# 7. Try importing: from boruta import BorutaPy (optional)\n",
    "# 8. Try importing: from lifelines import CoxPHFitter (for survival analysis)\n",
    "#\n",
    "# TODO: Load preprocessed data:\n",
    "# 9. Load from '../results/preprocessing/':\n",
    "#    - X_train_scaled.csv, X_val_scaled.csv, X_test_scaled.csv\n",
    "#    - y_train.csv, y_val.csv, y_test.csv\n",
    "# 10. Load clinical data if available:\n",
    "#     - clinical_train.csv, clinical_val.csv, clinical_test.csv\n",
    "# 11. Print shapes and confirm data is loaded correctly\n",
    "#\n",
    "# TODO: Verify data from preprocessing:\n",
    "# 12. Check that target variables include RFS_MONTHS and RFS_STATUS (from notebook 2)\n",
    "# 13. Confirm that gene expression data is normalized and filtered (from notebook 2)\n",
    "# 14. Display basic statistics to validate data quality\n",
    "#\n",
    "# Expected output: Loaded preprocessed data and imported feature selection libraries\n",
    "\n",
    "# Write your code below:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# # Load preprocessed data\n",
    "# preprocessing_dir = '../results/preprocessing/'\n",
    "# X_train = pd.read_csv(f'{preprocessing_dir}X_train_scaled.csv', index_col=0)\n",
    "# X_val = pd.read_csv(f'{preprocessing_dir}X_val_scaled.csv', index_col=0)\n",
    "# X_test = pd.read_csv(f'{preprocessing_dir}X_test_scaled.csv', index_col=0)\n",
    "# y_train = pd.read_csv(f'{preprocessing_dir}y_train.csv', index_col=0)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8a4db",
   "metadata": {},
   "source": [
    "## 2. Advanced Variance-Based Feature Selection\n",
    "\n",
    "Apply stricter variance filtering than in preprocessing to identify genes with the highest variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5662587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 2: Advanced Variance Filtering (Stronger than Notebook 2)\n",
    "#\n",
    "# Your task: Apply more aggressive variance filtering to select highly variable genes\n",
    "#\n",
    "# TODO: Calculate variance statistics:\n",
    "# 1. Calculate variance for all features: gene_variances = X_train.var()\n",
    "# 2. Calculate coefficient of variation: gene_cv = X_train.std() / X_train.mean().abs()\n",
    "# 3. Calculate percentiles: np.percentile(gene_variances, [25, 50, 75, 90, 95])\n",
    "# 4. Print variance distribution summary\n",
    "#\n",
    "# TODO: Apply stronger variance threshold:\n",
    "# 5. In notebook 2, you may have used threshold ~0.01-0.05\n",
    "# 6. Now use higher threshold (e.g., 0.1 or top 75th percentile) to select only highly variable genes\n",
    "# 7. Create VarianceThreshold selector: selector = VarianceThreshold(threshold=0.1)\n",
    "# 8. Fit and transform: selector.fit(X_train)\n",
    "# 9. Get selected features: variance_selected_features = X_train.columns[selector.get_support()]\n",
    "#\n",
    "# TODO: Alternative: Select top N most variable genes:\n",
    "# 10. Sort genes by variance: sorted_by_variance = gene_variances.sort_values(ascending=False)\n",
    "# 11. Select top 1000 or top 50%: top_variance_genes = sorted_by_variance.head(1000).index\n",
    "# 12. Compare fixed threshold vs top-N selection\n",
    "#\n",
    "# TODO: Visualize variance distribution:\n",
    "# 13. Create histogram of gene variances\n",
    "# 14. Mark threshold on the plot\n",
    "# 15. Show number of genes selected vs removed\n",
    "#\n",
    "# TODO: Print summary:\n",
    "# 16. Original number of genes (after preprocessing)\n",
    "# 17. Number of genes after advanced variance filtering\n",
    "# 18. Percentage of genes retained\n",
    "# 19. Show variance statistics for selected vs removed genes\n",
    "#\n",
    "# Expected output: Highly variable genes selected with stricter threshold than preprocessing\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"ADVANCED VARIANCE FILTERING\")\n",
    "# print(\"=\"*60)\n",
    "# gene_variances = X_train.var()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb508c49",
   "metadata": {},
   "source": [
    "## 3. De-correlation: Remove Redundant Features\n",
    "\n",
    "Remove highly correlated features to reduce redundancy and multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f62e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 3: De-correlation - Remove Highly Correlated Features\n",
    "#\n",
    "# Your task: Identify and remove redundant features with high pairwise correlations\n",
    "#\n",
    "# TODO: Calculate feature-feature correlation matrix:\n",
    "# 1. Start with variance-filtered features from Activity 2\n",
    "# 2. Calculate correlation matrix: corr_matrix = X_train[variance_selected_features].corr()\n",
    "# 3. Get absolute correlations: abs_corr_matrix = corr_matrix.abs()\n",
    "#\n",
    "# TODO: Identify highly correlated pairs:\n",
    "# 4. Set correlation threshold (e.g., 0.8 or 0.9 for high correlation)\n",
    "# 5. Find upper triangle of correlation matrix to avoid duplicates:\n",
    "#    - upper_triangle = np.triu(abs_corr_matrix, k=1)\n",
    "# 6. Find pairs with correlation above threshold\n",
    "# 7. Create list of correlated feature pairs\n",
    "#\n",
    "# TODO: Remove redundant features:\n",
    "# 8. For each highly correlated pair, decide which to keep:\n",
    "#    - Option 1: Keep the one with higher variance\n",
    "#    - Option 2: Keep the one with higher correlation to target\n",
    "#    - Option 3: Keep the first one alphabetically (simplest)\n",
    "# 9. Create list of features to drop: features_to_drop = []\n",
    "# 10. Remove redundant features: decorrelated_features = [f for f in variance_selected_features if f not in features_to_drop]\n",
    "#\n",
    "# TODO: Alternative: Use clustering approach:\n",
    "# 11. Cluster features by correlation\n",
    "# 12. Select one representative from each cluster\n",
    "#\n",
    "# TODO: Visualize correlations:\n",
    "# 13. Create heatmap of correlation matrix (sample of features)\n",
    "# 14. Show distribution of pairwise correlations\n",
    "# 15. Highlight features that were removed\n",
    "#\n",
    "# TODO: Print de-correlation summary:\n",
    "# 16. Number of highly correlated pairs found\n",
    "# 17. Number of features removed due to redundancy\n",
    "# 18. Number of features remaining after de-correlation\n",
    "# 19. Show examples of correlated pairs that were addressed\n",
    "#\n",
    "# Expected output: De-correlated feature set with redundancy removed\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"DE-CORRELATION: REMOVING REDUNDANT FEATURES\")\n",
    "# print(\"=\"*60)\n",
    "# corr_matrix = X_train[variance_selected_features].corr()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c91bf",
   "metadata": {},
   "source": [
    "## 4. Differential Gene Expression Between Categories\n",
    "\n",
    "Identify genes that are significantly different between outcome categories using statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588be03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 4: Differential Gene Expression Analysis\n",
    "#\n",
    "# Your task: Identify genes with significant expression differences between outcome categories\n",
    "#\n",
    "# TODO: Prepare data for differential expression:\n",
    "# 1. Start with de-correlated features from Activity 3\n",
    "# 2. Define outcome categories from y_train (e.g., RFS_STATUS: 0=no recurrence, 1=recurrence)\n",
    "# 3. Split samples by category: group_0 = X_train[y_train['RFS_STATUS'] == 0]\n",
    "#                               group_1 = X_train[y_train['RFS_STATUS'] == 1]\n",
    "#\n",
    "# TODO: Calculate differential expression statistics:\n",
    "# 4. For each gene, calculate:\n",
    "#    - Mean expression in group 0 and group 1\n",
    "#    - Log2 fold change: log2(mean_group1 / mean_group0)\n",
    "#    - Handle zero/negative values appropriately\n",
    "# 5. Calculate statistical significance using t-test:\n",
    "#    - For each gene: t_stat, p_value = ttest_ind(group_0[gene], group_1[gene])\n",
    "# 6. Store results in DataFrame: de_results with columns [gene, log2fc, p_value, mean_group0, mean_group1]\n",
    "#\n",
    "# TODO: Apply multiple testing correction:\n",
    "# 7. Import: from statsmodels.stats.multitest import multipletests\n",
    "# 8. Correct p-values: reject, pvals_corrected, _, _ = multipletests(de_results['p_value'], method='fdr_bh')\n",
    "# 9. Add corrected p-values to results: de_results['p_adj'] = pvals_corrected\n",
    "#\n",
    "# TODO: Select differentially expressed genes:\n",
    "# 10. Set thresholds:\n",
    "#     - p_adj < 0.05 (statistical significance after correction)\n",
    "#     - abs(log2fc) > 1 (at least 2-fold change)\n",
    "# 11. Select significant genes: de_genes = de_results[(de_results['p_adj'] < 0.05) & (de_results['log2fc'].abs() > 1)]\n",
    "# 12. Get list of DE gene names: de_selected_features = de_genes.index.tolist()\n",
    "#\n",
    "# TODO: Alternative: Use Mann-Whitney U test (non-parametric):\n",
    "# 13. For non-normally distributed data, use mannwhitneyu instead of ttest_ind\n",
    "# 14. Compare results between parametric and non-parametric tests\n",
    "#\n",
    "# TODO: Visualize differential expression:\n",
    "# 15. Create volcano plot: x=log2fc, y=-log10(p_adj), color by significance\n",
    "# 16. Create MA plot: x=mean expression, y=log2fc\n",
    "# 17. Show distribution of fold changes and p-values\n",
    "# 18. Highlight top differentially expressed genes\n",
    "#\n",
    "# TODO: Analyze DE results:\n",
    "# 19. Count upregulated genes (log2fc > 1) and downregulated genes (log2fc < -1)\n",
    "# 20. Show top 10 most significant genes with largest fold changes\n",
    "# 21. Compare mean expression levels between groups for top genes\n",
    "#\n",
    "# TODO: Print DE summary:\n",
    "# 22. Total genes tested\n",
    "# 23. Number of significant genes at different thresholds\n",
    "# 24. Distribution of fold changes\n",
    "# 25. Examples of top differentially expressed genes\n",
    "#\n",
    "# Expected output: Differentially expressed genes between outcome categories\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"DIFFERENTIAL GENE EXPRESSION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "# from scipy.stats import ttest_ind\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c052ff",
   "metadata": {},
   "source": [
    "## 5. Random Forest Importance with KneeLocator\n",
    "\n",
    "Use Random Forest to rank features by importance and apply KneeLocator to find optimal cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 5: Random Forest Importance with KneeLocator\n",
    "#\n",
    "# Your task: Use Random Forest to rank features and find optimal number using the elbow method\n",
    "#\n",
    "# TODO: Train Random Forest on all current features:\n",
    "# 1. Use features from previous activities (or all preprocessed features)\n",
    "# 2. Create RandomForestClassifier: rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# 3. Fit on training data: rf.fit(X_train[current_features], y_train['RFS_STATUS'])\n",
    "# 4. Extract feature importances: rf_importances = pd.Series(rf.feature_importances_, index=current_features)\n",
    "#\n",
    "# TODO: Sort features by importance:\n",
    "# 5. Sort importances descending: rf_ranking = rf_importances.sort_values(ascending=False)\n",
    "# 6. Calculate cumulative importance: cumulative_importance = rf_ranking.cumsum()\n",
    "# 7. Normalize cumulative importance: cumulative_importance / cumulative_importance.max()\n",
    "#\n",
    "# TODO: Apply KneeLocator to find optimal cutoff:\n",
    "# 8. Check if kneed library is available:\n",
    "#    try:\n",
    "#        from kneed import KneeLocator\n",
    "# 9. Create x values (feature ranks) and y values (sorted importances)\n",
    "# 10. Apply KneeLocator: kl = KneeLocator(x=range(len(rf_ranking)), y=rf_ranking.values, curve='convex', direction='decreasing')\n",
    "# 11. Find knee point: knee_point = kl.knee\n",
    "# 12. Select features up to knee point: rf_knee_features = rf_ranking.iloc[:knee_point].index.tolist()\n",
    "#\n",
    "# TODO: Alternative cutoff methods if KneeLocator unavailable:\n",
    "# 13. Select features contributing to 90% cumulative importance\n",
    "# 14. Select top N features (e.g., top 100 or 200)\n",
    "# 15. Select features with importance > threshold (e.g., > 0.001)\n",
    "#\n",
    "# TODO: Validate Random Forest selection:\n",
    "# 16. Train new RF with only selected features\n",
    "# 17. Evaluate on validation set: accuracy, AUC\n",
    "# 18. Compare performance with all features vs selected features\n",
    "#\n",
    "# TODO: Visualize Random Forest importance:\n",
    "# 19. Plot feature importance curve with knee point marked\n",
    "# 20. Plot cumulative importance curve\n",
    "# 21. Show bar plot of top 20 most important features\n",
    "# 22. Create importance distribution histogram\n",
    "#\n",
    "# TODO: Print RF importance summary:\n",
    "# 23. Total features evaluated\n",
    "# 24. Knee point location (feature rank)\n",
    "# 25. Number of features selected by knee method\n",
    "# 26. Cumulative importance of selected features\n",
    "# 27. Show top 20 features with their importance scores\n",
    "#\n",
    "# Expected output: Optimal feature set identified using Random Forest importance and KneeLocator\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"RANDOM FOREST IMPORTANCE WITH KNEELOCATOR\")\n",
    "# print(\"=\"*60)\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76a5f0",
   "metadata": {},
   "source": [
    "## 6. Other ML-Based Feature Importance\n",
    "\n",
    "Apply LASSO (L1 regularization) and SVM-based feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 6: LASSO and SVM-Based Feature Selection\n",
    "#\n",
    "# Your task: Use L1-regularized models to identify important features through embedded selection\n",
    "#\n",
    "# TODO: Apply LASSO (L1-regularized Logistic Regression):\n",
    "# 1. Import: from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "# 2. Create L1-penalized logistic regression: lasso_lr = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\n",
    "# 3. Fit on training data: lasso_lr.fit(X_train[current_features], y_train['RFS_STATUS'])\n",
    "# 4. Extract coefficients: lasso_coefs = pd.Series(lasso_lr.coef_[0], index=current_features)\n",
    "# 5. Get non-zero coefficients: lasso_selected = lasso_coefs[lasso_coefs != 0].index.tolist()\n",
    "#\n",
    "# TODO: Optimize LASSO regularization strength:\n",
    "# 6. Try different C values: C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "# 7. For each C, count number of selected features\n",
    "# 8. Plot: C vs number of selected features\n",
    "# 9. Choose optimal C that balances sparsity and performance\n",
    "#\n",
    "# TODO: Apply Linear SVM with L1 penalty:\n",
    "# 10. Import: from sklearn.svm import LinearSVC\n",
    "# 11. Create LinearSVC with L1 penalty: svm_l1 = LinearSVC(penalty='l1', dual=False, C=0.1, random_state=42, max_iter=10000)\n",
    "# 12. Fit on training data: svm_l1.fit(X_train[current_features], y_train['RFS_STATUS'])\n",
    "# 13. Extract coefficients: svm_coefs = pd.Series(svm_l1.coef_[0], index=current_features)\n",
    "# 14. Get non-zero coefficients: svm_selected = svm_coefs[svm_coefs != 0].index.tolist()\n",
    "#\n",
    "# TODO: Use SelectFromModel wrapper:\n",
    "# 15. Import: from sklearn.feature_selection import SelectFromModel\n",
    "# 16. Create selector: selector = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', C=0.1))\n",
    "# 17. Fit and transform: selector.fit(X_train[current_features], y_train['RFS_STATUS'])\n",
    "# 18. Get selected features: sfm_selected = current_features[selector.get_support()]\n",
    "#\n",
    "# TODO: Compare LASSO and SVM selections:\n",
    "# 19. Calculate overlap between LASSO and SVM selected features\n",
    "# 20. Identify features selected by both methods (intersection)\n",
    "# 21. Identify features selected by either method (union)\n",
    "# 22. Create combined ML-based feature set: ml_selected_features\n",
    "#\n",
    "# TODO: Analyze coefficient patterns:\n",
    "# 23. Compare coefficient magnitudes between LASSO and SVM\n",
    "# 24. Identify features with consistently large coefficients\n",
    "# 25. Plot coefficient distributions\n",
    "#\n",
    "# TODO: Visualize ML-based selection:\n",
    "# 26. Create bar plots of top coefficients for LASSO and SVM\n",
    "# 27. Create scatter plot: LASSO coef vs SVM coef\n",
    "# 28. Show regularization path (coefficient vs C values)\n",
    "#\n",
    "# TODO: Validate ML selections:\n",
    "# 29. Train models with selected features on train set\n",
    "# 30. Evaluate on validation set: accuracy, AUC\n",
    "# 31. Compare LASSO vs SVM vs combined selection performance\n",
    "#\n",
    "# TODO: Print ML selection summary:\n",
    "# 32. Number of features selected by LASSO\n",
    "# 33. Number of features selected by SVM\n",
    "# 34. Number of features in intersection and union\n",
    "# 35. Show top 20 features by absolute coefficient magnitude\n",
    "#\n",
    "# Expected output: Feature selection via LASSO and SVM with coefficient analysis\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"ML-BASED FEATURE SELECTION: LASSO AND SVM\")\n",
    "# print(\"=\"*60)\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a84c2c",
   "metadata": {},
   "source": [
    "## 7. Boruta All-Relevant Feature Selection\n",
    "\n",
    "Apply Boruta algorithm to identify all features relevant to the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 7: Boruta All-Relevant Feature Selection\n",
    "#\n",
    "# Your task: Use Boruta algorithm to identify all statistically relevant features\n",
    "#\n",
    "# TODO: Check Boruta availability:\n",
    "# 1. Try importing: from boruta import BorutaPy\n",
    "# 2. If not available, print installation instructions: pip install Boruta\n",
    "# 3. Handle case where Boruta is not installed gracefully\n",
    "#\n",
    "# TODO: Prepare data for Boruta:\n",
    "# 4. Convert to numpy arrays: X_array = X_train[current_features].values\n",
    "# 5. Convert target: y_array = y_train['RFS_STATUS'].values\n",
    "# 6. Ensure data types are appropriate\n",
    "#\n",
    "# TODO: Configure and run Boruta:\n",
    "# 7. Create base estimator: rf_boruta = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\n",
    "# 8. Create Boruta selector:\n",
    "#    boruta = BorutaPy(\n",
    "#        estimator=rf_boruta,\n",
    "#        n_estimators=100,\n",
    "#        max_iter=100,\n",
    "#        random_state=42,\n",
    "#        verbose=2\n",
    "#    )\n",
    "# 9. Fit Boruta: boruta.fit(X_array, y_array)\n",
    "#\n",
    "# TODO: Extract Boruta results:\n",
    "# 10. Get confirmed features: boruta_confirmed = current_features[boruta.support_].tolist()\n",
    "# 11. Get tentative features: boruta_tentative = current_features[boruta.support_weak_].tolist()\n",
    "# 12. Get rejected features: boruta_rejected = current_features[~(boruta.support_ | boruta.support_weak_)].tolist()\n",
    "# 13. Get feature rankings: boruta_ranking = pd.Series(boruta.ranking_, index=current_features)\n",
    "#\n",
    "# TODO: Decide on tentative features:\n",
    "# 14. Option 1: Include tentative features (liberal approach)\n",
    "# 15. Option 2: Exclude tentative features (conservative approach)\n",
    "# 16. Create final Boruta feature set: boruta_selected_features = boruta_confirmed (or + tentative)\n",
    "#\n",
    "# TODO: Analyze Boruta iterations:\n",
    "# 17. If available, analyze feature importance history across iterations\n",
    "# 18. Identify features that were quickly confirmed vs those that took many iterations\n",
    "# 19. Check convergence behavior\n",
    "#\n",
    "# TODO: Visualize Boruta results:\n",
    "# 20. Create bar plot showing confirmed, tentative, and rejected counts\n",
    "# 21. Plot feature importance for confirmed features\n",
    "# 22. Show Boruta ranking distribution\n",
    "# 23. If history available, plot importance evolution over iterations\n",
    "#\n",
    "# TODO: Compare with shadow features:\n",
    "# 24. If shadow feature importances are accessible, compare with real features\n",
    "# 25. Show why certain features were confirmed or rejected\n",
    "#\n",
    "# TODO: Print Boruta summary:\n",
    "# 26. Number of confirmed features\n",
    "# 27. Number of tentative features\n",
    "# 28. Number of rejected features\n",
    "# 29. Show list of top confirmed features\n",
    "# 30. Explain decision on tentative features\n",
    "#\n",
    "# Expected output: All-relevant features identified via Boruta algorithm\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"BORUTA ALL-RELEVANT FEATURE SELECTION\")\n",
    "# print(\"=\"*60)\n",
    "#\n",
    "# try:\n",
    "#     from boruta import BorutaPy\n",
    "#     boruta_available = True\n",
    "#     print(\"‚úì Boruta library available\")\n",
    "# except ImportError:\n",
    "#     boruta_available = False\n",
    "#     print(\"‚ö†Ô∏è  Boruta not installed\")\n",
    "#     print(\"To install: pip install Boruta\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 9: Method Comparison and Final Feature Set Selection\n",
    "#\n",
    "# Your task: Compare all 7 methods and create optimized ensemble feature sets\n",
    "#\n",
    "# TODO: Collect all method results:\n",
    "# 1. Gather feature sets from all activities:\n",
    "#    - variance_selected_features (Activity 2)\n",
    "#    - decorrelated_features (Activity 3)\n",
    "#    - de_selected_features (Activity 4)\n",
    "#    - rf_knee_features (Activity 5)\n",
    "#    - ml_selected_features (Activity 6)\n",
    "#    - boruta_selected_features (Activity 7)\n",
    "#    - cox_selected_features (Activity 8)\n",
    "# 2. Handle any missing results if a method wasn't completed\n",
    "#\n",
    "# TODO: Create method comparison matrix:\n",
    "# 3. Get all unique features across methods: all_features = set().union(*all_feature_sets)\n",
    "# 4. Create binary matrix: rows=features, columns=methods, values=1 if selected, 0 otherwise\n",
    "# 5. Calculate vote count for each feature: vote_counts = matrix.sum(axis=1)\n",
    "#\n",
    "# TODO: Calculate method statistics:\n",
    "# 6. For each method, count number of features selected\n",
    "# 7. Calculate pairwise overlap (Jaccard similarity) between methods\n",
    "# 8. Create method similarity matrix\n",
    "# 9. Identify which methods agree most/least\n",
    "#\n",
    "# TODO: Create ensemble feature sets with different strategies:\n",
    "# 10. **Unanimous**: Features selected by all 7 methods (intersection)\n",
    "# 11. **Strong Consensus**: Features selected by ‚â•5 methods (71%+)\n",
    "# 12. **Majority Consensus**: Features selected by ‚â•4 methods (57%+)\n",
    "# 13. **Liberal Consensus**: Features selected by ‚â•3 methods (43%+)\n",
    "# 14. **Union**: Features selected by any method (union of all)\n",
    "#\n",
    "# TODO: Validate ensemble feature sets:\n",
    "# 15. For each ensemble strategy:\n",
    "#     - Train RandomForestClassifier on train set\n",
    "#     - Evaluate on validation set: accuracy, precision, recall, F1, AUC\n",
    "#     - Compare feature set size vs performance\n",
    "# 16. Identify optimal trade-off: performance vs number of features\n",
    "#\n",
    "# TODO: Create stability analysis:\n",
    "# 17. Calculate feature selection frequency across all methods\n",
    "# 18. Identify most stable features (selected by many methods)\n",
    "# 19. Identify method-specific features (selected by only one method)\n",
    "#\n",
    "# TODO: Visualize method comparison:\n",
    "# 20. Create UpSet plot or Venn diagram showing method overlaps\n",
    "# 21. Create heatmap of method similarity matrix\n",
    "# 22. Plot feature set size vs validation performance\n",
    "# 23. Show vote count distribution across features\n",
    "# 24. Create bar plot: number of features per method\n",
    "#\n",
    "# TODO: Recommend final feature sets:\n",
    "# 25. **Minimal Set**: Unanimous features (highest confidence, smallest size)\n",
    "# 26. **Balanced Set**: Majority consensus (good balance of size vs coverage)\n",
    "# 27. **Comprehensive Set**: Liberal consensus (more features, more coverage)\n",
    "# 28. Provide rationale for each recommendation based on use case\n",
    "#\n",
    "# TODO: Print comparison summary:\n",
    "# 29. Table showing: Method | # Features | Overlap with Others\n",
    "# 30. Show feature counts for each ensemble strategy\n",
    "# 31. Display validation performance for each ensemble\n",
    "# 32. List top 20 features by vote count\n",
    "# 33. Recommend which ensemble to use for final modeling\n",
    "#\n",
    "# TODO: Export results:\n",
    "# 34. Save all method results to '../results/feature_selection/'\n",
    "# 35. Export ensemble feature sets (minimal, balanced, comprehensive)\n",
    "# 36. Save method comparison matrices\n",
    "# 37. Create comprehensive feature selection report (JSON or CSV)\n",
    "# 38. Save visualizations\n",
    "#\n",
    "# Expected output: Complete method comparison with optimal ensemble feature sets ready for modeling\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"FEATURE SELECTION METHOD COMPARISON\")\n",
    "# print(\"=\"*60)\n",
    "#\n",
    "# # Collect all feature sets\n",
    "# all_methods = {\n",
    "#     'Variance': variance_selected_features,\n",
    "#     'Decorrelation': decorrelated_features,\n",
    "#     'Diff_Expression': de_selected_features,\n",
    "#     'RandomForest': rf_knee_features,\n",
    "#     'ML_Methods': ml_selected_features,\n",
    "#     'Boruta': boruta_selected_features,\n",
    "#     'Cox': cox_selected_features\n",
    "# }\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40b00e",
   "metadata": {},
   "source": [
    "## 9. Compare Methods and Create Final Feature Set\n",
    "\n",
    "Compare all 7 feature selection methods and create optimal ensemble feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32191a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 8: Cox Regression for Survival-Based Feature Association\n",
    "#\n",
    "# Your task: Identify genes associated with survival outcomes using Cox proportional hazards models\n",
    "#\n",
    "# TODO: Check lifelines availability:\n",
    "# 1. Try importing: from lifelines import CoxPHFitter\n",
    "# 2. If not available, provide alternative statistical approach\n",
    "# 3. Print installation instructions if needed: pip install lifelines\n",
    "#\n",
    "# TODO: Prepare survival data:\n",
    "# 4. Extract survival time: duration = y_train['RFS_MONTHS']\n",
    "# 5. Extract event indicator: event = y_train['RFS_STATUS']\n",
    "# 6. Handle missing values in survival data\n",
    "# 7. Ensure duration > 0 (Cox requirement)\n",
    "#\n",
    "# TODO: Perform univariate Cox regression for each gene:\n",
    "# 8. Initialize results storage: cox_results = {}\n",
    "# 9. For each gene in current_features:\n",
    "#    - Create DataFrame: df = pd.DataFrame({'duration': duration, 'event': event, 'gene': X_train[gene]})\n",
    "#    - Fit Cox model: cph = CoxPHFitter(); cph.fit(df, duration_col='duration', event_col='event')\n",
    "#    - Extract hazard ratio: hr = cph.hazard_ratios_['gene']\n",
    "#    - Extract p-value: p_val = cph.summary.loc['gene', 'p']\n",
    "#    - Store results: cox_results[gene] = {'hr': hr, 'p_value': p_val}\n",
    "# 10. Handle fitting errors gracefully (some genes may fail to converge)\n",
    "#\n",
    "# TODO: Compile Cox results:\n",
    "# 11. Create DataFrame: cox_df = pd.DataFrame(cox_results).T\n",
    "# 12. Calculate log hazard ratios: cox_df['log_hr'] = np.log(cox_df['hr'])\n",
    "# 13. Calculate -log10(p-value): cox_df['neg_log_p'] = -np.log10(cox_df['p_value'])\n",
    "# 14. Sort by statistical significance\n",
    "#\n",
    "# TODO: Apply multiple testing correction:\n",
    "# 15. Import: from statsmodels.stats.multitest import multipletests\n",
    "# 16. Correct p-values: reject, p_adj, _, _ = multipletests(cox_df['p_value'], method='fdr_bh')\n",
    "# 17. Add to results: cox_df['p_adj'] = p_adj\n",
    "#\n",
    "# TODO: Select significant genes:\n",
    "# 18. Set thresholds:\n",
    "#     - p_adj < 0.05 (statistical significance)\n",
    "#     - HR > 1.5 or HR < 0.67 (meaningful effect size: 50% increase/decrease in hazard)\n",
    "# 19. Select significant genes: cox_significant = cox_df[(cox_df['p_adj'] < 0.05) & ((cox_df['hr'] > 1.5) | (cox_df['hr'] < 0.67))]\n",
    "# 20. Get selected features: cox_selected_features = cox_significant.index.tolist()\n",
    "#\n",
    "# TODO: Alternative if lifelines unavailable:\n",
    "# 21. Use correlation between gene expression and survival time\n",
    "# 22. Use logistic regression with survival status as binary outcome\n",
    "# 23. Apply statistical tests comparing expression in patients with events vs without\n",
    "#\n",
    "# TODO: Visualize Cox results:\n",
    "# 24. Create volcano plot: x=log(HR), y=-log10(p_adj), color by significance\n",
    "# 25. Create forest plot showing hazard ratios with confidence intervals (if available)\n",
    "# 26. Show distribution of hazard ratios\n",
    "# 27. Highlight top genes with strongest associations\n",
    "#\n",
    "# TODO: Interpret Cox results:\n",
    "# 28. Identify protective genes (HR < 1): lower expression = higher risk\n",
    "# 29. Identify risk genes (HR > 1): higher expression = higher risk\n",
    "# 30. Compare biological interpretation with clinical expectations\n",
    "#\n",
    "# TODO: Print Cox summary:\n",
    "# 31. Total genes tested\n",
    "# 32. Number of significant genes after correction\n",
    "# 33. Number of protective vs risk genes\n",
    "# 34. Show top 20 genes by hazard ratio and significance\n",
    "# 35. Report genes with clinical relevance if known\n",
    "#\n",
    "# Expected output: Survival-associated genes identified via Cox regression\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"COX REGRESSION-BASED SURVIVAL ASSOCIATION\")\n",
    "# print(\"=\"*60)\n",
    "#\n",
    "# try:\n",
    "#     from lifelines import CoxPHFitter\n",
    "#     lifelines_available = True\n",
    "#     print(\"‚úì Lifelines library available\")\n",
    "# except ImportError:\n",
    "#     lifelines_available = False\n",
    "#     print(\"‚ö†Ô∏è  Lifelines not installed\")\n",
    "#     print(\"To install: pip install lifelines\")\n",
    "#     print(\"Using alternative statistical approach...\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e605e3e",
   "metadata": {},
   "source": [
    "## 8. Cox Regression-Based Association\n",
    "\n",
    "Use survival analysis to identify genes associated with recurrence-free survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b6e07",
   "metadata": {},
   "source": [
    "## üí° Coding Hints and Templates\n",
    "\n",
    "Need help getting started? Here are code templates for each feature selection method:\n",
    "\n",
    "### üìã **Template 1: Advanced Variance Filtering**\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "gene_variances = X_train.var()\n",
    "threshold = np.percentile(gene_variances, 75)  # Top 25%\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "selector.fit(X_train)\n",
    "variance_selected = X_train.columns[selector.get_support()]\n",
    "```\n",
    "\n",
    "### üìã **Template 2: De-correlation**\n",
    "```python\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = np.triu(corr_matrix, k=1)\n",
    "to_drop = [column for column in corr_matrix.columns \n",
    "           if any(upper[corr_matrix.columns.get_loc(column)] > 0.85)]\n",
    "decorrelated_features = [f for f in X_train.columns if f not in to_drop]\n",
    "```\n",
    "\n",
    "### üìã **Template 3: Differential Expression**\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "de_results = []\n",
    "for gene in X_train.columns:\n",
    "    group0 = X_train.loc[y_train['RFS_STATUS'] == 0, gene]\n",
    "    group1 = X_train.loc[y_train['RFS_STATUS'] == 1, gene]\n",
    "    t_stat, p_val = ttest_ind(group0, group1)\n",
    "    log2fc = np.log2((group1.mean() + 1e-10) / (group0.mean() + 1e-10))\n",
    "    de_results.append({'gene': gene, 'log2fc': log2fc, 'p_value': p_val})\n",
    "\n",
    "de_df = pd.DataFrame(de_results).set_index('gene')\n",
    "_, p_adj, _, _ = multipletests(de_df['p_value'], method='fdr_bh')\n",
    "de_df['p_adj'] = p_adj\n",
    "de_selected = de_df[(de_df['p_adj'] < 0.05) & (de_df['log2fc'].abs() > 1)].index\n",
    "```\n",
    "\n",
    "### üìã **Template 4: Random Forest with KneeLocator**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from kneed import KneeLocator\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train['RFS_STATUS'])\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "kl = KneeLocator(range(len(importances)), importances.values, \n",
    "                 curve='convex', direction='decreasing')\n",
    "rf_selected = importances.iloc[:kl.knee].index\n",
    "```\n",
    "\n",
    "### üìã **Template 5: LASSO Selection**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\n",
    "lasso.fit(X_train, y_train['RFS_STATUS'])\n",
    "lasso_selected = X_train.columns[lasso.coef_[0] != 0]\n",
    "```\n",
    "\n",
    "### üìã **Template 6: Boruta**\n",
    "```python\n",
    "from boruta import BorutaPy\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\n",
    "boruta = BorutaPy(rf, n_estimators=100, max_iter=100, random_state=42)\n",
    "boruta.fit(X_train.values, y_train['RFS_STATUS'].values)\n",
    "boruta_selected = X_train.columns[boruta.support_]\n",
    "```\n",
    "\n",
    "### üìã **Template 7: Cox Regression**\n",
    "```python\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "cox_results = {}\n",
    "for gene in X_train.columns:\n",
    "    df = pd.DataFrame({\n",
    "        'duration': y_train['RFS_MONTHS'],\n",
    "        'event': y_train['RFS_STATUS'],\n",
    "        'gene': X_train[gene]\n",
    "    })\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df, duration_col='duration', event_col='event')\n",
    "    cox_results[gene] = {\n",
    "        'hr': cph.hazard_ratios_['gene'],\n",
    "        'p': cph.summary.loc['gene', 'p']\n",
    "    }\n",
    "```\n",
    "\n",
    "### üìã **Template 8: Ensemble Voting**\n",
    "```python\n",
    "# Create voting matrix\n",
    "all_features = set().union(*[set(fs) for fs in all_feature_sets])\n",
    "vote_matrix = pd.DataFrame(0, index=all_features, columns=method_names)\n",
    "for method, features in zip(method_names, all_feature_sets):\n",
    "    vote_matrix.loc[features, method] = 1\n",
    "\n",
    "vote_counts = vote_matrix.sum(axis=1)\n",
    "majority_features = vote_counts[vote_counts >= 4].index  # Selected by ‚â•4 methods\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2e2a1",
   "metadata": {},
   "source": [
    "## üéØ Learning Assessment\n",
    "\n",
    "### ‚úÖ **Self-Check Questions**\n",
    "\n",
    "After completing the feature selection activities, you should be able to answer:\n",
    "\n",
    "1. **Advanced Variance Filtering**\n",
    "   - How does this differ from the variance filtering in preprocessing (notebook 2)?\n",
    "   - When should you use stricter variance thresholds?\n",
    "   - What are the trade-offs between aggressive vs conservative variance filtering?\n",
    "\n",
    "2. **De-correlation**\n",
    "   - Why is removing highly correlated features important for some ML algorithms?\n",
    "   - How do you decide which feature to keep when two are highly correlated?\n",
    "   - What correlation threshold is appropriate for your data?\n",
    "\n",
    "3. **Differential Expression**\n",
    "   - What is the difference between fold change and statistical significance?\n",
    "   - Why is multiple testing correction necessary?\n",
    "   - How do you interpret log2 fold change values?\n",
    "\n",
    "4. **Random Forest with KneeLocator**\n",
    "   - What does the \"knee\" point represent in feature importance curves?\n",
    "   - How does RF importance differ from permutation importance?\n",
    "   - What are advantages of automatic threshold selection via KneeLocator?\n",
    "\n",
    "5. **LASSO and SVM Selection**\n",
    "   - How does L1 regularization lead to feature selection?\n",
    "   - What is the relationship between C parameter and number of selected features?\n",
    "   - When should you use LASSO vs SVM for feature selection?\n",
    "\n",
    "6. **Boruta Algorithm**\n",
    "   - What makes Boruta \"all-relevant\" vs \"minimal-optimal\" feature selection?\n",
    "   - How do shadow features help identify important features?\n",
    "   - How should you handle tentative features?\n",
    "\n",
    "7. **Cox Regression**\n",
    "   - What does a hazard ratio > 1 indicate for gene expression?\n",
    "   - How is survival-based selection different from classification-based selection?\n",
    "   - Why are both RFS_MONTHS and RFS_STATUS needed?\n",
    "\n",
    "8. **Method Comparison**\n",
    "   - Which methods tend to select similar features?\n",
    "   - What are advantages of ensemble feature selection?\n",
    "   - How do you balance feature set size vs predictive performance?\n",
    "\n",
    "### üèÜ **Success Criteria**\n",
    "\n",
    "You have successfully completed this notebook if you can:\n",
    "- ‚úÖ Apply all 7 feature selection methods independently\n",
    "- ‚úÖ Understand the theoretical basis of each method\n",
    "- ‚úÖ Interpret results from statistical and ML-based approaches\n",
    "- ‚úÖ Compare and contrast different selection strategies\n",
    "- ‚úÖ Create ensemble feature sets using voting\n",
    "- ‚úÖ Validate feature selections on held-out data\n",
    "- ‚úÖ Export optimized feature sets for downstream modeling\n",
    "- ‚úÖ Make informed decisions about which features to use\n",
    "\n",
    "### üöÄ **Extension Challenges** (Optional)\n",
    "\n",
    "For advanced students:\n",
    "1. Implement stability selection using bootstrap resampling\n",
    "2. Use SHAP values for feature importance ranking\n",
    "3. Apply recursive feature elimination with cross-validation (RFECV)\n",
    "4. Integrate pathway/network information into feature selection\n",
    "5. Implement multi-objective optimization (accuracy vs interpretability vs cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa9ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Feature Selection Summary\n",
    "\n",
    "In this notebook, you have successfully completed:\n",
    "\n",
    "### ‚úÖ **Completed Tasks:**\n",
    "1. **Data Loading**: Loaded preprocessed data from notebook 2\n",
    "2. **Advanced Variance Filtering**: Applied stricter thresholds than preprocessing\n",
    "3. **De-correlation**: Removed redundant highly correlated features\n",
    "4. **Differential Expression**: Identified category-specific genes using statistical tests\n",
    "5. **Random Forest + KneeLocator**: Used elbow method for optimal feature selection\n",
    "6. **ML-Based Selection**: Applied LASSO and SVM for embedded feature selection\n",
    "7. **Boruta Algorithm**: Identified all-relevant features using shadow features\n",
    "8. **Cox Regression**: Found survival-associated genes\n",
    "9. **Method Comparison**: Created ensemble feature sets from all 7 methods\n",
    "\n",
    "### üéØ **Key Achievements:**\n",
    "- **Comprehensive Analysis**: Applied 7 complementary feature selection methods\n",
    "- **Statistical Rigor**: Used multiple testing correction and proper thresholds\n",
    "- **Automated Optimization**: Used KneeLocator for data-driven threshold selection\n",
    "- **Survival Integration**: Connected gene expression to patient outcomes\n",
    "- **Ensemble Strategy**: Combined methods for robust feature sets\n",
    "- **Validation**: Tested feature sets on held-out validation data\n",
    "\n",
    "### üîÑ **Comparison with Previous Notebooks:**\n",
    "- **Notebook 1 (Exploration)**: Understood data structure and quality\n",
    "- **Notebook 2 (Preprocessing)**: Basic filtering (variance ~0.01), normalization, train/test split\n",
    "- **Notebook 3 (Feature Selection)**: **Advanced filtering** (variance >0.1), statistical testing, ML-based ranking, ensemble selection\n",
    "\n",
    "### üìÅ **Exported Files:**\n",
    "- `../results/feature_selection/`: All method results and rankings\n",
    "- `../results/feature_selection/ensemble_minimal.csv`: High-confidence features\n",
    "- `../results/feature_selection/ensemble_balanced.csv`: Recommended for modeling\n",
    "- `../results/feature_selection/ensemble_comprehensive.csv`: Maximum coverage\n",
    "- `../results/feature_selection/method_comparison.json`: Complete analysis report\n",
    "\n",
    "### üîÑ **Next Steps:**\n",
    "In the next notebook (`04_model_development.ipynb`), we will:\n",
    "1. **Train ML models** using the selected feature sets\n",
    "2. **Compare algorithms**: Random Forest, SVM, Neural Networks, etc.\n",
    "3. **Hyperparameter tuning**: Optimize model performance\n",
    "4. **Clinical evaluation**: Assess models using survival metrics\n",
    "5. **Model interpretation**: Understand feature contributions to predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work completing advanced feature selection! üéâ**\n",
    "\n",
    "You've successfully applied 7 state-of-the-art methods and created optimized feature sets that balance statistical rigor, predictive power, and biological interpretability. These refined feature sets will serve as the foundation for robust machine learning models in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
