{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbaf2fa",
   "metadata": {},
   "source": [
    "# Machine Learning Course: Model Development & Risk Classification\n",
    "\n",
    "## Notebook 4: Comprehensive Model Training, Evaluation, and Risk Stratification\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Implement time-independent classification models for risk prediction\n",
    "2. Develop time-dependent models using survival analysis approaches\n",
    "3. Evaluate models using appropriate metrics (C-index, precision, recall, calibration)\n",
    "4. Stratify patients into meaningful risk groups (low, medium, high risk)\n",
    "5. Compare model performance across different algorithms and feature sets\n",
    "6. Validate model robustness using cross-validation and bootstrap methods\n",
    "7. Interpret model predictions and feature importance\n",
    "8. Select and export final production-ready models\n",
    "\n",
    "### Prerequisites\n",
    "- Completed `01_data_exploration.ipynb`, `02_data_preprocessing.ipynb`, and `03_feature_selection.ipynb`\n",
    "- Understanding of machine learning concepts and evaluation metrics\n",
    "- Basic knowledge of survival analysis and risk stratification\n",
    "\n",
    "### Model Development Framework\n",
    "This notebook implements a comprehensive modeling pipeline:\n",
    "\n",
    "**1. Time-Independent Classification**\n",
    "- Logistic Regression with regularization\n",
    "- Support Vector Machines (SVM)\n",
    "- Random Forest and Gradient Boosting\n",
    "- Neural Networks (Multi-layer Perceptron)\n",
    "\n",
    "**2. Time-Dependent Classification**\n",
    "- Cox Proportional Hazards models\n",
    "- Random Survival Forests\n",
    "- Time-varying coefficient models\n",
    "- Accelerated Failure Time models\n",
    "\n",
    "**3. Evaluation Metrics**\n",
    "- **C-index (Concordance Index)**: Primary metric for ranking predictions\n",
    "- **Precision/Recall**: Focus on precision in low-risk group identification\n",
    "- **Calibration**: How well predicted probabilities match actual outcomes\n",
    "- **Risk Stratification**: Ability to separate patients into meaningful groups\n",
    "\n",
    "**4. Risk Stratification Analysis**\n",
    "- Patient classification into risk groups (low, medium, high)\n",
    "- Survival curve analysis by risk group\n",
    "- Clinical utility assessment\n",
    "- Decision curve analysis\n",
    "\n",
    "**5. Model Selection & Validation**\n",
    "- Cross-validation strategies\n",
    "- Bootstrap validation\n",
    "- Model ensemble approaches\n",
    "- Final model selection and export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5157cd0",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's import all necessary libraries for comprehensive model development, evaluation, and risk stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 1: Import Libraries for Model Development and Risk Classification\n",
    "#\n",
    "# Your task: Import comprehensive libraries for machine learning, survival analysis, and evaluation\n",
    "#\n",
    "# TODO: Import core data manipulation and visualization libraries:\n",
    "# 1. pandas (as pd) - for data manipulation and analysis\n",
    "# 2. numpy (as np) - for numerical operations and array handling\n",
    "# 3. matplotlib.pyplot (as plt) - for plotting and visualization\n",
    "# 4. seaborn (as sns) - for statistical visualization\n",
    "# 5. os, warnings, json - for system operations and data handling\n",
    "#\n",
    "# TODO: Import machine learning libraries:\n",
    "# 6. From sklearn.linear_model import: LogisticRegression, ElasticNet\n",
    "# 7. From sklearn.svm import: SVC\n",
    "# 8. From sklearn.ensemble import: RandomForestClassifier, GradientBoostingClassifier\n",
    "# 9. From sklearn.neural_network import: MLPClassifier\n",
    "# 10. From sklearn.model_selection import: cross_val_score, StratifiedKFold, GridSearchCV\n",
    "# 11. From sklearn.metrics import: roc_auc_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "# 12. From sklearn.calibration import: calibration_curve, CalibratedClassifierCV\n",
    "#\n",
    "# TODO: Import survival analysis libraries:\n",
    "# 13. Try to import lifelines:\n",
    "#     - from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "#     - from lifelines.utils import concordance_index\n",
    "#     - from lifelines.statistics import logrank_test\n",
    "# 14. Try to import scikit-survival (alternative):\n",
    "#     - from sksurv.linear_model import CoxPHRegressor\n",
    "#     - from sksurv.ensemble import RandomSurvivalForest\n",
    "#     - from sksurv.metrics import concordance_index_censored\n",
    "#\n",
    "# TODO: Import additional statistical and evaluation libraries:\n",
    "# 15. From scipy.stats import: chi2_contingency, mannwhitneyu\n",
    "# 16. From scipy import stats\n",
    "# 17. import joblib - for model saving/loading\n",
    "# 18. import time - for timing model training\n",
    "#\n",
    "# TODO: Configure environment:\n",
    "# 19. Suppress warnings: warnings.filterwarnings('ignore')\n",
    "# 20. Set matplotlib and seaborn styles\n",
    "# 21. Set random seeds for reproducibility (np.random.seed(42))\n",
    "# 22. Set pandas display options for better output\n",
    "#\n",
    "# TODO: Check library availability:\n",
    "# 23. Test survival analysis library imports\n",
    "# 24. Print versions of key libraries\n",
    "# 25. Provide installation instructions for missing libraries\n",
    "# 26. Set flags for available functionality\n",
    "#\n",
    "# Expected output: Successfully imported libraries with availability status for survival analysis\n",
    "\n",
    "# Write your code below:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import warnings\n",
    "# import json\n",
    "# import joblib\n",
    "# import time\n",
    "# \n",
    "# # Machine Learning libraries\n",
    "# from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score, \n",
    "#                              classification_report, confusion_matrix, roc_curve, auc)\n",
    "# from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "# \n",
    "# # Statistical libraries\n",
    "# from scipy.stats import chi2_contingency, mannwhitneyu\n",
    "# from scipy import stats\n",
    "# \n",
    "# # Check survival analysis libraries availability\n",
    "# lifelines_available = False\n",
    "# sksurv_available = False\n",
    "# \n",
    "# try:\n",
    "#     from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "#     from lifelines.utils import concordance_index\n",
    "#     from lifelines.statistics import logrank_test\n",
    "#     lifelines_available = True\n",
    "#     print(\"‚úì Lifelines available for survival analysis\")\n",
    "# except ImportError:\n",
    "#     print(\"‚ö†Ô∏è  Lifelines not available. Install with: pip install lifelines\")\n",
    "# \n",
    "# try:\n",
    "#     from sksurv.linear_model import CoxPHRegressor\n",
    "#     from sksurv.ensemble import RandomSurvivalForest\n",
    "#     from sksurv.metrics import concordance_index_censored\n",
    "#     sksurv_available = True\n",
    "#     print(\"‚úì Scikit-survival available for survival analysis\")\n",
    "# except ImportError:\n",
    "#     print(\"‚ö†Ô∏è  Scikit-survival not available. Install with: pip install scikit-survival\")\n",
    "# \n",
    "# # Configure environment\n",
    "# warnings.filterwarnings('ignore')\n",
    "# plt.style.use('default')\n",
    "# sns.set_palette(\"Set2\")\n",
    "# np.random.seed(42)\n",
    "# pd.set_option('display.max_columns', 20)\n",
    "# \n",
    "# print(f\"\\\\nüì¶ Library versions:\")\n",
    "# print(f\"Pandas: {pd.__version__}\")\n",
    "# print(f\"NumPy: {np.__version__}\")\n",
    "# print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "# print(f\"\\\\nüéØ Model development environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c73ec5",
   "metadata": {},
   "source": [
    "## 2. Load Data and Feature Sets\n",
    "\n",
    "Load the preprocessed data and selected feature sets from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ad5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 2: Load Preprocessed Data and Selected Feature Sets\n",
    "#\n",
    "# Your task: Load clean datasets and optimized feature sets for model development\n",
    "#\n",
    "# TODO: Set up data paths:\n",
    "# 1. Define PREPROCESSED_PATH = '../results/preprocessed/'\n",
    "# 2. Define FEATURE_SELECTION_PATH = '../results/feature_selection/'\n",
    "# 3. Define MODELS_PATH = '../results/models/'\n",
    "# 4. Create models directory: os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "#\n",
    "# TODO: Load training and validation data:\n",
    "# 5. Load training features: X_train = pd.read_csv(PREPROCESSED_PATH + 'splits/X_train.csv', index_col=0)\n",
    "# 6. Load training target: y_train = pd.read_csv(PREPROCESSED_PATH + 'splits/y_train.csv', index_col=0)['target']\n",
    "# 7. Load validation data: X_val, y_val\n",
    "# 8. Load test data: X_test, y_test (for final evaluation)\n",
    "# 9. Print data dimensions and class distributions\n",
    "#\n",
    "# TODO: Load selected feature sets:\n",
    "# 10. Load feature selection results (if available):\n",
    "#     - Load ensemble feature sets from feature selection notebook\n",
    "#     - Load method-specific feature sets (RF, Boruta, correlation, etc.)\n",
    "#     - Handle case where feature selection results might not exist\n",
    "# 11. If feature selection results not available, use all features\n",
    "#\n",
    "# TODO: Prepare multiple feature sets for comparison:\n",
    "# 12. Create different feature sets for model comparison:\n",
    "#     - full_features: all available features\n",
    "#     - top_100_features: top 100 by variance or correlation\n",
    "#     - top_50_features: top 50 most important features\n",
    "#     - ensemble_features: if available from feature selection\n",
    "# 13. Print feature set sizes and overlap analysis\n",
    "#\n",
    "# TODO: Load survival data (if available):\n",
    "# 14. Check if survival information is available in clinical data\n",
    "# 15. If available, prepare survival DataFrame with:\n",
    "#     - duration: survival time (OS_MONTHS or similar)\n",
    "#     - event: event indicator (1 if event occurred, 0 if censored)\n",
    "# 16. Align survival data with feature matrices\n",
    "# 17. Handle missing survival data appropriately\n",
    "#\n",
    "# TODO: Create data summary:\n",
    "# 18. Print comprehensive data loading summary:\n",
    "#     - Sample counts for train/validation/test\n",
    "#     - Feature set sizes and descriptions\n",
    "#     - Class balance in each split\n",
    "#     - Survival data availability\n",
    "#     - Memory usage statistics\n",
    "#\n",
    "# TODO: Validate data integrity:\n",
    "# 19. Check for any remaining missing values\n",
    "# 20. Verify that indices align across X and y\n",
    "# 21. Confirm data types are appropriate for modeling\n",
    "# 22. Check for any data leakage between splits\n",
    "#\n",
    "# Expected output: Loaded datasets with multiple feature sets ready for model development\n",
    "\n",
    "# Write your code below:\n",
    "# PREPROCESSED_PATH = '../results/preprocessed/'\n",
    "# FEATURE_SELECTION_PATH = '../results/feature_selection/'\n",
    "# MODELS_PATH = '../results/models/'\n",
    "# os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "# \n",
    "# print(\"LOADING PREPROCESSED DATA AND FEATURE SETS\")\n",
    "# print(\"=\"*50)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499dc1f",
   "metadata": {},
   "source": [
    "## 3. Time-Independent Classification Models\n",
    "\n",
    "Develop classification models that predict risk without considering time-to-event information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362461e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 3: Train Time-Independent Classification Models\n",
    "#\n",
    "# Your task: Implement multiple classification algorithms for risk prediction\n",
    "#\n",
    "# TODO: Set up model training framework:\n",
    "# 1. Print \"TIME-INDEPENDENT CLASSIFICATION MODELS\" header with separators\n",
    "# 2. Initialize dictionary to store model results: model_results = {}\n",
    "# 3. Set up cross-validation strategy: cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#\n",
    "# TODO: Define model configurations:\n",
    "# 4. Create dictionary of models to evaluate:\n",
    "#    models = {\n",
    "#        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "#        'Logistic Regression (L1)': LogisticRegression(penalty='l1', solver='liblinear', random_state=42),\n",
    "#        'Logistic Regression (L2)': LogisticRegression(penalty='l2', random_state=42, max_iter=1000),\n",
    "#        'SVM (RBF)': SVC(probability=True, random_state=42),\n",
    "#        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "#        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "#        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "#    }\n",
    "#\n",
    "# TODO: Train models on each feature set:\n",
    "# 5. For each feature set (full_features, top_100, top_50, ensemble):\n",
    "#    - Create feature subset: X_train_subset = X_train[feature_set]\n",
    "#    - For each model in models dictionary:\n",
    "#      - Train model using cross-validation\n",
    "#      - Calculate performance metrics\n",
    "#      - Store results in model_results\n",
    "#\n",
    "# TODO: Implement comprehensive model evaluation:\n",
    "# 6. For each model and feature set combination:\n",
    "#    - Calculate cross-validation scores: cv_scores = cross_val_score(model, X_train_subset, y_train, cv=cv, scoring='roc_auc')\n",
    "#    - Calculate mean and std of CV scores\n",
    "#    - Fit model on full training set: model.fit(X_train_subset, y_train)\n",
    "#    - Make predictions on validation set: y_val_pred = model.predict_proba(X_val_subset)[:, 1]\n",
    "#    - Calculate validation metrics: AUC, precision, recall, F1-score\n",
    "#\n",
    "# TODO: Calculate additional evaluation metrics:\n",
    "# 7. For each trained model:\n",
    "#    - Calculate C-index (concordance index): c_index = roc_auc_score(y_val, y_val_pred)\n",
    "#    - Calculate precision in low-risk group: identify patients predicted as low risk, calculate precision\n",
    "#    - Calculate recall for high-risk group: sensitivity for identifying high-risk patients\n",
    "#    - Calculate calibration metrics: calibration_curve(y_val, y_val_pred, n_bins=10)\n",
    "#\n",
    "# TODO: Implement model calibration:\n",
    "# 8. For each model, create calibrated version:\n",
    "#    - calibrated_model = CalibratedClassifierCV(base_model, method='platt', cv=3)\n",
    "#    - Fit calibrated model and evaluate calibration improvement\n",
    "#    - Store calibrated predictions for comparison\n",
    "#\n",
    "# TODO: Create performance comparison:\n",
    "# 9. Create summary DataFrame with all model results:\n",
    "#    - Columns: Model, Feature Set, CV AUC (mean¬±std), Validation AUC, Precision, Recall, F1, C-index\n",
    "#    - Sort by validation AUC or C-index\n",
    "#    - Identify top-performing models\n",
    "#\n",
    "# TODO: Visualize model performance:\n",
    "# 10. Create ROC curves for top models\n",
    "# 11. Create calibration plots comparing models\n",
    "# 12. Create feature importance plots for tree-based models\n",
    "# 13. Create performance comparison bar plots\n",
    "#\n",
    "# TODO: Analyze feature importance:\n",
    "# 14. For models that provide feature importance (RF, GB):\n",
    "#     - Extract feature importance scores\n",
    "#     - Create feature importance rankings\n",
    "#     - Compare feature importance across models\n",
    "#     - Identify most consistently important features\n",
    "#\n",
    "# TODO: Save model results:\n",
    "# 15. Save trained models: joblib.dump(model, MODELS_PATH + f'{model_name}_{feature_set}.pkl')\n",
    "# 16. Save model performance summary\n",
    "# 17. Save predictions on validation set\n",
    "# 18. Create model training report\n",
    "#\n",
    "# Expected output: Trained time-independent models with comprehensive performance evaluation\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"TIME-INDEPENDENT CLASSIFICATION MODELS\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# # Initialize results storage\n",
    "# model_results = {}\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424246d4",
   "metadata": {},
   "source": [
    "## 4. Time-Dependent Classification Models\n",
    "\n",
    "Develop survival analysis models that incorporate time-to-event information for risk prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 4: Train Time-Dependent Classification Models\n",
    "#\n",
    "# Your task: Implement survival analysis models for time-dependent risk prediction\n",
    "#\n",
    "# TODO: Set up survival analysis framework:\n",
    "# 1. Print \"TIME-DEPENDENT CLASSIFICATION MODELS\" header with separators\n",
    "# 2. Check if survival data is available from data loading\n",
    "# 3. Initialize dictionary to store survival model results: survival_results = {}\n",
    "#\n",
    "# TODO: Prepare survival data:\n",
    "# 4. If survival data available:\n",
    "#    - Create survival DataFrame with duration, event, and features\n",
    "#    - Ensure proper data types: duration as float, event as bool/int\n",
    "#    - Handle missing or negative survival times\n",
    "# 5. If survival data not available:\n",
    "#    - Create proxy survival data using target variable and synthetic times\n",
    "#    - Note limitations of proxy data approach\n",
    "#\n",
    "# TODO: Implement Cox Proportional Hazards models (if lifelines available):\n",
    "# 6. For each feature set:\n",
    "#    - Create Cox model: cph = CoxPHFitter()\n",
    "#    - Prepare data: survival_df = pd.concat([X_train_subset, survival_data], axis=1)\n",
    "#    - Fit model: cph.fit(survival_df, duration_col='duration', event_col='event')\n",
    "#    - Calculate C-index: c_index = cph.concordance_index_\n",
    "#    - Extract hazard ratios and confidence intervals\n",
    "#    - Make risk predictions: risk_scores = cph.predict_partial_hazard(X_val_subset)\n",
    "#\n",
    "# TODO: Implement alternative Cox models (if scikit-survival available):\n",
    "# 7. Use scikit-survival for Cox regression:\n",
    "#    - Create structured array for survival: y_structured = np.array([(event, duration) for event, duration in zip(events, durations)], dtype=[('event', bool), ('duration', float)])\n",
    "#    - Fit Cox model: cox_model = CoxPHRegressor().fit(X_train_subset, y_structured)\n",
    "#    - Calculate C-index: c_index = concordance_index_censored(events_val, durations_val, risk_scores)[0]\n",
    "#\n",
    "# TODO: Implement Random Survival Forest (if available):\n",
    "# 8. Train RSF model:\n",
    "#    - rsf_model = RandomSurvivalForest(n_estimators=100, random_state=42)\n",
    "#    - rsf_model.fit(X_train_subset, y_structured)\n",
    "#    - Get risk scores: risk_scores = rsf_model.predict(X_val_subset)\n",
    "#    - Calculate C-index and other metrics\n",
    "#\n",
    "# TODO: Alternative time-dependent approach (if survival libraries unavailable):\n",
    "# 9. Implement time-stratified classification:\n",
    "#    - Create time-based risk groups using target and available time information\n",
    "#    - Train time-specific classifiers for different time horizons\n",
    "#    - Combine predictions across time points\n",
    "#    - Calculate time-dependent AUC\n",
    "#\n",
    "# TODO: Evaluate survival model performance:\n",
    "# 10. For each survival model:\n",
    "#     - Calculate C-index (primary metric for survival models)\n",
    "#     - Calculate time-dependent AUC at different time points (if possible)\n",
    "#     - Assess proportional hazards assumption (for Cox models)\n",
    "#     - Calculate log-likelihood and AIC for model comparison\n",
    "#\n",
    "# TODO: Create risk stratification using survival models:\n",
    "# 11. Use model predictions to create risk groups:\n",
    "#     - Calculate risk score percentiles (tertiles or quartiles)\n",
    "#     - Create low, medium, high risk groups based on score thresholds\n",
    "#     - Validate risk group separation using log-rank tests\n",
    "#\n",
    "# TODO: Visualize survival model results:\n",
    "# 12. Create Kaplan-Meier curves by risk group\n",
    "# 13. Plot hazard ratios with confidence intervals (for Cox models)\n",
    "# 14. Create time-dependent ROC curves (if possible)\n",
    "# 15. Show survival probability predictions\n",
    "#\n",
    "# TODO: Compare time-dependent vs time-independent models:\n",
    "# 16. Compare C-index between survival models and classification models\n",
    "# 17. Assess which approach better stratifies patients\n",
    "# 18. Analyze advantages and limitations of each approach\n",
    "#\n",
    "# TODO: Handle missing survival analysis libraries:\n",
    "# 19. If neither lifelines nor scikit-survival available:\n",
    "#     - Implement simplified time-dependent analysis\n",
    "#     - Use logistic regression with time-stratified outcomes\n",
    "#     - Provide guidance on installing survival analysis libraries\n",
    "#     - Note limitations of simplified approach\n",
    "#\n",
    "# TODO: Save survival model results:\n",
    "# 20. Save trained survival models\n",
    "# 21. Save risk scores and group assignments\n",
    "# 22. Create survival analysis report\n",
    "# 23. Document model assumptions and limitations\n",
    "#\n",
    "# Expected output: Time-dependent models with C-index evaluation and risk stratification\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"TIME-DEPENDENT CLASSIFICATION MODELS\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# # Check survival data availability\n",
    "# if 'survival_data' in locals() and survival_data is not None:\n",
    "#     print(\"‚úì Survival data available for time-dependent modeling\")\n",
    "#     survival_available = True\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  No survival data available - using proxy approach\")\n",
    "#     survival_available = False\n",
    "# \n",
    "# # Check survival analysis libraries\n",
    "# if lifelines_available:\n",
    "#     print(\"‚úì Using lifelines for Cox regression\")\n",
    "# elif sksurv_available:\n",
    "#     print(\"‚úì Using scikit-survival for survival analysis\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  No survival analysis libraries - using time-stratified classification\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390861",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Model Evaluation\n",
    "\n",
    "Evaluate all models using appropriate metrics including C-index, precision in low risk, and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 5: Comprehensive Model Evaluation with Clinical Metrics\n",
    "#\n",
    "# Your task: Evaluate all models using clinically relevant metrics and assessment frameworks\n",
    "#\n",
    "# TODO: Set up comprehensive evaluation framework:\n",
    "# 1. Print \"COMPREHENSIVE MODEL EVALUATION\" header with separators\n",
    "# 2. Combine results from time-independent and time-dependent models\n",
    "# 3. Create unified evaluation DataFrame: all_model_results = pd.DataFrame()\n",
    "#\n",
    "# TODO: Calculate C-index for all models:\n",
    "# 4. For time-independent models:\n",
    "#    - C-index = AUC (for binary classification, C-index equals AUC)\n",
    "#    - Calculate using: c_index = roc_auc_score(y_true, y_pred_proba)\n",
    "# 5. For time-dependent models:\n",
    "#    - Use survival-specific concordance index\n",
    "#    - If lifelines: cph.concordance_index_\n",
    "#    - If scikit-survival: concordance_index_censored()\n",
    "# 6. Compare C-index values across all models\n",
    "#\n",
    "# TODO: Calculate precision in low-risk group:\n",
    "# 7. For each model's risk predictions:\n",
    "#    - Define low-risk threshold (e.g., bottom 25% or 33% of risk scores)\n",
    "#    - low_risk_threshold = np.percentile(risk_scores, 25)\n",
    "#    - low_risk_patients = risk_scores <= low_risk_threshold\n",
    "#    - Calculate precision in low-risk group: precision_low_risk = (true_negatives) / (predicted_low_risk)\n",
    "#    - This measures how well the model identifies truly low-risk patients\n",
    "#\n",
    "# TODO: Calculate precision in high-risk group:\n",
    "# 8. Define high-risk threshold (e.g., top 25% of risk scores)\n",
    "# 9. high_risk_threshold = np.percentile(risk_scores, 75)\n",
    "# 10. Calculate precision in high-risk group: how many predicted high-risk are truly high-risk\n",
    "# 11. Calculate recall for high-risk: sensitivity for detecting high-risk patients\n",
    "#\n",
    "# TODO: Assess model calibration:\n",
    "# 12. For each model with probability outputs:\n",
    "#     - Calculate calibration curve: fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "#     - Calculate Brier score: brier_score = np.mean((y_prob - y_true) ** 2)\n",
    "#     - Calculate calibration slope and intercept\n",
    "#     - Assess reliability diagram visually\n",
    "#\n",
    "# TODO: Calculate additional clinical metrics:\n",
    "# 13. Sensitivity (recall): ability to identify high-risk patients\n",
    "# 14. Specificity: ability to identify low-risk patients correctly\n",
    "# 15. Positive Predictive Value (PPV): precision for high-risk predictions\n",
    "# 16. Negative Predictive Value (NPV): precision for low-risk predictions\n",
    "# 17. Number Needed to Treat (NNT) estimates (if applicable)\n",
    "#\n",
    "# TODO: Implement risk stratification evaluation:\n",
    "# 18. For each model, create risk groups (tertiles or quartiles):\n",
    "#     - low_risk = bottom 33%, medium_risk = middle 33%, high_risk = top 33%\n",
    "#     - Calculate event rates in each risk group\n",
    "#     - Test statistical separation between groups using chi-square test\n",
    "#     - Calculate hazard ratios between risk groups (if survival data available)\n",
    "#\n",
    "# TODO: Create comprehensive evaluation metrics table:\n",
    "# 19. Create evaluation summary with columns:\n",
    "#     - Model Name, Feature Set, C-index, AUC, Precision (Low Risk), Precision (High Risk)\n",
    "#     - Sensitivity, Specificity, PPV, NPV, Brier Score, Calibration Slope\n",
    "# 20. Sort by C-index or clinical utility metrics\n",
    "# 21. Highlight top-performing models\n",
    "#\n",
    "# TODO: Visualize model evaluation results:\n",
    "# 22. Create ROC curves comparison plot\n",
    "# 23. Create calibration plots for all models\n",
    "# 24. Create precision-recall curves\n",
    "# 25. Create C-index comparison bar plot\n",
    "# 26. Create risk group separation visualization\n",
    "#\n",
    "# TODO: Assess model clinical utility:\n",
    "# 27. Calculate decision curve analysis (if possible):\n",
    "#     - Plot net benefit across probability thresholds\n",
    "#     - Compare with treat-all and treat-none strategies\n",
    "# 28. Calculate area under the decision curve\n",
    "# 29. Identify optimal probability thresholds for clinical decisions\n",
    "#\n",
    "# TODO: Evaluate model stability and robustness:\n",
    "# 30. Bootstrap evaluation (if computationally feasible):\n",
    "#     - Calculate confidence intervals for C-index and other metrics\n",
    "#     - Assess metric stability across bootstrap samples\n",
    "# 31. Cross-validation consistency analysis\n",
    "# 32. Feature importance stability assessment\n",
    "#\n",
    "# TODO: Create model ranking and selection criteria:\n",
    "# 33. Rank models by multiple criteria:\n",
    "#     - Primary: C-index or AUC\n",
    "#     - Secondary: Precision in low-risk group\n",
    "#     - Tertiary: Calibration quality\n",
    "#     - Clinical utility: Decision curve analysis\n",
    "# 34. Create weighted scoring system combining multiple metrics\n",
    "# 35. Identify top 3-5 models for further validation\n",
    "#\n",
    "# Expected output: Comprehensive evaluation report with clinical relevance assessment\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# def calculate_precision_in_risk_group(y_true, risk_scores, percentile_low=25, percentile_high=75):\n",
    "#     \\\"\\\"\\\"Calculate precision in low and high risk groups\\\"\\\"\\\"\n",
    "#     low_threshold = np.percentile(risk_scores, percentile_low)\n",
    "#     high_threshold = np.percentile(risk_scores, percentile_high)\n",
    "#     \n",
    "#     # Low risk group precision (NPV-like metric)\n",
    "#     low_risk_mask = risk_scores <= low_threshold\n",
    "#     if np.sum(low_risk_mask) > 0:\n",
    "#         precision_low = np.sum((y_true == 0) & low_risk_mask) / np.sum(low_risk_mask)\n",
    "#     else:\n",
    "#         precision_low = np.nan\n",
    "#     \n",
    "#     # High risk group precision (PPV-like metric)\n",
    "#     high_risk_mask = risk_scores >= high_threshold\n",
    "#     if np.sum(high_risk_mask) > 0:\n",
    "#         precision_high = np.sum((y_true == 1) & high_risk_mask) / np.sum(high_risk_mask)\n",
    "#     else:\n",
    "#         precision_high = np.nan\n",
    "#     \n",
    "#     return precision_low, precision_high\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d0008",
   "metadata": {},
   "source": [
    "## 6. Risk Stratification Analysis\n",
    "\n",
    "Stratify patients into meaningful risk groups and evaluate the clinical utility of these classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 6: Patient Risk Stratification and Clinical Utility Analysis\n",
    "#\n",
    "# Your task: Create meaningful patient risk groups and evaluate their clinical utility\n",
    "#\n",
    "# TODO: Set up risk stratification framework:\n",
    "# 1. Print \"PATIENT RISK STRATIFICATION ANALYSIS\" header with separators\n",
    "# 2. Select top-performing models from evaluation results\n",
    "# 3. Initialize risk stratification results storage: stratification_results = {}\n",
    "#\n",
    "# TODO: Create risk group definitions:\n",
    "# 4. For each top model, define risk groups using different strategies:\n",
    "#    - Tertiles: low (0-33%), medium (33-67%), high (67-100%)\n",
    "#    - Quartiles: low (0-25%), low-medium (25-50%), medium-high (50-75%), high (75-100%)\n",
    "#    - Clinical thresholds: if known risk score cutoffs exist\n",
    "#    - Equal-sized groups vs outcome-based groups\n",
    "#\n",
    "# TODO: Implement risk group assignment:\n",
    "# 5. For each model and stratification strategy:\n",
    "#    - Calculate risk score percentiles: risk_percentiles = np.percentile(risk_scores, [33, 67])\n",
    "#    - Assign patients to groups: \n",
    "#      * low_risk = risk_scores <= risk_percentiles[0]\n",
    "#      * medium_risk = (risk_scores > risk_percentiles[0]) & (risk_scores <= risk_percentiles[1])\n",
    "#      * high_risk = risk_scores > risk_percentiles[1]\n",
    "#    - Store group assignments and score thresholds\n",
    "#\n",
    "# TODO: Evaluate risk group separation:\n",
    "# 6. For each risk stratification:\n",
    "#    - Calculate event rates in each group: event_rate_low, event_rate_medium, event_rate_high\n",
    "#    - Test statistical significance: chi2, p_value = chi2_contingency([[events_low, non_events_low], [events_high, non_events_high]])\n",
    "#    - Calculate odds ratios between groups\n",
    "#    - Assess monotonic relationship: low < medium < high event rates\n",
    "#\n",
    "# TODO: Survival analysis by risk group (if survival data available):\n",
    "# 7. Create Kaplan-Meier survival curves for each risk group:\n",
    "#    - Use KaplanMeierFitter for each group\n",
    "#    - kmf_low.fit(durations_low, events_low, label='Low Risk')\n",
    "#    - Plot survival curves with confidence intervals\n",
    "# 8. Perform log-rank tests between groups:\n",
    "#    - Test low vs high: logrank_test(durations_low, durations_high, events_low, events_high)\n",
    "#    - Test pairwise differences between all groups\n",
    "# 9. Calculate hazard ratios between risk groups\n",
    "#\n",
    "# TODO: Calculate clinical utility metrics:\n",
    "# 10. For each risk stratification strategy:\n",
    "#     - Net Reclassification Improvement (NRI): improvement in patient classification\n",
    "#     - Integrated Discrimination Improvement (IDI): improvement in risk discrimination\n",
    "#     - Decision Curve Analysis: net benefit across probability thresholds\n",
    "#     - Number needed to screen/treat for each risk group\n",
    "#\n",
    "# TODO: Assess risk group characteristics:\n",
    "# 11. For each risk group, analyze:\n",
    "#     - Demographic characteristics (age, stage, etc.)\n",
    "#     - Biomarker profiles (if available)\n",
    "#     - Treatment response patterns (if available)\n",
    "#     - Survival outcomes and follow-up times\n",
    "#\n",
    "# TODO: Create comprehensive risk stratification visualization:\n",
    "# 12. Create multi-panel figure showing:\n",
    "#     - Risk score distributions by group\n",
    "#     - Event rates by risk group (bar plot with confidence intervals)\n",
    "#     - Kaplan-Meier survival curves (if applicable)\n",
    "#     - ROC curves with group-specific performance\n",
    "#\n",
    "# TODO: Validate risk group stability:\n",
    "# 13. Bootstrap analysis of risk group assignments:\n",
    "#     - Calculate stability of group membership across bootstrap samples\n",
    "#     - Assess consistency of event rate differences\n",
    "#     - Evaluate robustness of survival curve separation\n",
    "#\n",
    "# TODO: Compare risk stratification approaches:\n",
    "# 14. Compare different models' risk stratification:\n",
    "#     - Agreement in risk group assignments between models\n",
    "#     - Correlation of risk scores across models\n",
    "#     - Clinical utility comparison (which provides better stratification)\n",
    "#\n",
    "# TODO: Create clinical interpretation:\n",
    "# 15. For the best risk stratification approach:\n",
    "#     - Define clinical meaning of each risk group\n",
    "#     - Suggest treatment/monitoring strategies for each group\n",
    "#     - Calculate absolute risk estimates with confidence intervals\n",
    "#     - Provide risk communication guidelines\n",
    "#\n",
    "# TODO: Evaluate practical implementation:\n",
    "# 16. Assess feasibility of risk stratification in clinical practice:\n",
    "#     - Feature availability and measurement costs\n",
    "#     - Model complexity and interpretability\n",
    "#     - Integration with existing clinical workflows\n",
    "#     - Potential for clinical decision support\n",
    "#\n",
    "# TODO: Save risk stratification results:\n",
    "# 17. Export risk group assignments for validation set\n",
    "# 18. Save risk score thresholds and group definitions\n",
    "# 19. Create clinical implementation guide\n",
    "# 20. Generate risk stratification report with interpretation\n",
    "#\n",
    "# Expected output: Clinically meaningful risk groups with comprehensive evaluation and implementation guidance\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"PATIENT RISK STRATIFICATION ANALYSIS\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# def create_risk_groups(risk_scores, strategy='tertiles'):\n",
    "#     \\\"\\\"\\\"Create risk groups using different strategies\\\"\\\"\\\"\n",
    "#     if strategy == 'tertiles':\n",
    "#         thresholds = np.percentile(risk_scores, [33.33, 66.67])\n",
    "#         groups = np.where(risk_scores <= thresholds[0], 'Low',\n",
    "#                          np.where(risk_scores <= thresholds[1], 'Medium', 'High'))\n",
    "#     elif strategy == 'quartiles':\n",
    "#         thresholds = np.percentile(risk_scores, [25, 50, 75])\n",
    "#         groups = np.where(risk_scores <= thresholds[0], 'Low',\n",
    "#                          np.where(risk_scores <= thresholds[1], 'Low-Medium',\n",
    "#                                  np.where(risk_scores <= thresholds[2], 'Medium-High', 'High')))\n",
    "#     else:  # binary\n",
    "#         threshold = np.percentile(risk_scores, 50)\n",
    "#         groups = np.where(risk_scores <= threshold, 'Low', 'High')\n",
    "#         thresholds = [threshold]\n",
    "#     \n",
    "#     return groups, thresholds\n",
    "# \n",
    "# def evaluate_risk_group_separation(y_true, risk_groups):\n",
    "#     \\\"\\\"\\\"Evaluate statistical separation between risk groups\\\"\\\"\\\"\n",
    "#     unique_groups = np.unique(risk_groups)\n",
    "#     event_rates = {}\n",
    "#     \n",
    "#     for group in unique_groups:\n",
    "#         mask = risk_groups == group\n",
    "#         event_rate = np.mean(y_true[mask])\n",
    "#         n_events = np.sum(y_true[mask])\n",
    "#         n_total = np.sum(mask)\n",
    "#         \n",
    "#         # Calculate confidence interval for event rate\n",
    "#         se = np.sqrt(event_rate * (1 - event_rate) / n_total)\n",
    "#         ci_lower = max(0, event_rate - 1.96 * se)\n",
    "#         ci_upper = min(1, event_rate + 1.96 * se)\n",
    "#         \n",
    "#         event_rates[group] = {\n",
    "#             'rate': event_rate,\n",
    "#             'events': n_events,\n",
    "#             'total': n_total,\n",
    "#             'ci_lower': ci_lower,\n",
    "#             'ci_upper': ci_upper\n",
    "#         }\n",
    "#     \n",
    "#     return event_rates\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35824de",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Selection\n",
    "\n",
    "Compare all models comprehensively and select the best performing model for final validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43094ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 7: Comprehensive Model Comparison and Final Selection\n",
    "#\n",
    "# Your task: Compare all models systematically and select the best model for clinical use\n",
    "#\n",
    "# TODO: Set up model comparison framework:\n",
    "# 1. Print \"MODEL COMPARISON AND SELECTION\" header with separators\n",
    "# 2. Consolidate results from all models (time-independent + time-dependent)\n",
    "# 3. Create comprehensive comparison DataFrame with all metrics\n",
    "#\n",
    "# TODO: Create multi-criteria evaluation matrix:\n",
    "# 4. Define evaluation criteria with weights:\n",
    "#    - Primary criterion: C-index/AUC (weight: 0.3)\n",
    "#    - Clinical utility: Precision in low-risk group (weight: 0.25)\n",
    "#    - Discrimination: Risk group separation (weight: 0.2)\n",
    "#    - Calibration: Brier score or calibration slope (weight: 0.15)\n",
    "#    - Interpretability: Model complexity score (weight: 0.1)\n",
    "# 5. Normalize all metrics to 0-1 scale for comparison\n",
    "#\n",
    "# TODO: Calculate composite performance scores:\n",
    "# 6. For each model, calculate weighted composite score:\n",
    "#    - composite_score = Œ£(weight_i √ó normalized_metric_i)\n",
    "# 7. Rank models by composite score\n",
    "# 8. Identify top 5 models for detailed comparison\n",
    "#\n",
    "# TODO: Perform statistical significance testing:\n",
    "# 9. Compare top models using statistical tests:\n",
    "#    - Compare C-index differences using DeLong test (if available)\n",
    "#    - Bootstrap confidence intervals for metric differences\n",
    "#    - McNemar test for paired model comparisons (if applicable)\n",
    "# 10. Identify models with significantly better performance\n",
    "#\n",
    "# TODO: Analyze model agreement and diversity:\n",
    "# 11. Calculate prediction correlations between top models:\n",
    "#     - Pearson correlation of risk scores\n",
    "#     - Agreement in risk group assignments (kappa statistic)\n",
    "#     - Identify complementary vs redundant models\n",
    "# 12. Consider ensemble potential based on model diversity\n",
    "#\n",
    "# TODO: Evaluate model ensemble approaches:\n",
    "# 13. Create ensemble models using top performers:\n",
    "#     - Simple averaging: ensemble_score = mean(model_scores)\n",
    "#     - Weighted averaging: based on individual model performance\n",
    "#     - Stacking: train meta-model on model predictions\n",
    "# 14. Evaluate ensemble performance vs individual models\n",
    "# 15. Test ensemble stability and overfitting\n",
    "#\n",
    "# TODO: Assess practical considerations:\n",
    "# 16. For each top model, evaluate:\n",
    "#     - Feature requirements and data availability\n",
    "#     - Computational complexity and inference time\n",
    "#     - Model interpretability and explainability\n",
    "#     - Robustness to missing data\n",
    "#     - Calibration across different subgroups\n",
    "#\n",
    "# TODO: Create comprehensive comparison visualization:\n",
    "# 17. Create radar chart showing model performance across metrics\n",
    "# 18. Create scatter plot of C-index vs precision in low-risk\n",
    "# 19. Create model ranking visualization with confidence intervals\n",
    "# 20. Show feature importance comparison for top models\n",
    "#\n",
    "# TODO: Validate model selection criteria:\n",
    "# 21. Sensitivity analysis: how does ranking change with different weights?\n",
    "# 22. Cross-validation stability: consistent top performers across CV folds?\n",
    "# 23. Subgroup analysis: performance consistency across patient subgroups\n",
    "#\n",
    "# TODO: Clinical decision-making analysis:\n",
    "# 24. Decision curve analysis for top models:\n",
    "#     - Net benefit across probability thresholds\n",
    "#     - Clinical utility comparison\n",
    "#     - Optimal decision thresholds for each model\n",
    "# 25. Cost-effectiveness considerations (if cost data available)\n",
    "#\n",
    "# TODO: Final model selection:\n",
    "# 26. Apply selection criteria in priority order:\n",
    "#     - Statistical performance (C-index, calibration)\n",
    "#     - Clinical utility (risk stratification quality)\n",
    "#     - Practical implementation (interpretability, robustness)\n",
    "#     - Regulatory considerations (if applicable)\n",
    "# 27. Select final model with justification\n",
    "# 28. Select backup model in case of implementation issues\n",
    "#\n",
    "# TODO: Document selection rationale:\n",
    "# 29. Create detailed model selection report with:\n",
    "#     - Comparison methodology and criteria\n",
    "#     - Statistical analysis results\n",
    "#     - Clinical utility assessment\n",
    "#     - Implementation considerations\n",
    "#     - Final recommendation with rationale\n",
    "#\n",
    "# Expected output: Selected final model with comprehensive justification and backup options\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"MODEL COMPARISON AND SELECTION\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# def normalize_metric(metric_values, higher_is_better=True):\n",
    "#     \\\"\\\"\\\"Normalize metrics to 0-1 scale\\\"\\\"\\\"\n",
    "#     min_val, max_val = np.min(metric_values), np.max(metric_values)\n",
    "#     if max_val == min_val:\n",
    "#         return np.ones_like(metric_values)\n",
    "#     \n",
    "#     normalized = (metric_values - min_val) / (max_val - min_val)\n",
    "#     if not higher_is_better:\n",
    "#         normalized = 1 - normalized\n",
    "#     return normalized\n",
    "# \n",
    "# def calculate_composite_score(metrics_df, weights):\n",
    "#     \\\"\\\"\\\"Calculate weighted composite performance score\\\"\\\"\\\"\n",
    "#     composite_scores = np.zeros(len(metrics_df))\n",
    "#     \n",
    "#     for metric, weight in weights.items():\n",
    "#         if metric in metrics_df.columns:\n",
    "#             # Determine if higher is better for this metric\n",
    "#             higher_is_better = metric not in ['brier_score', 'complexity_score']\n",
    "#             normalized_values = normalize_metric(metrics_df[metric].values, higher_is_better)\n",
    "#             composite_scores += weight * normalized_values\n",
    "#     \n",
    "#     return composite_scores\n",
    "# \n",
    "# # Define evaluation criteria and weights\n",
    "# evaluation_criteria = {\n",
    "#     'c_index': 0.30,\n",
    "#     'precision_low_risk': 0.25,\n",
    "#     'risk_group_separation': 0.20,\n",
    "#     'calibration_score': 0.15,\n",
    "#     'interpretability_score': 0.10\n",
    "# }\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c876e8",
   "metadata": {},
   "source": [
    "## 8. Final Model Validation and Export\n",
    "\n",
    "Validate the selected model on test data and prepare for clinical implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù ACTIVITY 8: Final Model Validation and Clinical Implementation\n",
    "#\n",
    "# Your task: Validate the selected model on test data and prepare for clinical deployment\n",
    "#\n",
    "# TODO: Set up final validation framework:\n",
    "# 1. Print \"FINAL MODEL VALIDATION AND EXPORT\" header with separators\n",
    "# 2. Load the selected final model from previous activity\n",
    "# 3. Prepare test data with selected features: X_test_final = X_test[selected_features]\n",
    "#\n",
    "# TODO: Perform final model validation on test set:\n",
    "# 4. Make predictions on test set: test_predictions = final_model.predict_proba(X_test_final)[:, 1]\n",
    "# 5. Calculate final performance metrics:\n",
    "#    - Test C-index: test_c_index = roc_auc_score(y_test, test_predictions)\n",
    "#    - Test precision in low-risk: calculate using test predictions\n",
    "#    - Test calibration: calibration_curve(y_test, test_predictions)\n",
    "#    - Test risk stratification: create risk groups and evaluate separation\n",
    "#\n",
    "# TODO: Compare test performance to validation performance:\n",
    "# 6. Calculate performance differences:\n",
    "#    - c_index_difference = test_c_index - validation_c_index\n",
    "#    - precision_difference = test_precision_low - validation_precision_low\n",
    "# 7. Assess overfitting: significant drops indicate overfitting\n",
    "# 8. Calculate confidence intervals for test metrics using bootstrap\n",
    "#\n",
    "# TODO: Create final risk stratification on test set:\n",
    "# 9. Apply risk group thresholds from validation to test set\n",
    "# 10. Calculate event rates in each test risk group\n",
    "# 11. Perform survival analysis on test risk groups (if applicable)\n",
    "# 12. Validate risk group separation using statistical tests\n",
    "#\n",
    "# TODO: Model interpretation and feature importance:\n",
    "# 13. Extract final feature importance/coefficients:\n",
    "#     - For linear models: coefficient values and confidence intervals\n",
    "#     - For tree-based models: feature importance scores\n",
    "#     - For neural networks: permutation importance\n",
    "# 14. Create feature importance visualization\n",
    "# 15. Identify top 10 most important features with interpretation\n",
    "#\n",
    "# TODO: Create clinical implementation materials:\n",
    "# 16. Generate risk calculator/nomogram (for interpretable models):\n",
    "#     - Point-based scoring system\n",
    "#     - Risk probability tables\n",
    "#     - Clinical decision thresholds\n",
    "# 17. Create feature collection guidelines:\n",
    "#     - Required clinical variables\n",
    "#     - Laboratory test requirements\n",
    "#     - Data quality specifications\n",
    "#\n",
    "# TODO: Model robustness and sensitivity analysis:\n",
    "# 18. Test model performance on subgroups:\n",
    "#     - Performance by age groups, disease stage, etc.\n",
    "#     - Identify potential bias or performance disparities\n",
    "# 19. Missing data sensitivity:\n",
    "#     - Test performance with simulated missing features\n",
    "#     - Evaluate graceful degradation\n",
    "# 20. Feature perturbation analysis:\n",
    "#     - How sensitive are predictions to small feature changes?\n",
    "#\n",
    "# TODO: Create comprehensive validation report:\n",
    "# 21. Compile final model report including:\n",
    "#     - Model architecture and hyperparameters\n",
    "#     - Training/validation/test performance\n",
    "#     - Feature importance and interpretation\n",
    "#     - Clinical utility and risk stratification results\n",
    "#     - Implementation guidelines and requirements\n",
    "#\n",
    "# TODO: Export final model and artifacts:\n",
    "# 22. Save final trained model: joblib.dump(final_model, MODELS_PATH + 'final_risk_model.pkl')\n",
    "# 23. Save preprocessing pipeline: joblib.dump(preprocessing_pipeline, MODELS_PATH + 'preprocessing_pipeline.pkl')\n",
    "# 24. Export selected features list: pd.Series(selected_features).to_csv(MODELS_PATH + 'selected_features.csv')\n",
    "# 25. Save risk group thresholds: json.dump(risk_thresholds, open(MODELS_PATH + 'risk_thresholds.json', 'w'))\n",
    "#\n",
    "# TODO: Create model deployment package:\n",
    "# 26. Create prediction function template:\n",
    "#     def predict_risk(patient_features):\n",
    "#         # Load model and preprocessing\n",
    "#         # Apply preprocessing\n",
    "#         # Make prediction\n",
    "#         # Return risk score and group\n",
    "# 27. Create validation dataset for deployment testing\n",
    "# 28. Generate model card/documentation for clinical users\n",
    "#\n",
    "# TODO: Final performance summary:\n",
    "# 29. Create executive summary with:\n",
    "#     - Final model performance metrics\n",
    "#     - Clinical utility assessment\n",
    "#     - Implementation requirements\n",
    "#     - Monitoring and maintenance recommendations\n",
    "# 30. Generate one-page model summary for clinical stakeholders\n",
    "#\n",
    "# Expected output: Production-ready model with comprehensive validation and implementation materials\n",
    "\n",
    "# Write your code below:\n",
    "# print(\"FINAL MODEL VALIDATION AND EXPORT\")\n",
    "# print(\"=\"*50)\n",
    "# \n",
    "# def create_risk_calculator_points(model, feature_names, reference_values=None):\n",
    "#     \\\"\\\"\\\"Create point-based risk calculator for linear models\\\"\\\"\\\"\n",
    "#     if hasattr(model, 'coef_'):\n",
    "#         coefficients = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "#         \n",
    "#         # Calculate points (typically scaled to make max points = 100)\n",
    "#         max_coef = np.max(np.abs(coefficients))\n",
    "#         points = (coefficients / max_coef) * 100\n",
    "#         \n",
    "#         risk_calculator = pd.DataFrame({\n",
    "#             'Feature': feature_names,\n",
    "#             'Coefficient': coefficients,\n",
    "#             'Points': points.round(1)\n",
    "#         })\n",
    "#         \n",
    "#         return risk_calculator\n",
    "#     else:\n",
    "#         print(\"Risk calculator only available for linear models\")\n",
    "#         return None\n",
    "# \n",
    "# def bootstrap_test_performance(model, X_test, y_test, n_bootstrap=1000):\n",
    "#     \\\"\\\"\\\"Calculate bootstrap confidence intervals for test performance\\\"\\\"\\\"\n",
    "#     bootstrap_scores = []\n",
    "#     n_samples = len(X_test)\n",
    "#     \n",
    "#     for i in range(n_bootstrap):\n",
    "#         # Bootstrap sample\n",
    "#         indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "#         X_boot = X_test.iloc[indices]\n",
    "#         y_boot = y_test.iloc[indices]\n",
    "#         \n",
    "#         # Calculate performance\n",
    "#         pred_boot = model.predict_proba(X_boot)[:, 1]\n",
    "#         score_boot = roc_auc_score(y_boot, pred_boot)\n",
    "#         bootstrap_scores.append(score_boot)\n",
    "#     \n",
    "#     # Calculate confidence intervals\n",
    "#     ci_lower = np.percentile(bootstrap_scores, 2.5)\n",
    "#     ci_upper = np.percentile(bootstrap_scores, 97.5)\n",
    "#     \n",
    "#     return np.mean(bootstrap_scores), ci_lower, ci_upper\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce667fc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Model Development Summary\n",
    "\n",
    "In this comprehensive notebook, you have successfully completed:\n",
    "\n",
    "### ‚úÖ **Completed Tasks:**\n",
    "1. **Library Setup**: Imported ML, survival analysis, and evaluation libraries with availability checking\n",
    "2. **Data Loading**: Loaded preprocessed data and selected feature sets from previous notebooks\n",
    "3. **Time-Independent Models**: Trained classification models (Logistic, SVM, RF, GB, Neural Networks)\n",
    "4. **Time-Dependent Models**: Implemented survival analysis models (Cox regression, RSF)\n",
    "5. **Comprehensive Evaluation**: Applied C-index, precision in low-risk, calibration metrics\n",
    "6. **Risk Stratification**: Created meaningful patient risk groups with clinical validation\n",
    "7. **Model Comparison**: Systematically compared all models using multi-criteria evaluation\n",
    "8. **Final Validation**: Validated selected model on test data with clinical implementation prep\n",
    "\n",
    "### üéØ **Key Modeling Achievements:**\n",
    "- **Comprehensive Approach**: Both time-independent and time-dependent modeling strategies\n",
    "- **Clinical Focus**: Emphasis on C-index and precision in low-risk group identification\n",
    "- **Risk Stratification**: Meaningful patient group classification with statistical validation\n",
    "- **Robust Evaluation**: Multiple metrics including calibration and clinical utility assessment\n",
    "- **Production Ready**: Final model with implementation materials and deployment package\n",
    "\n",
    "### üìä **Evaluation Metrics Implemented:**\n",
    "- **C-index (Concordance Index)**: Primary ranking metric for both classification and survival models\n",
    "- **Precision in Low-Risk**: Clinical utility metric for identifying low-risk patients accurately\n",
    "- **Risk Group Separation**: Statistical validation of meaningful risk stratification\n",
    "- **Model Calibration**: Assessment of prediction reliability using calibration curves\n",
    "- **Clinical Utility**: Decision curve analysis and net benefit evaluation\n",
    "\n",
    "### üè• **Clinical Implementation Features:**\n",
    "- **Risk Group Definitions**: Low, medium, high risk classifications with clear thresholds\n",
    "- **Survival Analysis**: Time-to-event modeling with Kaplan-Meier curves and log-rank tests\n",
    "- **Model Interpretability**: Feature importance analysis and clinical decision support\n",
    "- **Robustness Testing**: Subgroup analysis and missing data sensitivity assessment\n",
    "\n",
    "### üîÑ **Next Steps (Optional Advanced Work):**\n",
    "1. **External Validation**: Validate model on independent datasets from other institutions\n",
    "2. **Prospective Validation**: Design prospective clinical study for model validation\n",
    "3. **Clinical Decision Support**: Integrate model into electronic health record systems\n",
    "4. **Regulatory Submission**: Prepare materials for FDA/CE mark approval (if applicable)\n",
    "5. **Continuous Learning**: Implement model updating pipeline with new data\n",
    "\n",
    "### üìÅ **Exported Files:**\n",
    "- `../results/models/final_risk_model.pkl`: Production-ready trained model\n",
    "- `../results/models/preprocessing_pipeline.pkl`: Data preprocessing pipeline\n",
    "- `../results/models/selected_features.csv`: Final feature set for model\n",
    "- `../results/models/risk_thresholds.json`: Risk group classification thresholds\n",
    "- `../results/models/model_validation_report.json`: Comprehensive model documentation\n",
    "\n",
    "### üéØ **Model Performance Summary:**\n",
    "- **Best Model**: [Selected based on comprehensive evaluation criteria]\n",
    "- **Test C-index**: [Final performance on held-out test set]\n",
    "- **Precision in Low-Risk**: [Accuracy of low-risk predictions]\n",
    "- **Risk Stratification**: [Statistical significance of patient group separation]\n",
    "- **Clinical Utility**: [Decision curve analysis and net benefit assessment]\n",
    "\n",
    "---\n",
    "\n",
    "**Exceptional work completing the comprehensive model development pipeline! üéâ**\n",
    "\n",
    "You've successfully developed, evaluated, and validated both time-independent and time-dependent models for patient risk classification. The rigorous evaluation framework with clinical metrics (C-index, precision in low-risk) and comprehensive risk stratification analysis provides a solid foundation for clinical implementation. Your final model is production-ready with complete documentation and deployment materials."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
